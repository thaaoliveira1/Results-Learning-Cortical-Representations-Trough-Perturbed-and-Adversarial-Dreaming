{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importing the PyTorch library for deep learning\n",
    "import numpy as np  # Importing the NumPy library for numerical operations\n",
    "# plotting\n",
    "import matplotlib  # Importing the Matplotlib library for data visualization\n",
    "import torch.utils.data  # Importing PyTorch's utilities for handling datasets\n",
    "import torchvision.datasets as dset  # Importing the torchvision.datasets module for pre-built datasets\n",
    "import torchvision.transforms as transforms  # Importing transformations for image preprocessing\n",
    "from torch.utils.data import Dataset, TensorDataset  # Importing utility classes for custom datasets\n",
    "from scipy import linalg  # Importing functions from SciPy for linear algebra operations\n",
    "matplotlib.use('Agg')  # Set the backend of matplotlib to 'Agg', which is a non-interactive backend for saving figures\n",
    "import matplotlib.pyplot as plt  # Import the pyplot module from matplotlib for creating and customizing plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weights initialization called on netG and netD\n",
    "def initialize_weights(module):\n",
    "    # Get the name of the module's class\n",
    "    module_classname = module.__class__.__name__\n",
    "\n",
    "    # Initialize the weights for convolutional layers\n",
    "    if module_classname.find('Conv') != -1:\n",
    "        # Initialize the weights using a normal distribution with mean 0 and standard deviation 0.02\n",
    "        module.weight.data.normal_(0.0, 0.02)\n",
    "\n",
    "    # Initialize the weights for batch normalization layers\n",
    "    elif module_classname.find('BatchNorm') != -1:\n",
    "        # Initialize the weights using a normal distribution with mean 1.0 and standard deviation 0.02\n",
    "        module.weight.data.normal_(1.0, 0.02)\n",
    "        # Fill the bias with zeros\n",
    "        module.bias.data.fill_(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code defines a custom weight initialization function called **`weights_init`**. This function is called on the modules **`netG`** and **`netD`** in a network.\n",
    "\n",
    "2. Retrieve the class name of the current module: The function accesses the class name of the current module using **`module.__class__.__name__`**. This allows us to identify the type of the module.\n",
    "\n",
    "3. If the module is a convolutional layer: The code checks if the class name contains the substring 'Conv'. If it does, this indicates that the module is a convolutional layer. In this case, the weights of the module are initialized using a normal distribution with mean 0 and standard deviation 0.02. This initialization helps in randomly initializing the weights close to zero.\n",
    "\n",
    "4. If the module is a batch normalization layer: The code checks if the class name contains the substring 'BatchNorm'. If it does, this indicates that the module is a batch normalization layer. In this case, the weights of the module are initialized using a normal distribution with mean 1.0 and standard deviation 0.02. Additionally, the bias of the module is set to all zeros. This initialization helps in standardizing the inputs during training.\n",
    "\n",
    "Overall, this custom weight initialization function ensures that the weights of the convolutional and batch normalization layers are properly initialized with appropriate values, contributing to the effectiveness and stability of the neural network training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Occlude(object):\n",
    "    def __init__(self, drop_rate=0.0, tile_size=7):\n",
    "        self.drop_rate = drop_rate\n",
    "        self.tile_size = tile_size\n",
    "\n",
    "    def __call__(self, images, dim=0):\n",
    "        # Create a copy of the input images\n",
    "        images_modified = images.clone()\n",
    "\n",
    "        # Determine the device to be used (CPU or GPU)\n",
    "        if dim == 0:\n",
    "            device = 'cpu'\n",
    "        else:\n",
    "            device = images.get_device()\n",
    "            if device == -1:\n",
    "                device = 'cpu'\n",
    "\n",
    "        # Create a mask tensor of ones with the same size as the images\n",
    "        mask = torch.ones((images_modified.size(dim), images_modified.size(dim + 1), images_modified.size(dim + 2)),\n",
    "                          device=device)\n",
    "\n",
    "        i = 0\n",
    "        while i < images_modified.size(dim + 1):\n",
    "            j = 0\n",
    "            while j < images_modified.size(dim + 2):\n",
    "                # Randomly drop tiles based on the drop rate\n",
    "                if np.random.rand() < self.drop_rate:\n",
    "                    for k in range(mask.size(0)):\n",
    "                        mask[k, i:i + self.tile_size, j:j + self.tile_size] = 0  # Set the tile to zero in the mask\n",
    "                j += self.tile_size\n",
    "            i += self.tile_size\n",
    "\n",
    "        # Apply the mask to each image by element-wise multiplication\n",
    "        images_modified = images_modified * mask\n",
    "        return images_modified\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`Occlude`** class represents an image augmentation technique that randomly occludes (drops) tiles within the input images. It can be used to introduce variations during training or testing of a machine learning model.\n",
    "\n",
    "1. The class is initialized with two parameters: **`drop_rate`** (probability of dropping a tile) and **`tile_size`** (size of the tiles to be occluded).\n",
    "\n",
    "2. The **`__call__`** method is defined, which allows the class instance to be called like a function.\n",
    "\n",
    "3. A copy of the input images is created using the **`clone`** method of the **`images`** tensor.\n",
    "\n",
    "4. The device is determined based on the **`dim`** parameter. If **`dim`** is 0, the device is set to CPU; otherwise, it uses the device of the input images tensor.\n",
    "\n",
    "5. A mask tensor is created with the same dimensions as the input images. It is initialized with ones using the **`torch.ones`** function.\n",
    "\n",
    "6. Two nested **`while`** loops are used to iterate over the tiles of the images.\n",
    "\n",
    "7. For each tile, a random number is generated between 0 and 1, and if it is less than the drop rate, the tile is occluded.\n",
    "\n",
    "8. When a tile needs to be occluded, the corresponding region in the mask tensor is set to zero, effectively occluding that tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor_batch):\n",
    "        # Create a copy of the tensor\n",
    "        tensor_batch = tensor_batch.clone()\n",
    "\n",
    "        # Iterate over each channel in the tensor\n",
    "        for i in range(len(tensor_batch)):\n",
    "            # Iterate over each channel in the tensor\n",
    "            for j in range(len(tensor_batch[i])):\n",
    "                # Unnormalize the tensor by multiplying by the standard deviation and adding the mean\n",
    "                tensor_batch[i][j].mul_(self.std[j]).add_(self.mean[j])\n",
    "                # The equivalent of normalization: t.sub_(m).div_(s)\n",
    "\n",
    "        return tensor_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code defines a class called **`UnNormalize`** that is used to reverse the normalization applied to a batch of tensor images. Here's how it works:\n",
    "\n",
    "2. The **`__init__`** method initializes the **`UnNormalize`** object with the mean and standard deviation values.\n",
    "\n",
    "3. The **`__call__`** method is invoked when an instance of **`UnNormalize`** is called as a function. It takes a batch of tensor images (**`tensor_batch`**) as input and returns the unnormalized batch of images.\n",
    "\n",
    "4. The method iterates over each tensor in the batch using a **`for`** loop.\n",
    "\n",
    "5. Within the inner loop, it iterates over each channel of the tensor using another **`for`** loop.\n",
    "\n",
    "6. For each channel, it performs the unnormalization by multiplying the tensor values by the corresponding standard deviation (**`self.std[j]`**) and adding the mean value (**`self.mean[j]`**).\n",
    "\n",
    "7. Finally, it returns the unnormalized tensor batch.\n",
    "\n",
    "In summary, the **`UnNormalize`** class allows you to reverse the normalization process applied to a batch of tensor images by multiplying the tensor values by the standard deviation and adding the mean for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name, dataroot, imageSize, is_train=True, drop_rate=0.0, tile_size=32):\n",
    "    \n",
    "            # Use the CIFAR10 dataset\n",
    "    if dataset_name == 'cifar10':\n",
    "        dataset = dset.CIFAR10(\n",
    "            train=is_train,\n",
    "            root=dataroot, download=False,\n",
    "            transform=transforms.Compose([\n",
    "                # Resize the image to the specified imageSize\n",
    "                transforms.Resize(imageSize),\n",
    "                # Convert the image to a tensor\n",
    "                transforms.ToTensor(),\n",
    "                # Normalize the image tensor with mean and standard deviation of 0.5\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                # Apply Occlude transformation with the specified drop_rate and tile_size\n",
    "                Occlude(drop_rate=drop_rate, tile_size=tile_size),\n",
    "            ])\n",
    "        )\n",
    "        # Create an instance of the UnNormalize class with mean and standard deviation of 0.5\n",
    "        unorm = UnNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        # Set the number of image channels to 3 (RGB)\n",
    "        img_channels = 3\n",
    "        \n",
    "            # Use the SVHN dataset\n",
    "    elif dataset_name == 'svhn':\n",
    "        if is_train:\n",
    "            split = 'train'\n",
    "        else:\n",
    "            split = 'test'\n",
    "        dataset = dset.SVHN(\n",
    "            root=dataroot, download=True,\n",
    "            split = split,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(imageSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                Occlude(drop_rate=drop_rate, tile_size=tile_size)\n",
    "            ]))\n",
    "        unorm = UnNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        img_channels = 3\n",
    "\n",
    "            # Use the MNIST dataset\n",
    "    elif dataset_name == 'mnist':\n",
    "        dataset = dset.MNIST(\n",
    "            train=is_train,\n",
    "            root=dataroot, download=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(imageSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                Occlude(drop_rate=drop_rate, tile_size=tile_size)\n",
    "            ])\n",
    "        )\n",
    "        unorm = UnNormalize(mean=(0.5,), std=(0.5,))\n",
    "        img_channels = 1\n",
    "        \n",
    "                # Use the FASHION dataset\n",
    "    elif dataset_name == 'fashion':\n",
    "        dataset = dset.FashionMNIST(\n",
    "            train=is_train,\n",
    "            root=dataroot, download=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(imageSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                Occlude(drop_rate=drop_rate, tile_size=tile_size)\n",
    "            ]))\n",
    "                # Create an instance of the UnNormalize class with mean and standard deviation of 0.5\n",
    "        unorm = UnNormalize(mean=(0.5,), std=(0.5,))\n",
    "                # Set the number of image channels to 1 (grayscale)\n",
    "        img_channels = 1\n",
    "    else:\n",
    "        raise NotImplementedError(\"No such dataset {}\".format(dataset_name))\n",
    "\n",
    "    assert dataset\n",
    "    return dataset, unorm, img_channels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code defines a function `get_dataset` that retrieves a dataset for image classification tasks.\n",
    "\n",
    "2. The function takes parameters such as the dataset name, root directory, image size, and optional arguments.\n",
    "\n",
    "3. The function checks the dataset name to determine the appropriate dataset.\n",
    "\n",
    "4. If the dataset is CIFAR-10:\n",
    "   a. The CIFAR10 dataset is created with the specified parameters.\n",
    "   b. The transformation pipeline includes resizing, tensor conversion, pixel normalization, and occlusion transformation.\n",
    "   c. An `UnNormalize` object is created for later visualizations.\n",
    "   d. The `img_channels` variable is set to 3.\n",
    "\n",
    "5. If the dataset is SVHN:\n",
    "   a. The SVHN dataset is created with the specified parameters.\n",
    "   b. The transformation pipeline includes resizing, tensor conversion, pixel normalization, and occlusion transformation.\n",
    "   c. An `UnNormalize` object is created for later visualizations.\n",
    "   d. The `img_channels` variable is set to 3.\n",
    "\n",
    "6. If the dataset is MNIST:\n",
    "   a. The MNIST dataset is created with the specified parameters.\n",
    "   b. The transformation pipeline includes resizing, tensor conversion, pixel normalization, and occlusion transformation.\n",
    "   c. An `UnNormalize` object is created for later visualizations.\n",
    "   d. The `img_channels` variable is set to 1.\n",
    "\n",
    "7. If the dataset is FashionMNIST:\n",
    "   a. The FashionMNIST dataset is created with the specified parameters.\n",
    "   b. The transformation pipeline includes resizing, tensor conversion, pixel normalization, and occlusion transformation.\n",
    "   c. An `UnNormalize` object is created for later visualizations.\n",
    "   d. The `img_channels` variable is set to 1.\n",
    "\n",
    "8. If an unsupported dataset name is provided, a `NotImplementedError` is raised.\n",
    "\n",
    "9. The function returns the created dataset object, the unnormalization object, and the number of image channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the current classification accuracy\n",
    "def compute_acc(predictions, labels):\n",
    "    correct = 0\n",
    "    # Get the predicted labels by selecting the index with the maximum value from the predictions tensor\n",
    "    predicted_labels = predictions.data.max(1)[1]\n",
    "    # Compare the predicted labels with the ground truth labels and count the number of correct predictions\n",
    "    correct = predicted_labels.eq(labels.data).cpu().sum()\n",
    "    # Compute the accuracy as the percentage of correct predictions\n",
    "    accuracy = float(correct) / float(len(labels.data)) * 100.0\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_latent(dim_latent, batch_size, device):\n",
    "    # Generate random values from a normal distribution with mean 0 and standard deviation 1\n",
    "    latent_z = np.random.normal(0, 1, (batch_size, dim_latent))\n",
    "    # Convert the generated values to a tensor and move it to the specified device (e.g., CPU or GPU)\n",
    "    latent_z = torch.tensor(latent_z, dtype=torch.float32, device=device)\n",
    "    # Reshape the tensor to have dimensions (batch_size, dim_latent, 1, 1)\n",
    "    latent_z = latent_z.view(batch_size, dim_latent, 1, 1)\n",
    "    return latent_z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The **`compute_acc`** function takes in two arguments: **`predictions`** and **`labels`**, which represent the predicted values and the ground truth labels, respectively.\n",
    "\n",
    "2. The **`correct`** variable is initialized to 0.\n",
    "\n",
    "3. **`preds_`** is assigned the predicted labels by selecting the index with the maximum value from the **`predictions`** tensor.\n",
    "\n",
    "4. The **`eq`** function compares the predicted labels with the ground truth labels, and the **`cpu`** function moves the tensor to the CPU memory if it was on a GPU.\n",
    "\n",
    "5. The number of correct predictions is computed by summing up the values where the predicted label equals the ground truth label.\n",
    "\n",
    "6. The accuracy is calculated by dividing the number of correct predictions by the total number of labels and multiplying by 100 to get the percentage.\n",
    "\n",
    "7. The **`get_latent`** function takes three arguments: **`dim_latent`**, **`batch_size`**, and **`device`**. It is used to generate a tensor of random values from a normal distribution.\n",
    "\n",
    "8. Random values are generated using **`np.random.normal`** with a mean of 0 and a standard deviation of 1, resulting in a **`(batch_size, dim_latent)`** array.\n",
    "\n",
    "9. The generated values are then converted to a PyTorch tensor of type **`torch.float32`** and moved to the specified **`device`**.\n",
    "\n",
    "10. The tensor is reshaped to have dimensions **`(batch_size, dim_latent, 1, 1)`** to match the expected input shape for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig_losses(epoch, d_losses, g_losses, r_losses_real, r_losses_fake, kl_losses, fid_nrem, fid_rem,  files_dir):\n",
    "    # Create an array of epoch values from 0 to 'epoch'\n",
    "    epochs = np.arange(0, epoch+1)\n",
    "    # Create a new figure with a size of 10x5 inches\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    # Add the first subplot (1 row, 2 columns, subplot 1)\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Plot the generator losses if available\n",
    "    if g_losses is not None:\n",
    "        ax1.plot(epochs, g_losses, label='generator (REM)')\n",
    "    \n",
    "    # Plot the discriminator losses if available\n",
    "    if d_losses is not None:\n",
    "        ax1.plot(epochs, d_losses, color='green', label='discriminator (Wake, REM)')\n",
    "    \n",
    "    # Set the x-axis label\n",
    "    ax1.set_xlabel('epochs')\n",
    "    # Set the y-axis label\n",
    "    ax1.set_ylabel('loss')\n",
    "    # Set the title of the subplot\n",
    "    ax1.set_title('losses with training')\n",
    "    \n",
    "    # Plot the real data reconstruction losses if available\n",
    "    if r_losses_real is not None:\n",
    "        ax1.plot(epochs, r_losses_real, color='orange', label='data rec. (Wake)')\n",
    "    \n",
    "    # Plot the fake data reconstruction losses if available\n",
    "    if r_losses_fake is not None:\n",
    "        ax1.plot(epochs, r_losses_fake, color='magenta', label='latent rec. (NREM)')\n",
    "    \n",
    "    # Plot the KL divergence losses if available\n",
    "    if kl_losses is not None:\n",
    "        ax1.plot(epochs, kl_losses, color='brown', label='KL div. (Wake)')\n",
    "    \n",
    "    # Add a legend to the subplot\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Check if FID scores for NREM and REM are available\n",
    "    if fid_nrem is not None and fid_rem is not None:\n",
    "        # Add the second subplot (1 row, 2 columns, subplot 2)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        # Plot the FID scores for NREM and REM\n",
    "        ax2.plot(epochs, fid_nrem, color='darkorange', label='FID NREM')\n",
    "        ax2.plot(epochs, fid_rem, color='magenta', label='FID REM')\n",
    "        # Add a legend to the subplot\n",
    "        ax2.legend()\n",
    "    \n",
    "    # Save the figure as 'losses.pdf' in the specified directory\n",
    "    fig.savefig(files_dir + '/losses.pdf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code defines a function called **`save_loss_fig`** that is responsible for creating a figure and saving it as a PDF file, depicting the losses and FID (FrÃ©chet Inception Distance) scores during training. Here's how it works:\n",
    "\n",
    "1. The function takes several input parameters: **`epoch`** (the current epoch), various loss values (**`generator_losses`**, **`discriminator_losses`**, **`real_losses_real`**, **`real_losses_fake`**, **`kl_losses`**), FID scores (**`fid_nrem`**, **`fid_rem`**), and the output directory path (**`output_dir`**).\n",
    "\n",
    "2. An array of epoch values is created using **`np.arange`** to span from 0 to the current epoch.\n",
    "\n",
    "3. A figure is created using **`plt.figure(figsize=(10, 5))`** with a size of 10x5 inches.\n",
    "\n",
    "4. The first subplot (ax1) is added to the figure (121 denotes a grid of 1 row and 2 columns, with this subplot occupying the first position).\n",
    "\n",
    "5. If generator losses are provided (**`generator_losses`** is not **`None`**), they are plotted against the epochs with the label 'generator (REM)'.\n",
    "\n",
    "6. If discriminator losses are provided (**`discriminator_losses`** is not **`None`**), they are plotted in green color with the label 'discriminator (Wake, REM)'.\n",
    "\n",
    "7. If real losses for real data (**`real_losses_real`**) are provided, they are plotted in orange color with the label 'data rec. (Wake)'.\n",
    "\n",
    "8. If real losses for fake data (**`real_losses_fake`**) are provided, they are plotted in magenta color with the label 'latent rec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_fig_trainval(epoch, losses, accuracies, directory):\n",
    "    # Generate the x-axis values from 0 to epoch\n",
    "    epochs = np.arange(0, epoch+1)\n",
    "\n",
    "    # Create a new figure with a size of 10x5 inches\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Add the first subplot for loss values\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.plot(epochs, losses['train'], label='train loss')  # Plot train loss\n",
    "    ax1.plot(epochs, losses['val'], label='validation loss')  # Plot validation loss\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Add the second subplot for accuracy values\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(epochs, accuracies['train'], label='train accuracy')  # Plot train accuracy\n",
    "    ax2.plot(epochs, accuracies['val'], label='val accuracy')  # Plot validation accuracy\n",
    "    ax2.set_xlabel('epochs')\n",
    "    ax2.set_ylabel('accuracy (%)')\n",
    "    ax2.set_ylim(0, 100)  # Set the y-axis limits\n",
    "    ax2.legend()\n",
    "\n",
    "    # Save the figure as a PDF file in the specified directory\n",
    "    fig.savefig(directory + '/trainval.pdf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`save_fig_trainval`** function takes several inputs: **`epoch`** (the number of epochs), **`losses`** (a dictionary containing loss values for training and validation), **`accuracies`** (a dictionary containing accuracy values for training and validation), and **`directory`** (the directory path where the resulting PDF file will be saved).\n",
    "\n",
    "1. The **`epochs`** variable is created using NumPy's **`arange`** function to generate values from 0 to **`epoch+1`**.\n",
    "\n",
    "2. A new figure object **`fig`** is created with a size of 10x5 inches using **`plt.figure(figsize=(10, 5))`**.\n",
    "\n",
    "3. The first subplot **`ax1`** is added to the figure at position 121 (1 row, 2 columns, 1st position).\n",
    "\n",
    "4. The **`ax1.plot`** function is called twice to plot the training and validation losses against the epochs using the values from the **`losses`** dictionary.\n",
    "\n",
    "5. The x-label and y-label for the first subplot are set using **`ax1.set_xlabel`** and **`ax1.set_ylabel`**.\n",
    "\n",
    "6. A legend is added to the first subplot using **`ax1.legend`**.\n",
    "\n",
    "7. The second subplot **`ax2`** is added to the figure at position 122 (1 row, 2 columns, 2nd position).\n",
    "\n",
    "8. The **`ax2.plot`** function is called twice to plot the training and validation accuracies against the epochs using the values from the **`accuracies`** dictionary.\n",
    "\n",
    "9. The x-label and y-label for the second subplot are set using **`ax2.set_xlabel`** and **`ax2.set_ylabel`**.\n",
    "\n",
    "10. The y-axis limits for the second subplot are set to range from 0 to 100 using **`ax2.set_ylim`**.\n",
    "\n",
    "11. A legend is added to the second subplot using **`ax2.legend`**.\n",
    "\n",
    "12. Finally, the figure is saved as a PDF file in the specified **`directory`** using **`fig.savefig`**.\n",
    "\n",
    "This function is useful for visualizing the training and validation progress of a machine learning model. It generates a single figure with two subplots: one for plotting the loss values and another for plotting the accuracy values. The resulting figure is saved as a PDF file for further analysis or reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import stats\n",
    "\n",
    "def kl_loss(latent_output):\n",
    "    # Compute the mean and standard deviation along the batch dimension\n",
    "    mean = torch.mean(latent_output, dim=0)\n",
    "    std = torch.std(latent_output, dim=0)\n",
    "    \n",
    "    # Compute the KL divergence loss using the formula\n",
    "    kl_loss = torch.mean((std ** 2 + mean ** 2) / 2 - torch.log(std) - 1/2)\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(images, model, batch_size=128, dims=2048, cuda=False):\n",
    "    model.eval()\n",
    "    act = np.empty((len(images), dims))\n",
    "    \n",
    "    # Move images to GPU if cuda is enabled\n",
    "    if cuda:\n",
    "        batch = images.cuda()\n",
    "    else:\n",
    "        batch = images\n",
    "    \n",
    "    # Get the model predictions for the batch of images\n",
    "    pred = model(batch)[0]\n",
    "    \n",
    "    # If the model output is not a scalar, apply global spatial average pooling\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = torch.nn.functional.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "    \n",
    "    # Convert the tensor to numpy array and reshape it\n",
    "    act = pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    return act\n",
    "    \n",
    "    # Compute the mean and covariance of the activation values\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def mean_and_sem(array, color=None, axis=0):\n",
    "    # Compute the mean of the array along the specified axis\n",
    "    mean = array.mean(axis=0)\n",
    "    \n",
    "    # Compute the standard error of the mean (SEM)\n",
    "    sem_plus = mean + stats.sem(array, axis=axis)\n",
    "    sem_minus = mean - stats.sem(array, axis=axis)\n",
    "    \n",
    "    # Fill the area between the upper and lower SEM with the specified color\n",
    "    if color is not None:\n",
    "        ax.fill_between(np.arange(mean.shape[0]), sem_plus, sem_minus, color=color, alpha=0.5)\n",
    "    else:\n",
    "        ax.fill_between(np.arange(mean.shape[0]), sem_plus, sem_minus, alpha=0.5)\n",
    "    \n",
    "    return mean\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The **`kl_loss`** function calculates the Kullback-Leibler (KL) divergence loss for a given **`latent_output`**. It computes the mean and standard deviation of the **`latent_output`**, and then uses these values to compute the KL divergence loss according to the formula.\n",
    "\n",
    "2. The **`calculate_activation_statistics`** function is used to compute the mean and covariance of the activation values produced by a given **`model`** for a batch of **`images`**. It first sets the model to evaluation mode and initializes an array **`act`** to store the activation values. It then moves the images to the GPU if **`cuda`** is enabled. The model is applied to the batch of images, and if the model output is not a scalar, global spatial average pooling is performed. The resulting tensor is converted to a numpy array and reshaped before returning it.\n",
    "\n",
    "3. The **`mean_and_sem`** function calculates the mean and standard error of the mean (SEM) for a given array **`array`** along the specified **`axis`**. It computes the mean along the axis and then calculates the upper and lower SEM by adding and subtracting the SEM, respectively, using the **`stats.sem`** function from SciPy. Finally, it fills the area between the upper and lower SEM with a specified color.\n",
    "\n",
    "Overall, this code provides utility functions for calculating KL divergence loss, activation statistics, and mean with SEM, which can be useful in various machine learning and statistical analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    # Numpy implementation of the Frechet Distance.\n",
    "    # The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    # and X_2 ~ N(mu_2, C_2) is\n",
    "    #         d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \n",
    "    # Convert mu1 and mu2 to arrays with at least 1 dimension\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    # Convert sigma1 and sigma2 to arrays with at least 2 dimensions\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    # Check if mu1 and mu2 have the same shape\n",
    "    assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n",
    "    \n",
    "    # Check if sigma1 and sigma2 have the same shape\n",
    "    assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n",
    "\n",
    "    # Calculate the difference between mu1 and mu2\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # Calculate the square root of the matrix product of sigma1 and sigma2\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Check if the covmean contains any non-finite values\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        # Add epsilon to the diagonal of sigma1 and sigma2\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # Check if the covmean is a complex object\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        # Take the real component of the covmean\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Calculate the trace of covmean\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    # Calculate and return the Frechet distance\n",
    "    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)\n",
    "\n",
    "\n",
    "def calculate_frechet(inception_real, inception_fake, model, return_statistics=False):\n",
    "    # Calculate the mean and covariance of the real and fake inception outputs\n",
    "    mu_1 = np.mean(inception_real, axis=0)\n",
    "    mu_2 = np.mean(inception_fake, axis=0)\n",
    "    std_1 = np.cov(inception_real, rowvar=False)\n",
    "    std_2 = np.cov(inception_fake, rowvar=False)\n",
    "    \n",
    "    # Calculate the Frechet distance\n",
    "    fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "\n",
    "    return fid_value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`calculate_frechet_distance`** is a function that calculates the Frechet distance between two multivariate Gaussian distributions. It takes four parameters: **`mu1`**, **`sigma1`**, **`mu2`**, and **`sigma2`**, which represent the mean and covariance matrices of the two distributions. The optional parameter **`eps`** is used for numerical stability. The function implements the formula for the Frechet distance: **`d^2 = ||mu1 - mu2||^2 + Tr(C1 + C2 - 2*sqrt(C1*C2))`**, where **`mu1`** and **`mu2`** are the means, **`C1`** and **`C2`** are the covariance matrices, and **`Tr`** denotes the trace operation.\n",
    "\n",
    "2. The function first ensures that **`mu1`** and **`mu2`** are at least 1-dimensional arrays, and **`sigma1`** and **`sigma2`** are at least 2-dimensional arrays using the **`np.atleast_1d`** and **`np.atleast_2d`** functions.\n",
    "\n",
    "3. It then checks if **`mu1`** and **`mu2`** have the same shape, and if **`sigma1`** and **`sigma2`** have the same shape. If the shapes are not equal, it raises assertions to indicate that the mean vectors and covariance matrices have different lengths or dimensions.\n",
    "\n",
    "4. The function calculates the difference between **`mu1`** and **`mu2`** and stores it in the variable **`diff`**.\n",
    "\n",
    "5. Next, it computes the square root of the matrix product of **`sigma1`** and **`sigma2`** using the **`sqrtm`** function from **`scipy.linalg`**. This operation is stored in the variable **`covmean`**.\n",
    "\n",
    "6. The function checks if **`covmean`** contains any non-finite (e.g., NaN or infinity) values. If it does, it adds a small epsilon value to the diagonal of **`sigma1`** and **`sigma2`** to ensure numerical stability. This step helps handle cases where the covariance matrices are ill-conditioned or singular.\n",
    "\n",
    "7. If the **`covmean`** matrix is complex, the function checks if the imaginary components on the diagonal are close to zero within a tolerance. If not, it raises a **`ValueError`** indicating the presence of a significant imaginary component.\n",
    "\n",
    "8. Finally, the function calculates the trace of **`covmean`** and returns the Frechet distance as the sum of the squared difference between the means and the trace of the covariance matrices.\n",
    "\n",
    "9. The **`calculate_frechet`** function takes two inputs: **`inception_real`** and **`inception_fake`**, which represent the real and fake samples, respectively, and calculates the Frechet Inception Distance (FID) between them. It also takes a **`model`** parameter, which is not used in the provided code snippet.\n",
    "\n",
    "10. Inside **`calculate_frechet`**, the mean (**`mu_1`** and **`mu_2`**) and covariance (**`std_1`** and **`std_2`**) of the real and fake samples are calculated using the **`np.mean`** and **`np.cov`** functions.\n",
    "\n",
    "11. The FID value is obtained by calling the **`calculate_frechet_distance`** function with the calculated mean and covariance values.\n",
    "\n",
    "12. Finally, the FID value is returned as the result of the **`calculate_frechet`** function.\n",
    "\n",
    "In summary, the code provides a way to calculate the Frechet Inception Distance, which is a measure of similarity between two sets of samples based on their mean vectors and covariance matrices. The **`calculate_frechet_distance`** function implements the mathematical formula for the Frechet distance, while the **`calculate_frechet`** function orchestrates the calculation by extracting the mean and covariance values from the input samples and calling the `calculate_frechet_distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
