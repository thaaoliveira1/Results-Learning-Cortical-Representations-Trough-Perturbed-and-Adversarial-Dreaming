{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # Enable compatibility with Python 2 and 3 print statements\n",
    "import argparse  # Module for parsing command-line arguments\n",
    "import os  # Module for interacting with the operating system\n",
    "import numpy as np  # Numerical computing library\n",
    "import torch.optim as optim  # Optimization algorithms for PyTorch\n",
    "import torch.utils.data  # Utilities for handling data in PyTorch\n",
    "import torchvision.utils as vutils  # Utility functions for image visualization in PyTorch\n",
    "from torch.autograd import Variable  # Functionality for automatic differentiation in PyTorch\n",
    "from functions import *  # Import functions from a custom functions module\n",
    "from model import *  # Import classes and functions from a custom model module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(imageSize=32, dataset='fashion', dataroot='./datasets/', num_workers=2, is_continue=1, batch_size=64, image_size=32, latent_size=256, num_epochs=55, weight_cycle_consistency=1.0, W=1.0, N=1.0, R=1.0, epsilon=0.0, num_filters=64, dropout_prob=0.0, learning_rate_generator=0.0002, learning_rate_discriminator=0.0002, beta1=0.5, lmbd=0.5, num_gpus=1, output_folder='trained_fashion', gpu_id='0', outf='ex_fashion', workers=2, niter=55)\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import argparse\n",
    "\n",
    "# Creating an argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Adding arguments with their default values and descriptions\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\n",
    "parser.add_argument('--dataset', default='fashion', help='Dataset to use: cifar10 | imagenet | mnist')\n",
    "parser.add_argument('--dataroot', default='./datasets/', help='Path to the dataset')\n",
    "parser.add_argument('--num_workers', type=int, help='Number of data loading workers', default=2)\n",
    "parser.add_argument('--is_continue', type=int, default=1, help='Use pre-trained model')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Input batch size')\n",
    "parser.add_argument('--image_size', type=int, default=32, help='Height/width of the input image to the network')\n",
    "parser.add_argument('--latent_size', type=int, default=256, help='Size of the latent vector')\n",
    "parser.add_argument('--num_epochs', type=int, default=55, help='Number of epochs to train for')\n",
    "parser.add_argument('--weight_cycle_consistency', type=float, default=1.0, help='Weight of Cycle Consistency')\n",
    "parser.add_argument('--W', type=float, default=1.0, help='Wake')\n",
    "parser.add_argument('--N', type=float, default=1.0, help='NREM')\n",
    "parser.add_argument('--R', type=float, default=1.0, help='REM')\n",
    "parser.add_argument('--epsilon', type=float, default=0.0, help='Amount of noise in the wake latent space')\n",
    "parser.add_argument('--num_filters', type=int, default=64, help='Filters factor')\n",
    "parser.add_argument('--dropout_prob', type=float, default=0.0, help='Probability of dropout')\n",
    "parser.add_argument('--learning_rate_generator', type=float, default=0.0002, help='Learning rate for the generator')\n",
    "parser.add_argument('--learning_rate_discriminator', type=float, default=0.0002, help='Learning rate for the discriminator')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='Beta1 for Adam optimizer')\n",
    "parser.add_argument('--lmbd', type=float, default=0.5, help='convex combination factor for REM')\n",
    "parser.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use')\n",
    "parser.add_argument('--output_folder', default='trained_fashion', help='Folder to output images and model checkpoints')\n",
    "parser.add_argument('--gpu_id', type=str, default='0', help='The ID of the specified GPU')\n",
    "parser.add_argument('--outf', default='ex_fashion', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "\n",
    "\n",
    "\n",
    "# Parsing the command-line arguments\n",
    "opt, unknown = parser.parse_known_args()\n",
    "\n",
    "# Set the number of iterations to the number of epochs\n",
    "opt.niter = opt.num_epochs\n",
    "\n",
    "# Assign the value of latent_size based on opt.latent_size\n",
    "latent_size = opt.latent_size\n",
    "\n",
    "# Printing the parsed arguments\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the GPU ID if using only 1 GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_id\n",
    "\n",
    "# Define the directories for saving samples, training curves, and models\n",
    "dir_files = './results/' + opt.dataset + '/' + opt.outf  # Directory for saving samples and training curves\n",
    "dir_checkpoint = './checkpoints/' + opt.dataset + '/' + opt.outf  # Directory for saving models\n",
    "model = './checkpoints/' + opt.dataset + '/' + opt.output_folder  # Directory for saving models\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(dir_files, exist_ok=True)\n",
    "os.makedirs(dir_checkpoint, exist_ok=True)\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset and get relevant information\n",
    "dataset, unorm, img_channels = get_dataset(opt.dataset, opt.dataroot, opt.imageSize, is_train=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=int(opt.workers), drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute path of the 'model' directory is: /Users/Thamires/Documents/Knowledge_Discovery/Results-Learning-Cortical-Representations-Trough-Perturbed-and-Adversarial-Dreaming/code/checkpoints/fashion/trained_fashion\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the directory\n",
    "model_abs_path = os.path.abspath(model)\n",
    "\n",
    "# Print the absolute path\n",
    "print(f\"The absolute path of the 'model' directory is: {model_abs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (dis): Sequential(\n",
       "    (0): Conv2d(256, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and assign values to hyperparameters\n",
    "num_gpus = int(opt.num_gpus)\n",
    "latent_dim = int(opt.latent_size)\n",
    "batch_size = opt.batch_size\n",
    "lmbd = 0.5\n",
    "\n",
    "\n",
    "# Instantiate generator and discriminator networks\n",
    "generator = Generator(num_gpus, latent_dim=latent_dim, ngf=opt.num_filters, img_channels=img_channels)\n",
    "generator.apply(initialize_weights)\n",
    "discriminator = Discriminator(num_gpus, latent_dim=latent_dim, ndf=opt.num_filters, img_channels=img_channels, dropout_prob=opt.dropout_prob)\n",
    "discriminator.apply(initialize_weights)\n",
    "\n",
    "# Move networks to the GPU\n",
    "generator.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path, map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Update the variable name here\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load the generator and discriminator models from the checkpoint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m generator\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mgenerator\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      5\u001b[0m discriminator\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mdiscriminator\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'generator'"
     ]
    }
   ],
   "source": [
    "model_path = os.path.abspath('./checkpoints/fashion/trained_fashion/trained_fashion.pth')\n",
    "checkpoint = torch.load(model_path, map_location='cpu')  # Update the variable name here\n",
    "# Load the generator and discriminator models from the checkpoint\n",
    "generator.load_state_dict(checkpoint['generator'])\n",
    "discriminator.load_state_dict(checkpoint['discriminator'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code first checks if a trained model checkpoint file exists using the **`os.path.exists()`** function.\n",
    "\n",
    "2. If a checkpoint file exists, it indicates that a pre-trained model is available, and the code proceeds to load the data from the checkpoint.\n",
    "\n",
    "3. The **`torch.load()`** function is used to load the checkpoint file, specifying the path to the file (**`dir_checkpoint+'/trained.pth'`**) and the **`map_location`** parameter as **`'cpu'`** to ensure compatibility.\n",
    "\n",
    "4. The generator and discriminator models (**`netG`** and **`netD`**) are updated with the saved state dictionaries using the **`load_state_dict()`** method.\n",
    "\n",
    "5. A message is printed indicating that training will start from the loaded model.\n",
    "\n",
    "6. If no trained model checkpoint file exists, the code proceeds to the **`else`** block and prints a message indicating that training will restart.\n",
    "\n",
    "In summary, this code checks if a pre-trained model checkpoint file exists and loads the model from the checkpoint if it exists. If no checkpoint file is found, it assumes that training will start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (tconv1): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (tconv2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (tconv3): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (tconv4): Sequential(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load epoch 3 networks\n",
    "netG3 = Generator(num_gpus, latent_dim=latent_size, img_channels=img_channels)\n",
    "netG3.apply(initialize_weights)\n",
    "netD3 = Discriminator(num_gpus, latent_dim=latent_size, img_channels=img_channels)\n",
    "netD3.apply(initialize_weights)\n",
    "# send to GPU\n",
    "netD3.to(device)\n",
    "netG3.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model detected, restart training...\n"
     ]
    }
   ],
   "source": [
    "# Check if the checkpoint file exists\n",
    "if os.path.exists(dir_checkpoint+'/trained2.pth'):\n",
    "    # Load data from last checkpoint\n",
    "    print('Loading pre-trained model...')\n",
    "    # Load the checkpoint data\n",
    "    checkpoint = torch.load(dir_checkpoint+'/trained2.pth', map_location='cpu')\n",
    "    # Load the generator state_dict from the checkpoint\n",
    "    netG3.load_state_dict(checkpoint['generator'])\n",
    "    # Load the discriminator state_dict from the checkpoint\n",
    "    netD3.load_state_dict(checkpoint['discriminator'])\n",
    "    print('Start training from loaded model...')\n",
    "else:\n",
    "    print('No pre-trained model detected, restart training...')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code checks if a specific checkpoint file (**`trained2.pth`**) exists in the specified directory.\n",
    "\n",
    "2. If the checkpoint file exists, it proceeds to load the pre-trained model.\n",
    "\n",
    "3. It prints a message indicating that the pre-trained model is being loaded.\n",
    "\n",
    "4. The **`torch.load()`** function is used to load the checkpoint data into a dictionary.\n",
    "\n",
    "5. The generator's state_dict is loaded from the checkpoint dictionary into the **`netG3`** model.\n",
    "\n",
    "6. The discriminator's state_dict is loaded from the checkpoint dictionary into the **`netD3`** model.\n",
    "\n",
    "7. A message is printed to indicate that training will start from the loaded model if it exists.\n",
    "\n",
    "8. If the checkpoint file does not exist, a message is printed indicating that no pre-trained model is detected, and training should be restarted.\n",
    "\n",
    "In summary, this code checks if a specific checkpoint file exists and loads the generator and discriminator models' state_dicts from the checkpoint file if it exists. It allows you to continue training from a saved model checkpoint or start training from scratch if no checkpoint is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare images\n",
    "dataloader_iter = iter(dataloader)\n",
    "# Get the first batch of images and their corresponding labels\n",
    "image_eval1, _ = next(dataloader_iter)\n",
    "# Get the second batch of images and their corresponding labels\n",
    "image_eval2, _ = next(dataloader_iter)\n",
    "# Create clones of the image batches for saving\n",
    "image_eval1_save = image_eval1.clone()\n",
    "image_eval2_save = image_eval2.clone()\n",
    "# Save the first 3 images from the first batch as evaluation wake1 images\n",
    "vutils.save_image(unorm(image_eval1_save[:3]).data, '%s/eval_wake1_%03d.png' % (dir_files, 0), nrow=1)\n",
    "# Save the first 3 images from the second batch as evaluation wake2 images\n",
    "vutils.save_image(unorm(image_eval2_save[:3]).data, '%s/eval_wake2_%03d.png' % (dir_files, 0), nrow=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code prepares the images for evaluation by obtaining two batches of images from the dataloader.\n",
    "\n",
    "2. It creates an iterator (**`dataloader_iter`**) from the dataloader to iterate over the batches.\n",
    "\n",
    "3. The **`next()`** function is used to get the next batch of images and their corresponding labels from the iterator. Here, **`image_eval1`** and **`_`** are used to store the images and labels of the first batch, respectively.\n",
    "\n",
    "4. Similarly, the **`next()`** function is used again to get the next batch of images and their corresponding labels. Here, **`image_eval2`** and **`_`** are used to store the images and labels of the second batch.\n",
    "\n",
    "5. Clones of the image batches are created for saving purposes. This is done to preserve the original images before any transformations.\n",
    "\n",
    "6. The **`vutils.save_image()`** function is used to save the first 3 images from the first batch (**`image_eval1_save`**) as evaluation wake1 images. The **`unorm()`** function is used to unnormalize the image tensor before saving.\n",
    "\n",
    "7. Similarly, the **`vutils.save_image()`** function is used to save the first 3 images from the second batch (**`image_eval2_save`**) as evaluation wake2 images. Again, the **`unorm()`** function is used to unnormalize the image tensor before saving.\n",
    "\n",
    "In summary, this code prepares two batches of images for evaluation. It saves the first 3 images from each batch as wake1 and wake2 evaluation images, respectively, by unnormalizing and saving the image tensors using the **`vutils.save_image()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with final epoch networks\n",
    "with torch.no_grad():\n",
    "    # Move the images to the device (GPU)\n",
    "    image_eval1 = image_eval1.to(device)\n",
    "    image_eval2 = image_eval2.to(device)\n",
    "    # Get the latent outputs from the discriminator for the images\n",
    "    latent_output1, _ = discriminator(image_eval1)\n",
    "    latent_output2, _ = discriminator(image_eval2)\n",
    "    # Generate NREM (Non-REM) sample using the generator\n",
    "    nrem = generator(latent_output1)\n",
    "    # Generate random noise vector\n",
    "    noise = torch.randn(batch_size, latent_size, device=device)\n",
    "    # Combine the latent outputs, noise, and perform linear interpolation\n",
    "    latent_rem = 0.25 * latent_output1 + 0.25 * latent_output2 + 0.5 * noise\n",
    "    # Generate REM (Rapid Eye Movement) sample using the generator\n",
    "    rem = generator(latent_rem)\n",
    "\n",
    "# Unnormalize the generated samples\n",
    "nrem = unorm(nrem)\n",
    "rem = unorm(rem)\n",
    "rec_image_eval1 = unorm(image_eval1)\n",
    "rec_image_eval2 = unorm(image_eval2)\n",
    "\n",
    "# Save the generated samples\n",
    "vutils.save_image(rec_image_eval1[:3].data, '%s/eval_rec1.png' % (dir_files), nrow=1)\n",
    "vutils.save_image(rec_image_eval2[:3].data, '%s/eval_rec2.png' % (dir_files), nrow=1)\n",
    "vutils.save_image(nrem[:3].data, '%s/eval_nrem.png' % (dir_files), nrow=1)\n",
    "vutils.save_image(rem[:3].data, '%s/eval_rem.png' % (dir_files), nrow=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code generates samples using the final epoch networks.\n",
    "\n",
    "2. The **`torch.no_grad()`** context manager is used to disable gradient calculations, as we are only generating samples and not training.\n",
    "\n",
    "3. The images (**`image_eval1`** and **`image_eval2`**) are moved to the device (GPU) using the **`to()`** method.\n",
    "\n",
    "4. The discriminator is used to obtain the latent outputs (**`latent_output1`** and **`latent_output2`**) for the images.\n",
    "\n",
    "5. The generator is used to generate the NREM (Non-REM) sample (**`nrem`**) by passing the **`latent_output1`** through it.\n",
    "\n",
    "6. Random noise is generated using **`torch.randn()`**.\n",
    "\n",
    "7. The latent outputs, noise, and original images are combined to create a blended latent representation for the REM (Rapid Eye Movement) sample (**`latent_rem`**).\n",
    "\n",
    "8. The generator is then used to generate the REM sample (**`rem`**) by passing the **`latent_rem`** through it.\n",
    "\n",
    "9. The generated samples are unnormalized using the **`unorm()`** function.\n",
    "\n",
    "10. The generated samples and reconstructed images are saved using the **`vutils.save_image()`** function.\n",
    "\n",
    "In summary, this code generates samples using the final epoch networks. It uses the discriminator to obtain latent outputs for the input images and generates NREM and REM samples using the generator. The generated samples are then unnormalized and saved as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with epoch 3 networks\n",
    "with torch.no_grad():\n",
    "    # Move the images to the device (GPU)\n",
    "    image_eval1 = image_eval1.to(device)\n",
    "    image_eval2 = image_eval2.to(device)\n",
    "    \n",
    "    # Get the latent outputs from the discriminator for the images\n",
    "    latent_output1, _ = netD3(image_eval1)\n",
    "    latent_output2, _ = netD3(image_eval2)\n",
    "    \n",
    "    # Generate NREM (Non-REM) sample using the generator\n",
    "    nrem = netG3(latent_output1)\n",
    "    \n",
    "    # Generate random noise vector\n",
    "    noise = torch.randn(batch_size, latent_size, device=device)\n",
    "    \n",
    "    # Combine the latent outputs, noise, and perform linear interpolation\n",
    "    latent_rem = 0.25 * latent_output1 + 0.25 * latent_output2 + 0.5 * noise\n",
    "    \n",
    "    # Generate REM (Rapid Eye Movement) sample using the generator\n",
    "    rem = netG3(latent_rem)\n",
    "\n",
    "# Unnormalize the generated samples\n",
    "nrem = unorm(nrem)\n",
    "rem = unorm(rem)\n",
    "rec_image_eval1 = unorm(rec_image_eval1)\n",
    "rec_image_eval2 = unorm(rec_image_eval2)\n",
    "\n",
    "# Save the generated samples\n",
    "vutils.save_image(rec_image_eval1[:3].data, '%s/eval3_rec1.png' % (dir_files), nrow=1)\n",
    "vutils.save_image(rec_image_eval2[:3].data, '%s/eval3_rec2.png' % (dir_files), nrow=1)\n",
    "vutils.save_image(nrem[:3].data, '%s/eval3_nrem.png' % (dir_files), nrow=1)\n",
    "vutils.save_image(rem[:3].data, '%s/eval3_rem.png' % (dir_files), nrow=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code is generating samples using the networks trained until epoch 3.\n",
    "\n",
    "2. The **`torch.no_grad()`** context manager is used to disable gradient calculation, reducing memory usage and speeding up computations.\n",
    "\n",
    "3. The images **`image_eval1`** and **`image_eval2`** are moved to the device (GPU).\n",
    "\n",
    "4. The latent outputs of the discriminator for the images are obtained.\n",
    "\n",
    "5. The generator **`netG3`** generates NREM samples using the first latent output.\n",
    "\n",
    "6. Random noise is generated using **`torch.randn`**.\n",
    "\n",
    "7. The latent outputs, noise, and original latent outputs are combined and interpolated to create the latent input for generating REM samples.\n",
    "\n",
    "8. The generator **`netG3`** generates REM samples using the interpolated latent input.\n",
    "\n",
    "9. The generated samples are unnormalized using the **`unorm`** function.\n",
    "\n",
    "10. The reconstructed images (**`rec_image_eval1`** and **`rec_image_eval2`**) are unnormalized.\n",
    "\n",
    "11. The generated samples and reconstructed images are saved as images.\n",
    "\n",
    "Please note that the variables **`rec_image_eval1`** and **`rec_image_eval2`** should be defined and assigned values before this section of the code is executed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
