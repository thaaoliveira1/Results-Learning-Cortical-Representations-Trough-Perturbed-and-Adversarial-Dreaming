{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # Importing a print function from future versions of Python\n",
    "import argparse  # Importing the argparse module for command-line argument parsing\n",
    "import os  # Importing the os module for operating system related functionalities\n",
    "import numpy as np  # Importing the numpy library for numerical computations\n",
    "import torch.optim as optim  # Importing the optim module from the torch library for optimization algorithms\n",
    "import torch.utils.data  # Importing the data module from the torch library for handling data loading and processing\n",
    "import torchvision.utils as vutils  # Importing the vutils module from torchvision for image utilities\n",
    "from torch.autograd import Variable  # Importing the Variable class from torch.autograd for automatic differentiation\n",
    "from functions import *  # Importing custom functions from a module named 'functions'\n",
    "from model import *  # Importing custom models from a module named 'model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='fashion', dataroot='./datasets/', workers=2, batchSize=64, imageSize=32, nz=256, niterC=20, nf=64, drop=0, lrC=0.2, ngpu=1, outf='output', acc_file='accuracies_occ.pth', num_classes=10, tile_size=4, gpu_id='0')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='fashion', help='cifar10 | imagenet | mnist')\n",
    "parser.add_argument('--dataroot', default='./datasets/', help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=256, help='size of the latent z vector')\n",
    "parser.add_argument('--niterC', type=int, default=20, help='number of epochs to train the classifier')\n",
    "parser.add_argument('--nf', type=int, default=64, help='filters factor')\n",
    "parser.add_argument('--drop', type=float, default=0, help='probably of dropping a patch')\n",
    "parser.add_argument('--lrC', type=float, default=0.2, help='learning rate of the classifier, default=0.0002')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--outf', default='output', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--acc_file', default='accuracies_occ.pth', help='folder to output accuracies')\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='Number of classes for AC-GAN')\n",
    "parser.add_argument('--tile_size', type=int, default=4, help='tile size for occlusions')\n",
    "parser.add_argument('--gpu_id', type=str, default='0', help='The ID of the specified GPU')\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the gpu id if using only 1 gpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_id\n",
    "\n",
    "dir_files = './results/'+opt.dataset+'/'+opt.outf\n",
    "dir_checkpoint = './checkpoints/'+opt.dataset+'/'+opt.outf\n",
    "acc_file = opt.acc_file\n",
    "\n",
    "try:\n",
    "    os.makedirs(dir_files)\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs(dir_checkpoint)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "drop_rate = opt.drop/100.0\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if opt.dataset == 'cifar10':\n",
    "    n_train = 50000\n",
    "    n_test = 10000\n",
    "elif opt.dataset == 'svhn':\n",
    "    n_train = 58606\n",
    "    n_test = 14651\n",
    "elif opt.dataset == 'mnist':\n",
    "    n_train = 50000\n",
    "    n_test = 10000\n",
    "elif opt.dataset == 'fashion':\n",
    "    n_train = 50000\n",
    "    n_test = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (tconv1): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (tconv2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (tconv3): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (tconv4): Sequential(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train dataset with occlusions (param drop_rate)\n",
    "dataset, unorm, img_channels = get_dataset(dataset_name=opt.dataset, dataroot=opt.dataroot, imageSize=opt.imageSize, is_train=True, drop_rate=drop_rate, tile_size=opt.tile_size)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=n_train, shuffle=True, num_workers=int(opt.workers), drop_last=True)\n",
    "# test dataset with occlusions (param drop_rate)\n",
    "test_dataset, unorm, img_channels = get_dataset(dataset_name=opt.dataset, dataroot=opt.dataroot, imageSize=opt.imageSize, is_train=False, drop_rate=drop_rate, tile_size=opt.tile_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=n_test, shuffle=False, num_workers=int(opt.workers), drop_last=True)\n",
    "\n",
    "# some hyper parameters\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "num_classes = int(opt.num_classes)\n",
    "batch_size = opt.batchSize\n",
    "\n",
    "netG = Generator(ngpu, latent_dim=nz, ngf=opt.nf, img_channels=img_channels)\n",
    "netG.apply(initialize_weights)\n",
    "netD = Discriminator(ngpu, latent_dim=nz, ndf=opt.nf, img_channels=img_channels,  dropout_prob=opt.drop)\n",
    "netD.apply(initialize_weights)\n",
    "# send to GPU\n",
    "netD.to(device)\n",
    "netG.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code snippet initializes the training and test datasets with occlusions using the **`get_dataset`** function. It passes various parameters such as the dataset name, data root, image size, training flag, drop rate, and tile size.\n",
    "- The **`get_dataset`** function returns the dataset, a normalization function (**`unorm`**), and the number of image channels. These values are assigned to the variables **`dataset`**, **`unorm`**, and **`img_channels`**, respectively.\n",
    "- Data loaders are then created for the training and test datasets using the **`torch.utils.data.DataLoader`** class. These data loaders handle the loading of data in batches during training and testing.\n",
    "- Several hyperparameters such as the number of GPUs to use (**`ngpu`**), size of the input latent vector (**`nz`**), number of classes for classification (**`num_classes`**), and batch size (**`batch_size`**) are defined.\n",
    "- Instances of the Generator and Discriminator models are created (**`netG`** and **`netD`**, respectively). The parameters passed to these models include the number of GPUs, latent dimension, number of generator filters (**`ngf`**), number of discriminator filters (**`ndf`**), image channels, and dropout probability (**`dropout_prob`**).\n",
    "- The **`apply`** method is used to apply weight initialization (**`initialize_weights`**) to the Generator and Discriminator models.\n",
    "- Finally, the Generator and Discriminator models are sent to the specified device (GPU) using the **`to`** method.\n",
    "\n",
    "Overall, this code sets up the necessary components for training and testing a generative adversarial network (GAN). It initializes the datasets, creates data loaders, defines model hyperparameters, creates instances of the Generator and Discriminator models, applies weight initialization, and sends the models to the GPU for accelerated computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "Start training from loaded model...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(dir_checkpoint+'/trained.pth'):\n",
    "    # Load data from last checkpoint\n",
    "    print('Loading pre-trained model...')\n",
    "    checkpoint = torch.load(dir_checkpoint+'/trained.pth', map_location='cpu')\n",
    "    netG.load_state_dict(checkpoint['generator'])\n",
    "    netD.load_state_dict(checkpoint['discriminator'])\n",
    "    d_losses = checkpoint.get('d_losses', [float('inf')])\n",
    "    g_losses = checkpoint.get('g_losses', [float('inf')])\n",
    "    r_losses = checkpoint.get('r_losses', [float('inf')])\n",
    "    print('Start training from loaded model...')\n",
    "else:\n",
    "    print('No pre-trained model detected, restart training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accuracies found...\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "if os.path.exists(dir_files+'/' + acc_file):\n",
    "    # Load data from last checkpoint\n",
    "    print('Loading accuracies...')\n",
    "    checkpoint = torch.load(dir_files+'/'+acc_file, map_location='cpu')\n",
    "    train_accuracies = checkpoint.get('train_accuracies', [float('inf')])\n",
    "    test_accuracies = checkpoint.get('test_accuracies', [float('inf')])\n",
    "    train_losses = checkpoint.get('train_losses', [float('inf')])\n",
    "    test_losses = checkpoint.get('test_losses', [float('inf')])\n",
    "else:\n",
    "    print('No accuracies found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing training representations ...\n",
      "Storing validation representations ...\n"
     ]
    }
   ],
   "source": [
    "n_epochs_c = opt.niterC\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Storing training representations ...\")\n",
    "image, label = next(iter(train_dataloader))\n",
    "image, label = image.to(device), label.to(device)\n",
    "netD.eval()\n",
    "with torch.no_grad():\n",
    "    latent_output, _ = netD(image)\n",
    "    train_features = latent_output.cpu()\n",
    "    train_labels = label.cpu().long()\n",
    "\n",
    "print(\"Storing validation representations ...\")\n",
    "image, label = next(iter(test_dataloader))\n",
    "image, label = image.to(device), label.to(device)\n",
    "netD.eval()\n",
    "with torch.no_grad():\n",
    "    latent_output, _ = netD(image)\n",
    "    test_features = latent_output.cpu()\n",
    "    test_labels = label.cpu().long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained classifier...\n",
      "Use trained classifier...\n"
     ]
    }
   ],
   "source": [
    "# create dataset of latent activities\n",
    "linear_train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "linear_test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "linear_train_loader = torch.utils.data.DataLoader(linear_train_dataset, batch_size=batch_size, shuffle=True, num_workers=opt.workers, drop_last=True)\n",
    "linear_test_loader = torch.utils.data.DataLoader(linear_test_dataset, batch_size=batch_size, shuffle=False, num_workers=opt.workers, drop_last=True)\n",
    "\n",
    "classifier = OutputClassifier(nz, num_classes=num_classes)\n",
    "classifier.to(device)\n",
    "optimizerC = optim.SGD(classifier.parameters(), lr=opt.lrC)\n",
    "\n",
    "if os.path.exists(dir_checkpoint + '/trained_classifier.pth'):\n",
    "    # Load data from last checkpoint\n",
    "    print('Loading trained classifier...')\n",
    "    checkpoint = torch.load(dir_checkpoint + '/trained_classifier.pth', map_location='cpu')\n",
    "    classifier.load_state_dict(checkpoint['classifier'])\n",
    "    print('Use trained classifier...')\n",
    "else:\n",
    "    print('No trained classifier detecte...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on train set...\n",
      "testing on test set...\n",
      "[20/20]  train_loss: 0.3402  test_loss: 0.3792  train_acc: 87.7421  test_acc: 86.0677\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store training and testing metrics\n",
    "store_train_acc = []\n",
    "store_test_acc = []\n",
    "store_train_loss = []\n",
    "store_test_loss = []\n",
    "\n",
    "print(\"training on train set...\")\n",
    "\n",
    "# Iterate over batches of data in the training set\n",
    "for feature, label in linear_train_loader:\n",
    "    feature, label = feature.to(device), label.to(device)\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    class_output = classifier(feature)  # Forward pass: compute the output of the classifier\n",
    "    class_err = class_criterion(class_output, label)  # Compute the classification error\n",
    "    # Store training metrics\n",
    "    train_acc = compute_acc(class_output, label)  # Compute the training accuracy\n",
    "    store_train_acc.append(train_acc)  # Append the training accuracy to the list\n",
    "    store_train_loss.append(class_err.item())  # Append the training loss to the list\n",
    "\n",
    "print(\"testing on test set...\")\n",
    "# Iterate over batches of data in the testing set\n",
    "for feature, label in linear_test_loader:\n",
    "    feature, label = feature.to(device), label.to(device)\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    class_output = classifier(feature)  # Forward pass: compute the output of the classifier\n",
    "    class_err = class_criterion(class_output, label)  # Compute the classification error\n",
    "    # Store testing metrics\n",
    "    test_acc = compute_acc(class_output, label)  # Compute the testing accuracy\n",
    "    store_test_acc.append(test_acc)  # Append the testing accuracy to the list\n",
    "    store_test_loss.append(class_err.item())  # Append the testing loss to the list\n",
    "\n",
    "# Print the average training and testing metrics for the current epoch\n",
    "print('[%d/%d]  train_loss: %.4f  test_loss: %.4f  train_acc: %.4f  test_acc: %.4f'\n",
    "      % (opt.niterC, n_epochs_c, np.mean(store_train_loss), np.mean(store_test_loss),\n",
    "         np.mean(store_train_acc), np.mean(store_test_acc)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code initializes empty lists to store the training and testing metrics, including training accuracy, testing accuracy, training loss, and testing loss.\n",
    "- It then enters a loop to train the linear classifier for each epoch (**`opt.niterC`** is the total number of epochs).\n",
    "- Inside the loop, it first performs training on the training set.\n",
    "    - It iterates over batches of data in the **`linear_train_loader`**.\n",
    "    - The input features and labels are moved to the specified device (GPU) if available.\n",
    "    - The classifier is set to evaluation mode using **`classifier.eval()`** to ensure that any layers like dropout behave correctly during evaluation.\n",
    "    - The forward pass is performed by passing the features through the classifier (**`class_output = classifier(feature)`**).\n",
    "    - The classification error is computed using the specified loss criterion (**`class_err = class_criterion(class_output, label)`**).\n",
    "    - The training accuracy is calculated by calling the **`compute_acc`** function, which compares the predicted labels with the ground truth labels.\n",
    "    - The training accuracy and loss are then appended to their respective lists.\n",
    "- After training on the training set, the code proceeds to evaluate the model on the testing set.\n",
    "    - It follows a similar procedure as the training loop, iterating over batches of data in the **`linear_test_loader`** and computing the testing accuracy and loss.\n",
    "- Finally, the code prints the average training and testing metrics for the current epoch, including the training loss, testing loss, training accuracy, and testing accuracy.\n",
    "\n",
    "The code essentially trains a linear classifier using the features extracted from the latent vectors obtained from a GAN. It performs forward passes on both the training and testing datasets, calculates the classification errors, and stores the accuracy and loss metrics for analysis. This allows monitoring the classifier's performance and evaluating how well it generalizes to unseen data during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies successfully saved.\n"
     ]
    }
   ],
   "source": [
    "# Append the average training metrics to the respective lists\n",
    "train_accuracies.append(np.mean(store_train_acc))\n",
    "train_losses.append(np.mean(store_train_loss))\n",
    "\n",
    "# Append the average testing metrics to the respective lists\n",
    "test_accuracies.append(np.mean(store_test_acc))\n",
    "test_losses.append(np.mean(store_test_loss))\n",
    "\n",
    "# Save the average metrics to a file\n",
    "torch.save({\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'test_accuracies': test_accuracies,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "}, dir_files + '/' + acc_file)\n",
    "print(f'Accuracies successfully saved.')\n",
    "\n",
    "# Create a figure for plotting\n",
    "e = np.arange(0, len(train_accuracies))\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Add subplot for loss plot\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(e, train_losses, label='train loss')\n",
    "ax1.plot(e, test_losses, label='test loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_title('losses with uns. training')\n",
    "ax1.legend()\n",
    "\n",
    "# Add subplot for accuracy plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(e, train_accuracies, label='train acc')\n",
    "ax2.plot(e, test_accuracies, label='test acc')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('accuracy (%)')\n",
    "ax2.set_title('accuracy with uns. training')\n",
    "ax2.legend()\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "fig.savefig(dir_files + '/linear_classif_occ.pdf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code appends the average training and testing metrics (accuracy and loss) obtained during the current epoch to their respective lists.\n",
    "- It then saves the accumulated metrics (lists) to a file using the **`torch.save`** function. The metrics are saved as a dictionary with keys such as **`'train_accuracies'`**, **`'test_accuracies'`**, **`'train_losses'`**, and **`'test_losses'`**. The saved file path is determined by **`dir_files + '/' + acc_file`**.\n",
    "- A figure is created using **`plt.figure(figsize=(10, 5))`** to plot the metrics.\n",
    "- The figure is divided into two subplots: one for the loss plot and the other for the accuracy plot.\n",
    "- In the loss subplot (**`ax1`**), the training and testing losses are plotted against the number of epochs (**`e`**). Labels and titles are set, and a legend is added.\n",
    "- In the accuracy subplot (**`ax2`**), the training and testing accuracies are plotted against the number of epochs. The y-axis limit is set to 0-100 to represent percentages. Similar to the loss subplot, labels, title, and legend are added.\n",
    "- Finally, the figure is saved as a PDF file using **`fig.savefig(dir_files + '/linear_classif_occ.pdf')`**.\n",
    "\n",
    "The code essentially accumulates the average training and testing metrics over multiple epochs and saves them for analysis. Additionally, it plots the training and testing losses as well as the training and testing accuracies over the epochs, providing a visual representation of the model's performance during unsupervised training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
