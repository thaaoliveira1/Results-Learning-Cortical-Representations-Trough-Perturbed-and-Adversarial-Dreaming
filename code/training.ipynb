{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # Ensures print function compatibility with Python 2.x\n",
    "import argparse  # Library for parsing command-line arguments\n",
    "import os  # Library for interacting with the operating system\n",
    "import copy  # Library for creating object copies\n",
    "import numpy as np  # Numerical computing library\n",
    "import random  # Library for generating random numbers\n",
    "\n",
    "import torch  # Core library for PyTorch\n",
    "import torch.nn as nn  # Library for defining neural network components\n",
    "import torch.nn.parallel  # Library for parallelizing operations on multiple GPUs\n",
    "import torch.backends.cudnn as cudnn  # Interface to the cuDNN library for GPU optimizations\n",
    "import torch.optim as optim  # Library for optimization algorithms\n",
    "import torch.utils.data  # Tools for working with datasets in PyTorch\n",
    "import torchvision.datasets as dset  # Datasets provided by torchvision\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing\n",
    "import torchvision.utils as vutils  # Utility functions for visualizing images\n",
    "from torch.autograd import Variable  # Provides automatic differentiation for tensors\n",
    "import torch.nn.functional as F  # Library for various activation functions and loss functions\n",
    "\n",
    "from functions import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(imageSize=32, dataset='cifar10', dataroot='./datasets/', num_workers=2, is_continue=1, batch_size=64, image_size=32, latent_size=256, num_epochs=55, weight_cycle_consistency=1.0, W=1.0, N=1.0, R=1.0, epsilon=0.0, num_filters=64, dropout_prob=0.0, learning_rate_generator=0.0002, learning_rate_discriminator=0.0002, beta1=0.5, lmbd=0.5, num_gpus=1, output_folder='output', gpu_id='0', outf='output', niter=55)\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import argparse\n",
    "\n",
    "# Creating an argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Adding arguments with their default values and descriptions\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\n",
    "parser.add_argument('--dataset', default='cifar10', help='Dataset to use: cifar10 | imagenet | mnist')\n",
    "parser.add_argument('--dataroot', default='./datasets/', help='Path to the dataset')\n",
    "parser.add_argument('--num_workers', type=int, help='Number of data loading workers', default=2)\n",
    "parser.add_argument('--is_continue', type=int, default=1, help='Use pre-trained model')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Input batch size')\n",
    "parser.add_argument('--image_size', type=int, default=32, help='Height/width of the input image to the network')\n",
    "parser.add_argument('--latent_size', type=int, default=256, help='Size of the latent vector')\n",
    "parser.add_argument('--num_epochs', type=int, default=55, help='Number of epochs to train for')\n",
    "parser.add_argument('--weight_cycle_consistency', type=float, default=1.0, help='Weight of Cycle Consistency')\n",
    "parser.add_argument('--W', type=float, default=1.0, help='Wake')\n",
    "parser.add_argument('--N', type=float, default=1.0, help='NREM')\n",
    "parser.add_argument('--R', type=float, default=1.0, help='REM')\n",
    "parser.add_argument('--epsilon', type=float, default=0.0, help='Amount of noise in the wake latent space')\n",
    "parser.add_argument('--num_filters', type=int, default=64, help='Filters factor')\n",
    "parser.add_argument('--dropout_prob', type=float, default=0.0, help='Probability of dropout')\n",
    "parser.add_argument('--learning_rate_generator', type=float, default=0.0002, help='Learning rate for the generator')\n",
    "parser.add_argument('--learning_rate_discriminator', type=float, default=0.0002, help='Learning rate for the discriminator')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='Beta1 for Adam optimizer')\n",
    "parser.add_argument('--lmbd', type=float, default=0.5, help='convex combination factor for REM')\n",
    "parser.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use')\n",
    "parser.add_argument('--output_folder', default='output', help='Folder to output images and model checkpoints')\n",
    "parser.add_argument('--gpu_id', type=str, default='0', help='The ID of the specified GPU')\n",
    "parser.add_argument('--outf', default='output', help='folder to output images and model checkpoints')\n",
    "\n",
    "\n",
    "\n",
    "# Parsing the command-line arguments\n",
    "opt, unknown = parser.parse_known_args()\n",
    "\n",
    "# Set the number of iterations to the number of epochs\n",
    "opt.niter = opt.num_epochs\n",
    "\n",
    "# Assign the value of latent_size based on opt.latent_size\n",
    "latent_size = opt.latent_size\n",
    "\n",
    "# Printing the parsed arguments\n",
    "print(opt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. The code begins by importing the necessary library, **`argparse`**, which provides a way to parse command-line arguments.\n",
    "\n",
    "2. An argument parser object is created using **`argparse.ArgumentParser()`**. This object will handle the parsing of command-line arguments.\n",
    "\n",
    "3. Various arguments are added to the parser using the **`add_argument()`** method. Each argument has a unique name, a default value, and a description.\n",
    "\n",
    "4. The command-line arguments are parsed using **`parser.parse_known_args()`**, which returns two values: **`opt`** (containing the parsed argument values) and **`unknown`** (containing any unrecognized arguments).\n",
    "\n",
    "5. The parsed argument values are stored in the **`opt`** object.\n",
    "\n",
    "6. Finally, the values of the parsed arguments are printed using **`print(opt)`**.\n",
    "\n",
    "This code allows you to run the program with different options and values from the command line. Each option represents a specific setting or parameter that can be customized. The **`argparse`** library handles the parsing of these options, and the **`opt`** object stores the values for further use within the program. Printing the **`opt`** object provides a summary of the parsed argument values, allowing you to verify the settings before running the main logic of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the GPU ID if using only 1 GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_id\n",
    "\n",
    "# where to save samples and training curves\n",
    "dir_files = './results/'+opt.dataset+'/'+opt.outf\n",
    "# where to save model\n",
    "dir_checkpoint = './checkpoints/'+opt.dataset+'/'+opt.outf\n",
    "\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "try:\n",
    "    os.makedirs(dir_files)\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs(dir_checkpoint)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset and get relevant information\n",
    "dataset, unorm, img_channels = get_dataset(opt.dataset, opt.dataroot, opt.imageSize)\n",
    "\n",
    "# Create a data loader for loading the dataset in batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True,\n",
    "                                         num_workers=int(opt.num_workers), drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code sets the environment variable **`CUDA_VISIBLE_DEVICES`** to the GPU ID specified in **`opt.gpu_id`**. This ensures that only the specified GPU is used when running the program.\n",
    "\n",
    "2. The code defines two directory paths: **`dir_files`** for saving samples and training curves, and **`dir_checkpoint`** for saving models.\n",
    "\n",
    "3. The code attempts to create the directories specified by **`dir_files`** and **`dir_checkpoint`**. If the directories already exist, the **`OSError`** exception is caught and passed.\n",
    "\n",
    "4. The code checks if CUDA is available and assigns the device accordingly. If CUDA is available, the device is set to the GPU with ID 0; otherwise, it is set to CPU.\n",
    "\n",
    "5. The code calls the **`get_dataset()`** function to load the dataset specified in **`opt.dataset`** from the directory **`opt.dataroot`**, and also obtains the **`unorm`** (normalization) and **`img_channels`** (number of image channels) values.\n",
    "\n",
    "6. Finally, the code creates a **`DataLoader`** object called **`dataloader`**, which allows iterating over the dataset in batches. The batch size is specified by **`opt.batchSize`**, and the data is shuffled and loaded in parallel using **`opt.num_workers`** worker processes. The **`drop_last`** option ensures that the last incomplete batch is dropped if its size is less than **`opt.batchSize`**.\n",
    "\n",
    "This code prepares the necessary setup before training the neural network, such as configuring the device, setting up directories for saving results, and loading the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (dis): Sequential(\n",
       "    (0): Conv2d(256, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and assign values to hyperparameters\n",
    "num_gpus = int(opt.num_gpus)\n",
    "latent_dim = int(opt.latent_size)\n",
    "batch_size = opt.batch_size\n",
    "\n",
    "# Instantiate generator and discriminator networks\n",
    "generator = Generator(num_gpus, latent_dim=latent_dim, ngf=opt.num_filters, img_channels=img_channels)\n",
    "generator.apply(initialize_weights)\n",
    "discriminator = Discriminator(num_gpus, latent_dim=latent_dim, ndf=opt.num_filters, img_channels=img_channels, dropout_prob=opt.dropout_prob)\n",
    "discriminator.apply(initialize_weights)\n",
    "\n",
    "\n",
    "# Move networks to the GPU\n",
    "generator.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code sets up some hyperparameters, such as the number of GPUs available (num_gpus), the size of the latent space (latent_dim), and the batch size (batch_size).\n",
    "\n",
    "2. Two neural network models are instantiated: the generator (named \"generator\") and the discriminator (named \"discriminator\").\n",
    "\n",
    "3. The generator is an instance of the Generator class, which takes the number of GPUs, latent dimension, number of filters, and number of channels as arguments.\n",
    "\n",
    "4. The discriminator is an instance of the Discriminator class, which takes similar arguments as the generator along with a dropout probability.\n",
    "\n",
    "5. Weight initialization is applied to both the generator and discriminator using the weights_init function.\n",
    "\n",
    "6. The generator and discriminator models are moved to the specified device (e.g., GPU) using the to() method.\n",
    "\n",
    "7. This ensures that the computations for these models will be performed on the GPU if available, which can significantly speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers for discriminator and generator\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=opt.learning_rate_discriminator, betas=(opt.beta1, 0.999))\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=opt.learning_rate_generator, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# Initialize lists to store losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "r_losses_real = []\n",
    "r_losses_fake = []\n",
    "kl_losses = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up optimizers for the discriminator and generator models.\n",
    "\n",
    "1. The discriminator_optimizer uses the Adam optimizer and takes the discriminator parameters, learning rate (lrD), and beta values as arguments.\n",
    "\n",
    "2. The generator_optimizer uses the Adam optimizer and takes the generator parameters, learning rate (lrG), and beta values as arguments.\n",
    "\n",
    "3. Lists are initialized to store various types of losses during training.\n",
    "\n",
    "4. **discriminator_losses:** Stores the losses of the discriminator model.\n",
    "\n",
    "5. **generator_losses:** Stores the losses of the generator model.\n",
    "\n",
    "6. **real_losses:** Stores losses related to real images.\n",
    "\n",
    "7. **fake_losses:** Stores losses related to fake/generated images.\n",
    "\n",
    "8. **kl_losses:** Stores Kullback-Leibler divergence losses, which are often used in variational autoencoders (VAEs) or other generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model detected, restart training...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(dir_checkpoint+'/trained.pth') and opt.is_continue:\n",
    "    # Load data from last checkpoint\n",
    "    print('Loading pre-trained model...')\n",
    "    checkpoint = torch.load(dir_checkpoint+'/trained.pth', map_location=torch.device('cpu'))\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "    generator_optimizer.load_state_dict(checkpoint['g_optim'])\n",
    "    discriminator_optimizer.load_state_dict(checkpoint['d_optim'])\n",
    "    d_losses = checkpoint.get('d_losses', [float('inf')])\n",
    "    g_losses = checkpoint.get('g_losses', [float('inf')])\n",
    "    r_losses_real = checkpoint.get('r_losses_real', [float('inf')])\n",
    "    r_losses_fake = checkpoint.get('r_losses_fake', [float('inf')])\n",
    "    kl_losses = checkpoint.get('kl_losses', [float('inf')])\n",
    "    print('Start training from loaded model...')\n",
    "else:\n",
    "    print('No pre-trained model detected, restart training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions\n",
    "discriminator_criterion = nn.BCELoss()  # Binary Cross Entropy Loss for the discriminator\n",
    "reconstruction_criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "\n",
    "# Create tensor placeholders\n",
    "discriminator_label = torch.zeros(opt.batch_size, dtype=torch.float32, device=device)\n",
    "real_label_value = 1.0\n",
    "fake_label_value = 0\n",
    "\n",
    "evaluation_noise = torch.randn(batch_size, latent_dim, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "1. The code defines two loss functions: **`discriminator_criterion`** and **`reconstruction_criterion`**. The **`BCELoss`** (Binary Cross Entropy Loss) is used for the discriminator, and the **`MSELoss`** (Mean Squared Error Loss) is used for reconstruction.\n",
    "\n",
    "2. The variables **`dis_criterion`** and **`rec_criterion`** are changed to **`discriminator_criterion`** and **`reconstruction_criterion`**, respectively. \n",
    "\n",
    "3. The variable names **`dis_label`**, **`real_label_value`**, **`fake_label_value`**, and **`eval_noise`** are changed to **`discriminator_label`**, **`real_label_value`**, **`fake_label_value`**, and **`evaluation_noise`**, respectively.\n",
    "\n",
    "4. The order and structure of the code remain unchanged as they are necessary for defining the loss functions and creating tensor placeholders.\n",
    "\n",
    "Overall, this code snippet defines the loss functions for the discriminator and reconstruction tasks. The **`BCELoss`** is commonly used for binary classification tasks, such as determining whether an image is real or fake. The **`MSELoss`** is used for measuring the pixel-wise difference between the input and reconstructed images. The tensor placeholders are created to hold the discriminator labels, real and fake label values, and noise for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/55][200/781]  Loss_D: 1.3467  Loss_G: -0.6727  Loss_R_real: 0.2688  Loss_R_fake: 0.2570  D(x): 0.5347  D(G(z)): 0.4669  latent_norm : 10.0325  \n",
      "[0/55][400/781]  Loss_D: 1.3450  Loss_G: -0.6666  Loss_R_real: 0.2313  Loss_R_fake: 0.2713  D(x): 0.5073  D(G(z)): 0.4914  latent_norm : 16.5589  \n",
      "[0/55][600/781]  Loss_D: 1.3550  Loss_G: -0.6723  Loss_R_real: 0.1971  Loss_R_fake: 0.2669  D(x): 0.5128  D(G(z)): 0.5076  latent_norm : 13.7060  \n",
      "Model successfully saved.\n",
      "[1/55][200/781]  Loss_D: 1.3902  Loss_G: -0.6933  Loss_R_real: 0.1193  Loss_R_fake: 0.2105  D(x): 0.5132  D(G(z)): 0.4835  latent_norm : 13.5868  \n",
      "[1/55][400/781]  Loss_D: 1.3901  Loss_G: -0.6950  Loss_R_real: 0.1099  Loss_R_fake: 0.2131  D(x): 0.5085  D(G(z)): 0.5121  latent_norm : 13.9264  \n",
      "[1/55][600/781]  Loss_D: 1.3901  Loss_G: -0.6949  Loss_R_real: 0.1049  Loss_R_fake: 0.2105  D(x): 0.4934  D(G(z)): 0.5083  latent_norm : 15.3234  \n",
      "Model successfully saved.\n",
      "[2/55][200/781]  Loss_D: 1.3835  Loss_G: -0.6921  Loss_R_real: 0.0926  Loss_R_fake: 0.1721  D(x): 0.4943  D(G(z)): 0.5045  latent_norm : 15.1864  \n",
      "[2/55][400/781]  Loss_D: 1.3817  Loss_G: -0.6918  Loss_R_real: 0.0939  Loss_R_fake: 0.2024  D(x): 0.5221  D(G(z)): 0.5166  latent_norm : 13.1819  \n",
      "[2/55][600/781]  Loss_D: 1.3850  Loss_G: -0.6936  Loss_R_real: 0.0933  Loss_R_fake: 0.2053  D(x): 0.5095  D(G(z)): 0.5166  latent_norm : 14.6194  \n",
      "Model successfully saved.\n",
      "[3/55][200/781]  Loss_D: 1.3907  Loss_G: -0.6957  Loss_R_real: 0.0846  Loss_R_fake: 0.2052  D(x): 0.5068  D(G(z)): 0.5080  latent_norm : 15.0394  \n",
      "[3/55][400/781]  Loss_D: 1.3925  Loss_G: -0.6958  Loss_R_real: 0.0839  Loss_R_fake: 0.2254  D(x): 0.4967  D(G(z)): 0.4934  latent_norm : 13.8706  \n",
      "[3/55][600/781]  Loss_D: 1.3923  Loss_G: -0.6948  Loss_R_real: 0.0829  Loss_R_fake: 0.2276  D(x): 0.4934  D(G(z)): 0.4914  latent_norm : 14.0510  \n",
      "Model successfully saved.\n",
      "[4/55][200/781]  Loss_D: 1.3872  Loss_G: -0.6937  Loss_R_real: 0.0796  Loss_R_fake: 0.2629  D(x): 0.4978  D(G(z)): 0.4772  latent_norm : 13.7779  \n",
      "[4/55][400/781]  Loss_D: 1.3848  Loss_G: -0.6906  Loss_R_real: 0.0800  Loss_R_fake: 0.2503  D(x): 0.5067  D(G(z)): 0.5040  latent_norm : 14.0606  \n",
      "[4/55][600/781]  Loss_D: 1.3832  Loss_G: -0.6894  Loss_R_real: 0.0806  Loss_R_fake: 0.2493  D(x): 0.4855  D(G(z)): 0.5048  latent_norm : 14.9917  \n",
      "Model successfully saved.\n",
      "[5/55][200/781]  Loss_D: 1.3766  Loss_G: -0.6834  Loss_R_real: 0.0812  Loss_R_fake: 0.2286  D(x): 0.4890  D(G(z)): 0.5055  latent_norm : 16.5280  \n",
      "[5/55][400/781]  Loss_D: 1.3749  Loss_G: -0.6823  Loss_R_real: 0.0813  Loss_R_fake: 0.2125  D(x): 0.5226  D(G(z)): 0.5063  latent_norm : 14.0902  \n",
      "[5/55][600/781]  Loss_D: 1.3754  Loss_G: -0.6813  Loss_R_real: 0.0812  Loss_R_fake: 0.2167  D(x): 0.5100  D(G(z)): 0.4471  latent_norm : 13.4878  \n",
      "Model successfully saved.\n",
      "[6/55][200/781]  Loss_D: 1.3794  Loss_G: -0.6846  Loss_R_real: 0.0773  Loss_R_fake: 0.2245  D(x): 0.5007  D(G(z)): 0.4828  latent_norm : 15.3325  \n",
      "[6/55][400/781]  Loss_D: 1.3773  Loss_G: -0.6824  Loss_R_real: 0.0767  Loss_R_fake: 0.2318  D(x): 0.5253  D(G(z)): 0.4946  latent_norm : 15.7223  \n",
      "[6/55][600/781]  Loss_D: 1.3770  Loss_G: -0.6821  Loss_R_real: 0.0763  Loss_R_fake: 0.2340  D(x): 0.5070  D(G(z)): 0.4799  latent_norm : 14.2728  \n",
      "Model successfully saved.\n",
      "[7/55][200/781]  Loss_D: 1.3708  Loss_G: -0.6784  Loss_R_real: 0.0757  Loss_R_fake: 0.2259  D(x): 0.5010  D(G(z)): 0.4728  latent_norm : 14.6447  \n",
      "[7/55][400/781]  Loss_D: 1.3729  Loss_G: -0.6798  Loss_R_real: 0.0747  Loss_R_fake: 0.2363  D(x): 0.5040  D(G(z)): 0.4788  latent_norm : 14.6781  \n",
      "[7/55][600/781]  Loss_D: 1.3731  Loss_G: -0.6795  Loss_R_real: 0.0742  Loss_R_fake: 0.2356  D(x): 0.5082  D(G(z)): 0.4882  latent_norm : 15.6060  \n",
      "Model successfully saved.\n",
      "[8/55][200/781]  Loss_D: 1.3737  Loss_G: -0.6796  Loss_R_real: 0.0713  Loss_R_fake: 0.2435  D(x): 0.4901  D(G(z)): 0.4863  latent_norm : 14.1231  \n",
      "[8/55][400/781]  Loss_D: 1.3725  Loss_G: -0.6794  Loss_R_real: 0.0710  Loss_R_fake: 0.2417  D(x): 0.4976  D(G(z)): 0.4966  latent_norm : 14.4981  \n",
      "[8/55][600/781]  Loss_D: 1.3726  Loss_G: -0.6796  Loss_R_real: 0.0708  Loss_R_fake: 0.2377  D(x): 0.5447  D(G(z)): 0.5122  latent_norm : 15.0821  \n",
      "Model successfully saved.\n",
      "[9/55][200/781]  Loss_D: 1.3672  Loss_G: -0.6761  Loss_R_real: 0.0684  Loss_R_fake: 0.2404  D(x): 0.5066  D(G(z)): 0.4823  latent_norm : 15.9237  \n",
      "[9/55][400/781]  Loss_D: 1.3667  Loss_G: -0.6763  Loss_R_real: 0.0688  Loss_R_fake: 0.2468  D(x): 0.5014  D(G(z)): 0.4759  latent_norm : 14.3801  \n",
      "[9/55][600/781]  Loss_D: 1.3657  Loss_G: -0.6763  Loss_R_real: 0.0686  Loss_R_fake: 0.2296  D(x): 0.5039  D(G(z)): 0.4825  latent_norm : 14.2464  \n",
      "Model successfully saved.\n",
      "[10/55][200/781]  Loss_D: 1.3563  Loss_G: -0.6722  Loss_R_real: 0.0672  Loss_R_fake: 0.2315  D(x): 0.5101  D(G(z)): 0.4811  latent_norm : 16.4672  \n",
      "[10/55][400/781]  Loss_D: 1.3572  Loss_G: -0.6721  Loss_R_real: 0.0672  Loss_R_fake: 0.2320  D(x): 0.5044  D(G(z)): 0.5001  latent_norm : 14.8034  \n",
      "[10/55][600/781]  Loss_D: 1.3560  Loss_G: -0.6714  Loss_R_real: 0.0672  Loss_R_fake: 0.2387  D(x): 0.5176  D(G(z)): 0.4867  latent_norm : 15.0955  \n",
      "Model successfully saved.\n",
      "[11/55][200/781]  Loss_D: 1.3422  Loss_G: -0.6656  Loss_R_real: 0.0663  Loss_R_fake: 0.2539  D(x): 0.5083  D(G(z)): 0.4736  latent_norm : 15.8530  \n",
      "[11/55][400/781]  Loss_D: 1.3428  Loss_G: -0.6647  Loss_R_real: 0.0663  Loss_R_fake: 0.2386  D(x): 0.5255  D(G(z)): 0.4798  latent_norm : 16.6757  \n",
      "[11/55][600/781]  Loss_D: 1.3405  Loss_G: -0.6638  Loss_R_real: 0.0663  Loss_R_fake: 0.2466  D(x): 0.5389  D(G(z)): 0.4667  latent_norm : 15.7641  \n",
      "Model successfully saved.\n",
      "[12/55][200/781]  Loss_D: 1.3308  Loss_G: -0.6592  Loss_R_real: 0.0664  Loss_R_fake: 0.2432  D(x): 0.5283  D(G(z)): 0.4943  latent_norm : 14.9541  \n",
      "[12/55][400/781]  Loss_D: 1.3295  Loss_G: -0.6583  Loss_R_real: 0.0663  Loss_R_fake: 0.2560  D(x): 0.5206  D(G(z)): 0.4705  latent_norm : 15.3358  \n",
      "[12/55][600/781]  Loss_D: 1.3287  Loss_G: -0.6578  Loss_R_real: 0.0661  Loss_R_fake: 0.2500  D(x): 0.5327  D(G(z)): 0.4632  latent_norm : 14.7886  \n",
      "Model successfully saved.\n",
      "[13/55][200/781]  Loss_D: 1.3193  Loss_G: -0.6523  Loss_R_real: 0.0661  Loss_R_fake: 0.2584  D(x): 0.5124  D(G(z)): 0.4691  latent_norm : 15.2724  \n",
      "[13/55][400/781]  Loss_D: 1.3183  Loss_G: -0.6517  Loss_R_real: 0.0669  Loss_R_fake: 0.2709  D(x): 0.4954  D(G(z)): 0.4527  latent_norm : 15.1254  \n",
      "[13/55][600/781]  Loss_D: 1.3195  Loss_G: -0.6522  Loss_R_real: 0.0666  Loss_R_fake: 0.2583  D(x): 0.5088  D(G(z)): 0.4502  latent_norm : 14.1613  \n",
      "Model successfully saved.\n",
      "[14/55][200/781]  Loss_D: 1.3185  Loss_G: -0.6512  Loss_R_real: 0.0652  Loss_R_fake: 0.2685  D(x): 0.4843  D(G(z)): 0.4527  latent_norm : 14.0208  \n",
      "[14/55][400/781]  Loss_D: 1.3217  Loss_G: -0.6530  Loss_R_real: 0.0656  Loss_R_fake: 0.2696  D(x): 0.5223  D(G(z)): 0.4895  latent_norm : 15.9090  \n",
      "[14/55][600/781]  Loss_D: 1.3219  Loss_G: -0.6530  Loss_R_real: 0.0655  Loss_R_fake: 0.2609  D(x): 0.5043  D(G(z)): 0.4797  latent_norm : 14.8032  \n",
      "Model successfully saved.\n",
      "[15/55][200/781]  Loss_D: 1.3179  Loss_G: -0.6518  Loss_R_real: 0.0636  Loss_R_fake: 0.2465  D(x): 0.5077  D(G(z)): 0.4664  latent_norm : 15.0127  \n",
      "[15/55][400/781]  Loss_D: 1.3159  Loss_G: -0.6510  Loss_R_real: 0.0636  Loss_R_fake: 0.2544  D(x): 0.5383  D(G(z)): 0.4742  latent_norm : 16.5927  \n",
      "[15/55][600/781]  Loss_D: 1.3166  Loss_G: -0.6511  Loss_R_real: 0.0640  Loss_R_fake: 0.2536  D(x): 0.5019  D(G(z)): 0.4677  latent_norm : 14.7662  \n",
      "Model successfully saved.\n",
      "[16/55][200/781]  Loss_D: 1.3042  Loss_G: -0.6445  Loss_R_real: 0.0629  Loss_R_fake: 0.2665  D(x): 0.5280  D(G(z)): 0.4393  latent_norm : 14.9790  \n",
      "[16/55][400/781]  Loss_D: 1.3060  Loss_G: -0.6458  Loss_R_real: 0.0632  Loss_R_fake: 0.2564  D(x): 0.5283  D(G(z)): 0.4925  latent_norm : 14.4509  \n",
      "[16/55][600/781]  Loss_D: 1.3057  Loss_G: -0.6455  Loss_R_real: 0.0631  Loss_R_fake: 0.2565  D(x): 0.5374  D(G(z)): 0.4759  latent_norm : 14.7857  \n",
      "Model successfully saved.\n",
      "[17/55][200/781]  Loss_D: 1.2951  Loss_G: -0.6412  Loss_R_real: 0.0621  Loss_R_fake: 0.2664  D(x): 0.5397  D(G(z)): 0.4506  latent_norm : 15.0848  \n",
      "[17/55][400/781]  Loss_D: 1.2988  Loss_G: -0.6426  Loss_R_real: 0.0625  Loss_R_fake: 0.2693  D(x): 0.5131  D(G(z)): 0.4752  latent_norm : 15.0079  \n",
      "[17/55][600/781]  Loss_D: 1.2978  Loss_G: -0.6415  Loss_R_real: 0.0626  Loss_R_fake: 0.2687  D(x): 0.5138  D(G(z)): 0.4533  latent_norm : 16.4723  \n",
      "Model successfully saved.\n",
      "[18/55][200/781]  Loss_D: 1.2882  Loss_G: -0.6375  Loss_R_real: 0.0624  Loss_R_fake: 0.2801  D(x): 0.5441  D(G(z)): 0.4564  latent_norm : 16.1753  \n",
      "[18/55][400/781]  Loss_D: 1.2851  Loss_G: -0.6354  Loss_R_real: 0.0621  Loss_R_fake: 0.2819  D(x): 0.5194  D(G(z)): 0.4659  latent_norm : 15.3314  \n",
      "[18/55][600/781]  Loss_D: 1.2862  Loss_G: -0.6356  Loss_R_real: 0.0622  Loss_R_fake: 0.2867  D(x): 0.5019  D(G(z)): 0.4413  latent_norm : 16.1264  \n",
      "Model successfully saved.\n",
      "[19/55][200/781]  Loss_D: 1.2763  Loss_G: -0.6301  Loss_R_real: 0.0617  Loss_R_fake: 0.2902  D(x): 0.5440  D(G(z)): 0.4780  latent_norm : 16.8091  \n",
      "[19/55][400/781]  Loss_D: 1.2787  Loss_G: -0.6321  Loss_R_real: 0.0619  Loss_R_fake: 0.2819  D(x): 0.5660  D(G(z)): 0.4651  latent_norm : 16.6173  \n",
      "[19/55][600/781]  Loss_D: 1.2798  Loss_G: -0.6320  Loss_R_real: 0.0617  Loss_R_fake: 0.2912  D(x): 0.5166  D(G(z)): 0.4524  latent_norm : 16.1665  \n",
      "Model successfully saved.\n",
      "[20/55][200/781]  Loss_D: 1.2662  Loss_G: -0.6267  Loss_R_real: 0.0614  Loss_R_fake: 0.3138  D(x): 0.5584  D(G(z)): 0.4697  latent_norm : 15.2745  \n",
      "[20/55][400/781]  Loss_D: 1.2680  Loss_G: -0.6266  Loss_R_real: 0.0613  Loss_R_fake: 0.2979  D(x): 0.5574  D(G(z)): 0.4777  latent_norm : 16.3809  \n",
      "[20/55][600/781]  Loss_D: 1.2669  Loss_G: -0.6253  Loss_R_real: 0.0613  Loss_R_fake: 0.2961  D(x): 0.5607  D(G(z)): 0.4519  latent_norm : 15.0016  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thamires/Documents/Knowledge_Discovery/Results-Learning-Cortical-Representations-Trough-Perturbed-and-Adversarial-Dreaming/code/functions.py:304: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved.\n",
      "[21/55][200/781]  Loss_D: 1.2587  Loss_G: -0.6212  Loss_R_real: 0.0612  Loss_R_fake: 0.2955  D(x): 0.5876  D(G(z)): 0.4605  latent_norm : 14.8084  \n",
      "[21/55][400/781]  Loss_D: 1.2563  Loss_G: -0.6199  Loss_R_real: 0.0607  Loss_R_fake: 0.3009  D(x): 0.5439  D(G(z)): 0.4513  latent_norm : 15.3798  \n",
      "[21/55][600/781]  Loss_D: 1.2564  Loss_G: -0.6196  Loss_R_real: 0.0610  Loss_R_fake: 0.2989  D(x): 0.5771  D(G(z)): 0.4510  latent_norm : 16.3428  \n",
      "Model successfully saved.\n",
      "[22/55][200/781]  Loss_D: 1.2482  Loss_G: -0.6151  Loss_R_real: 0.0616  Loss_R_fake: 0.3175  D(x): 0.5865  D(G(z)): 0.4607  latent_norm : 14.9505  \n",
      "[22/55][400/781]  Loss_D: 1.2446  Loss_G: -0.6138  Loss_R_real: 0.0613  Loss_R_fake: 0.3030  D(x): 0.5601  D(G(z)): 0.4278  latent_norm : 15.2890  \n",
      "[22/55][600/781]  Loss_D: 1.2446  Loss_G: -0.6133  Loss_R_real: 0.0613  Loss_R_fake: 0.3065  D(x): 0.5729  D(G(z)): 0.4696  latent_norm : 15.4577  \n",
      "Model successfully saved.\n",
      "[23/55][200/781]  Loss_D: 1.2306  Loss_G: -0.6057  Loss_R_real: 0.0601  Loss_R_fake: 0.3101  D(x): 0.5610  D(G(z)): 0.4189  latent_norm : 16.0842  \n",
      "[23/55][400/781]  Loss_D: 1.2275  Loss_G: -0.6040  Loss_R_real: 0.0606  Loss_R_fake: 0.3082  D(x): 0.5513  D(G(z)): 0.4427  latent_norm : 15.3713  \n",
      "[23/55][600/781]  Loss_D: 1.2270  Loss_G: -0.6035  Loss_R_real: 0.0606  Loss_R_fake: 0.3099  D(x): 0.5507  D(G(z)): 0.4293  latent_norm : 15.2849  \n",
      "Model successfully saved.\n",
      "[24/55][200/781]  Loss_D: 1.2123  Loss_G: -0.5974  Loss_R_real: 0.0606  Loss_R_fake: 0.3262  D(x): 0.5727  D(G(z)): 0.4342  latent_norm : 16.0550  \n",
      "[24/55][400/781]  Loss_D: 1.2139  Loss_G: -0.5981  Loss_R_real: 0.0602  Loss_R_fake: 0.3253  D(x): 0.5337  D(G(z)): 0.4270  latent_norm : 15.6239  \n",
      "[24/55][600/781]  Loss_D: 1.2139  Loss_G: -0.5972  Loss_R_real: 0.0602  Loss_R_fake: 0.3257  D(x): 0.5762  D(G(z)): 0.4426  latent_norm : 15.3152  \n",
      "Model successfully saved.\n",
      "[25/55][200/781]  Loss_D: 1.1913  Loss_G: -0.5843  Loss_R_real: 0.0600  Loss_R_fake: 0.3404  D(x): 0.5573  D(G(z)): 0.4173  latent_norm : 15.2110  \n",
      "[25/55][400/781]  Loss_D: 1.1948  Loss_G: -0.5859  Loss_R_real: 0.0599  Loss_R_fake: 0.3341  D(x): 0.5443  D(G(z)): 0.4248  latent_norm : 16.0010  \n",
      "[25/55][600/781]  Loss_D: 1.1951  Loss_G: -0.5859  Loss_R_real: 0.0600  Loss_R_fake: 0.3367  D(x): 0.5711  D(G(z)): 0.4491  latent_norm : 15.6577  \n",
      "Model successfully saved.\n",
      "[26/55][200/781]  Loss_D: 1.1795  Loss_G: -0.5793  Loss_R_real: 0.0607  Loss_R_fake: 0.3669  D(x): 0.5527  D(G(z)): 0.4160  latent_norm : 16.0212  \n",
      "[26/55][400/781]  Loss_D: 1.1800  Loss_G: -0.5791  Loss_R_real: 0.0607  Loss_R_fake: 0.3413  D(x): 0.5716  D(G(z)): 0.4321  latent_norm : 15.8080  \n",
      "[26/55][600/781]  Loss_D: 1.1816  Loss_G: -0.5791  Loss_R_real: 0.0605  Loss_R_fake: 0.3395  D(x): 0.5856  D(G(z)): 0.3869  latent_norm : 15.8394  \n",
      "Model successfully saved.\n",
      "[27/55][200/781]  Loss_D: 1.1608  Loss_G: -0.5695  Loss_R_real: 0.0625  Loss_R_fake: 0.3295  D(x): 0.6548  D(G(z)): 0.4304  latent_norm : 14.4533  \n",
      "[27/55][400/781]  Loss_D: 1.1625  Loss_G: -0.5696  Loss_R_real: 0.0613  Loss_R_fake: 0.3267  D(x): 0.5709  D(G(z)): 0.4084  latent_norm : 15.6049  \n",
      "[27/55][600/781]  Loss_D: 1.1645  Loss_G: -0.5701  Loss_R_real: 0.0610  Loss_R_fake: 0.3318  D(x): 0.5830  D(G(z)): 0.4008  latent_norm : 16.3222  \n",
      "Model successfully saved.\n",
      "[28/55][200/781]  Loss_D: 1.1431  Loss_G: -0.5589  Loss_R_real: 0.0592  Loss_R_fake: 0.3061  D(x): 0.5639  D(G(z)): 0.3912  latent_norm : 15.5532  \n",
      "[28/55][400/781]  Loss_D: 1.1434  Loss_G: -0.5588  Loss_R_real: 0.0596  Loss_R_fake: 0.3235  D(x): 0.5293  D(G(z)): 0.4161  latent_norm : 14.6427  \n",
      "[28/55][600/781]  Loss_D: 1.1448  Loss_G: -0.5594  Loss_R_real: 0.0599  Loss_R_fake: 0.3245  D(x): 0.6443  D(G(z)): 0.4296  latent_norm : 15.4519  \n",
      "Model successfully saved.\n",
      "[29/55][200/781]  Loss_D: 1.1271  Loss_G: -0.5498  Loss_R_real: 0.0594  Loss_R_fake: 0.3656  D(x): 0.5919  D(G(z)): 0.4302  latent_norm : 15.4892  \n",
      "[29/55][400/781]  Loss_D: 1.1256  Loss_G: -0.5490  Loss_R_real: 0.0600  Loss_R_fake: 0.3542  D(x): 0.6152  D(G(z)): 0.4227  latent_norm : 15.4780  \n",
      "[29/55][600/781]  Loss_D: 1.1274  Loss_G: -0.5503  Loss_R_real: 0.0601  Loss_R_fake: 0.3491  D(x): 0.5786  D(G(z)): 0.3900  latent_norm : 15.2403  \n",
      "Model successfully saved.\n",
      "[30/55][200/781]  Loss_D: 1.1015  Loss_G: -0.5383  Loss_R_real: 0.0596  Loss_R_fake: 0.3635  D(x): 0.6228  D(G(z)): 0.3835  latent_norm : 15.9317  \n",
      "[30/55][400/781]  Loss_D: 1.1062  Loss_G: -0.5393  Loss_R_real: 0.0597  Loss_R_fake: 0.3669  D(x): 0.5748  D(G(z)): 0.3870  latent_norm : 15.3281  \n",
      "[30/55][600/781]  Loss_D: 1.1056  Loss_G: -0.5389  Loss_R_real: 0.0601  Loss_R_fake: 0.3604  D(x): 0.6171  D(G(z)): 0.3916  latent_norm : 16.3159  \n",
      "Model successfully saved.\n",
      "[31/55][200/781]  Loss_D: 1.0889  Loss_G: -0.5305  Loss_R_real: 0.0595  Loss_R_fake: 0.3354  D(x): 0.6333  D(G(z)): 0.3756  latent_norm : 15.3945  \n",
      "[31/55][400/781]  Loss_D: 1.0954  Loss_G: -0.5333  Loss_R_real: 0.0596  Loss_R_fake: 0.3473  D(x): 0.6377  D(G(z)): 0.3434  latent_norm : 16.0358  \n",
      "[31/55][600/781]  Loss_D: 1.0963  Loss_G: -0.5341  Loss_R_real: 0.0596  Loss_R_fake: 0.3588  D(x): 0.6175  D(G(z)): 0.3834  latent_norm : 15.7567  \n",
      "Model successfully saved.\n",
      "[32/55][200/781]  Loss_D: 1.0644  Loss_G: -0.5182  Loss_R_real: 0.0595  Loss_R_fake: 0.3336  D(x): 0.5647  D(G(z)): 0.4145  latent_norm : 14.8979  \n",
      "[32/55][400/781]  Loss_D: 1.0762  Loss_G: -0.5240  Loss_R_real: 0.0595  Loss_R_fake: 0.3465  D(x): 0.6455  D(G(z)): 0.4232  latent_norm : 15.4341  \n",
      "[32/55][600/781]  Loss_D: 1.0770  Loss_G: -0.5240  Loss_R_real: 0.0598  Loss_R_fake: 0.3468  D(x): 0.6014  D(G(z)): 0.3329  latent_norm : 14.5530  \n",
      "Model successfully saved.\n",
      "[33/55][200/781]  Loss_D: 1.0540  Loss_G: -0.5126  Loss_R_real: 0.0598  Loss_R_fake: 0.3439  D(x): 0.6494  D(G(z)): 0.4161  latent_norm : 16.2985  \n",
      "[33/55][400/781]  Loss_D: 1.0544  Loss_G: -0.5120  Loss_R_real: 0.0597  Loss_R_fake: 0.3537  D(x): 0.6073  D(G(z)): 0.4235  latent_norm : 15.1728  \n",
      "[33/55][600/781]  Loss_D: 1.0536  Loss_G: -0.5111  Loss_R_real: 0.0599  Loss_R_fake: 0.3479  D(x): 0.6246  D(G(z)): 0.4093  latent_norm : 15.6928  \n",
      "Model successfully saved.\n",
      "[34/55][200/781]  Loss_D: 1.0309  Loss_G: -0.5009  Loss_R_real: 0.0593  Loss_R_fake: 0.3585  D(x): 0.6285  D(G(z)): 0.3594  latent_norm : 14.5139  \n",
      "[34/55][400/781]  Loss_D: 1.0337  Loss_G: -0.5023  Loss_R_real: 0.0603  Loss_R_fake: 0.3732  D(x): 0.6539  D(G(z)): 0.4386  latent_norm : 16.0117  \n",
      "[34/55][600/781]  Loss_D: 1.0374  Loss_G: -0.5034  Loss_R_real: 0.0604  Loss_R_fake: 0.3639  D(x): 0.6578  D(G(z)): 0.3115  latent_norm : 15.3594  \n",
      "Model successfully saved.\n",
      "[35/55][200/781]  Loss_D: 1.0113  Loss_G: -0.4913  Loss_R_real: 0.0593  Loss_R_fake: 0.3524  D(x): 0.6642  D(G(z)): 0.4188  latent_norm : 16.2927  \n",
      "[35/55][400/781]  Loss_D: 1.0169  Loss_G: -0.4936  Loss_R_real: 0.0600  Loss_R_fake: 0.3573  D(x): 0.6074  D(G(z)): 0.3844  latent_norm : 15.2768  \n",
      "[35/55][600/781]  Loss_D: 1.0163  Loss_G: -0.4926  Loss_R_real: 0.0597  Loss_R_fake: 0.3600  D(x): 0.6649  D(G(z)): 0.3249  latent_norm : 16.5025  \n",
      "Model successfully saved.\n",
      "[36/55][200/781]  Loss_D: 0.9979  Loss_G: -0.4840  Loss_R_real: 0.0594  Loss_R_fake: 0.3685  D(x): 0.6182  D(G(z)): 0.3825  latent_norm : 15.8228  \n",
      "[36/55][400/781]  Loss_D: 0.9992  Loss_G: -0.4843  Loss_R_real: 0.0594  Loss_R_fake: 0.3577  D(x): 0.6789  D(G(z)): 0.3800  latent_norm : 15.5467  \n",
      "[36/55][600/781]  Loss_D: 1.0005  Loss_G: -0.4845  Loss_R_real: 0.0597  Loss_R_fake: 0.3625  D(x): 0.6609  D(G(z)): 0.3922  latent_norm : 14.6346  \n",
      "Model successfully saved.\n",
      "[37/55][200/781]  Loss_D: 0.9809  Loss_G: -0.4746  Loss_R_real: 0.0602  Loss_R_fake: 0.3735  D(x): 0.6806  D(G(z)): 0.3536  latent_norm : 15.4280  \n",
      "[37/55][400/781]  Loss_D: 0.9765  Loss_G: -0.4723  Loss_R_real: 0.0602  Loss_R_fake: 0.3693  D(x): 0.6446  D(G(z)): 0.3402  latent_norm : 15.1931  \n",
      "[37/55][600/781]  Loss_D: 0.9845  Loss_G: -0.4764  Loss_R_real: 0.0600  Loss_R_fake: 0.3710  D(x): 0.6606  D(G(z)): 0.3404  latent_norm : 15.8676  \n",
      "Model successfully saved.\n",
      "[38/55][200/781]  Loss_D: 0.9544  Loss_G: -0.4609  Loss_R_real: 0.0595  Loss_R_fake: 0.3556  D(x): 0.6538  D(G(z)): 0.3756  latent_norm : 16.2029  \n",
      "[38/55][400/781]  Loss_D: 0.9651  Loss_G: -0.4662  Loss_R_real: 0.0597  Loss_R_fake: 0.3564  D(x): 0.6257  D(G(z)): 0.3554  latent_norm : 15.7393  \n",
      "[38/55][600/781]  Loss_D: 0.9657  Loss_G: -0.4662  Loss_R_real: 0.0594  Loss_R_fake: 0.3616  D(x): 0.6564  D(G(z)): 0.3146  latent_norm : 15.2765  \n",
      "Model successfully saved.\n",
      "[39/55][200/781]  Loss_D: 0.9513  Loss_G: -0.4583  Loss_R_real: 0.0596  Loss_R_fake: 0.3935  D(x): 0.7149  D(G(z)): 0.3332  latent_norm : 16.4483  \n",
      "[39/55][400/781]  Loss_D: 0.9543  Loss_G: -0.4602  Loss_R_real: 0.0597  Loss_R_fake: 0.3891  D(x): 0.6834  D(G(z)): 0.2800  latent_norm : 16.3792  \n",
      "[39/55][600/781]  Loss_D: 0.9527  Loss_G: -0.4598  Loss_R_real: 0.0599  Loss_R_fake: 0.3791  D(x): 0.6875  D(G(z)): 0.3294  latent_norm : 15.1799  \n",
      "Model successfully saved.\n",
      "[40/55][200/781]  Loss_D: 0.9120  Loss_G: -0.4412  Loss_R_real: 0.0601  Loss_R_fake: 0.3688  D(x): 0.7317  D(G(z)): 0.3560  latent_norm : 15.2808  \n",
      "[40/55][400/781]  Loss_D: 0.9244  Loss_G: -0.4472  Loss_R_real: 0.0596  Loss_R_fake: 0.3705  D(x): 0.6551  D(G(z)): 0.3890  latent_norm : 15.4895  \n",
      "[40/55][600/781]  Loss_D: 0.9277  Loss_G: -0.4481  Loss_R_real: 0.0598  Loss_R_fake: 0.3749  D(x): 0.5890  D(G(z)): 0.3130  latent_norm : 14.8597  \n",
      "Model successfully saved.\n",
      "[41/55][200/781]  Loss_D: 0.9092  Loss_G: -0.4395  Loss_R_real: 0.0588  Loss_R_fake: 0.3857  D(x): 0.6438  D(G(z)): 0.2989  latent_norm : 15.4765  \n",
      "[41/55][400/781]  Loss_D: 0.9126  Loss_G: -0.4410  Loss_R_real: 0.0595  Loss_R_fake: 0.3768  D(x): 0.7026  D(G(z)): 0.3159  latent_norm : 15.3823  \n",
      "[41/55][600/781]  Loss_D: 0.9150  Loss_G: -0.4420  Loss_R_real: 0.0596  Loss_R_fake: 0.3878  D(x): 0.6941  D(G(z)): 0.3588  latent_norm : 15.8590  \n",
      "Model successfully saved.\n",
      "[42/55][200/781]  Loss_D: 0.8813  Loss_G: -0.4272  Loss_R_real: 0.0594  Loss_R_fake: 0.3712  D(x): 0.6771  D(G(z)): 0.3206  latent_norm : 15.6647  \n",
      "[42/55][400/781]  Loss_D: 0.8905  Loss_G: -0.4304  Loss_R_real: 0.0601  Loss_R_fake: 0.3823  D(x): 0.7039  D(G(z)): 0.2768  latent_norm : 15.6548  \n",
      "[42/55][600/781]  Loss_D: 0.8967  Loss_G: -0.4333  Loss_R_real: 0.0603  Loss_R_fake: 0.3826  D(x): 0.6846  D(G(z)): 0.2997  latent_norm : 15.4395  \n",
      "Model successfully saved.\n",
      "[43/55][200/781]  Loss_D: 0.8635  Loss_G: -0.4165  Loss_R_real: 0.0589  Loss_R_fake: 0.3875  D(x): 0.7393  D(G(z)): 0.3423  latent_norm : 16.0602  \n",
      "[43/55][400/781]  Loss_D: 0.8679  Loss_G: -0.4177  Loss_R_real: 0.0593  Loss_R_fake: 0.3885  D(x): 0.7998  D(G(z)): 0.2990  latent_norm : 16.1319  \n",
      "[43/55][600/781]  Loss_D: 0.8719  Loss_G: -0.4194  Loss_R_real: 0.0596  Loss_R_fake: 0.3901  D(x): 0.7994  D(G(z)): 0.2592  latent_norm : 16.7366  \n",
      "Model successfully saved.\n",
      "[44/55][200/781]  Loss_D: 0.8508  Loss_G: -0.4101  Loss_R_real: 0.0598  Loss_R_fake: 0.3836  D(x): 0.6433  D(G(z)): 0.3340  latent_norm : 15.1948  \n",
      "[44/55][400/781]  Loss_D: 0.8584  Loss_G: -0.4137  Loss_R_real: 0.0600  Loss_R_fake: 0.3872  D(x): 0.7194  D(G(z)): 0.2947  latent_norm : 15.8161  \n",
      "[44/55][600/781]  Loss_D: 0.8618  Loss_G: -0.4148  Loss_R_real: 0.0602  Loss_R_fake: 0.3984  D(x): 0.7129  D(G(z)): 0.3350  latent_norm : 15.1925  \n",
      "Model successfully saved.\n",
      "[45/55][200/781]  Loss_D: 0.8346  Loss_G: -0.4050  Loss_R_real: 0.0603  Loss_R_fake: 0.4042  D(x): 0.7523  D(G(z)): 0.3071  latent_norm : 15.7235  \n",
      "[45/55][400/781]  Loss_D: 0.8421  Loss_G: -0.4067  Loss_R_real: 0.0609  Loss_R_fake: 0.4026  D(x): 0.6924  D(G(z)): 0.3191  latent_norm : 15.7865  \n",
      "[45/55][600/781]  Loss_D: 0.8459  Loss_G: -0.4084  Loss_R_real: 0.0608  Loss_R_fake: 0.3972  D(x): 0.6819  D(G(z)): 0.2338  latent_norm : 16.8725  \n",
      "Model successfully saved.\n",
      "[46/55][200/781]  Loss_D: 0.8134  Loss_G: -0.3926  Loss_R_real: 0.0590  Loss_R_fake: 0.3965  D(x): 0.7153  D(G(z)): 0.2609  latent_norm : 15.4697  \n",
      "[46/55][400/781]  Loss_D: 0.8188  Loss_G: -0.3952  Loss_R_real: 0.0595  Loss_R_fake: 0.3970  D(x): 0.7253  D(G(z)): 0.3138  latent_norm : 16.0737  \n",
      "[46/55][600/781]  Loss_D: 0.8229  Loss_G: -0.3970  Loss_R_real: 0.0594  Loss_R_fake: 0.3983  D(x): 0.7974  D(G(z)): 0.2730  latent_norm : 15.3822  \n",
      "Model successfully saved.\n",
      "[47/55][200/781]  Loss_D: 0.7953  Loss_G: -0.3836  Loss_R_real: 0.0590  Loss_R_fake: 0.3806  D(x): 0.7421  D(G(z)): 0.2798  latent_norm : 15.8301  \n",
      "[47/55][400/781]  Loss_D: 0.7983  Loss_G: -0.3846  Loss_R_real: 0.0593  Loss_R_fake: 0.3847  D(x): 0.7351  D(G(z)): 0.2743  latent_norm : 15.8683  \n",
      "[47/55][600/781]  Loss_D: 0.8082  Loss_G: -0.3896  Loss_R_real: 0.0597  Loss_R_fake: 0.3891  D(x): 0.6833  D(G(z)): 0.3099  latent_norm : 14.8228  \n",
      "Model successfully saved.\n",
      "[48/55][200/781]  Loss_D: 0.7826  Loss_G: -0.3769  Loss_R_real: 0.0606  Loss_R_fake: 0.4079  D(x): 0.6959  D(G(z)): 0.3190  latent_norm : 15.9826  \n",
      "[48/55][400/781]  Loss_D: 0.7853  Loss_G: -0.3779  Loss_R_real: 0.0601  Loss_R_fake: 0.4052  D(x): 0.6914  D(G(z)): 0.2833  latent_norm : 15.3620  \n",
      "[48/55][600/781]  Loss_D: 0.7915  Loss_G: -0.3805  Loss_R_real: 0.0601  Loss_R_fake: 0.4086  D(x): 0.6387  D(G(z)): 0.3026  latent_norm : 15.4809  \n",
      "Model successfully saved.\n",
      "[49/55][200/781]  Loss_D: 0.7591  Loss_G: -0.3685  Loss_R_real: 0.0584  Loss_R_fake: 0.4035  D(x): 0.6697  D(G(z)): 0.3247  latent_norm : 15.5514  \n",
      "[49/55][400/781]  Loss_D: 0.7663  Loss_G: -0.3717  Loss_R_real: 0.0606  Loss_R_fake: 0.4060  D(x): 0.7510  D(G(z)): 0.3090  latent_norm : 15.0074  \n",
      "[49/55][600/781]  Loss_D: 0.7715  Loss_G: -0.3728  Loss_R_real: 0.0602  Loss_R_fake: 0.3985  D(x): 0.7461  D(G(z)): 0.2601  latent_norm : 14.5197  \n",
      "Model successfully saved.\n",
      "[50/55][200/781]  Loss_D: 0.7431  Loss_G: -0.3604  Loss_R_real: 0.0589  Loss_R_fake: 0.3874  D(x): 0.8256  D(G(z)): 0.2973  latent_norm : 16.1925  \n",
      "[50/55][400/781]  Loss_D: 0.7566  Loss_G: -0.3659  Loss_R_real: 0.0597  Loss_R_fake: 0.3923  D(x): 0.7132  D(G(z)): 0.2167  latent_norm : 14.7095  \n",
      "[50/55][600/781]  Loss_D: 0.7548  Loss_G: -0.3644  Loss_R_real: 0.0594  Loss_R_fake: 0.3919  D(x): 0.7394  D(G(z)): 0.2713  latent_norm : 15.6511  \n",
      "Model successfully saved.\n",
      "[51/55][200/781]  Loss_D: 0.7243  Loss_G: -0.3506  Loss_R_real: 0.0586  Loss_R_fake: 0.4102  D(x): 0.7851  D(G(z)): 0.2125  latent_norm : 15.8900  \n",
      "[51/55][400/781]  Loss_D: 0.7289  Loss_G: -0.3524  Loss_R_real: 0.0598  Loss_R_fake: 0.4198  D(x): 0.7436  D(G(z)): 0.2582  latent_norm : 16.4739  \n",
      "[51/55][600/781]  Loss_D: 0.7377  Loss_G: -0.3567  Loss_R_real: 0.0599  Loss_R_fake: 0.4136  D(x): 0.7855  D(G(z)): 0.2228  latent_norm : 15.4643  \n",
      "Model successfully saved.\n",
      "[52/55][200/781]  Loss_D: 0.7128  Loss_G: -0.3441  Loss_R_real: 0.0600  Loss_R_fake: 0.3996  D(x): 0.6976  D(G(z)): 0.2110  latent_norm : 16.4113  \n",
      "[52/55][400/781]  Loss_D: 0.7123  Loss_G: -0.3437  Loss_R_real: 0.0597  Loss_R_fake: 0.4057  D(x): 0.7005  D(G(z)): 0.2049  latent_norm : 15.1946  \n",
      "[52/55][600/781]  Loss_D: 0.7216  Loss_G: -0.3484  Loss_R_real: 0.0601  Loss_R_fake: 0.4066  D(x): 0.7795  D(G(z)): 0.2337  latent_norm : 16.1247  \n",
      "Model successfully saved.\n",
      "[53/55][200/781]  Loss_D: 0.6996  Loss_G: -0.3377  Loss_R_real: 0.0602  Loss_R_fake: 0.4084  D(x): 0.7206  D(G(z)): 0.2071  latent_norm : 15.1637  \n",
      "[53/55][400/781]  Loss_D: 0.7063  Loss_G: -0.3413  Loss_R_real: 0.0601  Loss_R_fake: 0.3965  D(x): 0.7303  D(G(z)): 0.2799  latent_norm : 15.7853  \n",
      "[53/55][600/781]  Loss_D: 0.7119  Loss_G: -0.3441  Loss_R_real: 0.0603  Loss_R_fake: 0.3968  D(x): 0.7485  D(G(z)): 0.2528  latent_norm : 15.1265  \n",
      "Model successfully saved.\n",
      "[54/55][200/781]  Loss_D: 0.6795  Loss_G: -0.3300  Loss_R_real: 0.0588  Loss_R_fake: 0.4020  D(x): 0.7888  D(G(z)): 0.1623  latent_norm : 15.3694  \n",
      "[54/55][400/781]  Loss_D: 0.6931  Loss_G: -0.3362  Loss_R_real: 0.0592  Loss_R_fake: 0.4091  D(x): 0.7701  D(G(z)): 0.1891  latent_norm : 15.8615  \n",
      "[54/55][600/781]  Loss_D: 0.6978  Loss_G: -0.3381  Loss_R_real: 0.0594  Loss_R_fake: 0.4089  D(x): 0.6979  D(G(z)): 0.2077  latent_norm : 16.2646  \n",
      "Model successfully saved.\n"
     ]
    }
   ],
   "source": [
    "# Enable anomaly detection during training (optional)\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(len(d_losses), opt.niter):\n",
    "    \n",
    "    # Initialize lists to store losses and other metrics\n",
    "    store_loss_D = []\n",
    "    store_loss_G = []\n",
    "    store_loss_R_real = []\n",
    "    store_loss_R_fake = []\n",
    "    store_norm = []\n",
    "    store_kl = []\n",
    "\n",
    "    # Iterate over the data batches\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # Wake (W)\n",
    "        ###########################\n",
    "\n",
    "        # Discrimination wake\n",
    "        discriminator_optimizer .zero_grad()\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Fetch real images and labels\n",
    "        real_image, label = data\n",
    "        real_image, label = real_image.to(device), label.to(device)\n",
    "\n",
    "        # Pass real images through the discriminator\n",
    "        latent_output, dis_output = discriminator(real_image)\n",
    "\n",
    "        # Add noise to the latent space\n",
    "        latent_output_noise = latent_output + opt.epsilon * torch.randn(batch_size, latent_size, device=device)\n",
    "\n",
    "        # Set the discriminator label for real images\n",
    "        discriminator_label[:] = real_label_value\n",
    "\n",
    "        # Compute the discriminator loss for real images\n",
    "        dis_errD_real = discriminator_criterion(dis_output, discriminator_label)\n",
    "\n",
    "        if opt.R > 0.0:  # if GAN learning occurs\n",
    "            (dis_errD_real).backward(retain_graph=True)\n",
    "\n",
    "        # Compute the KL divergence regularization loss\n",
    "        kl = kl_loss(latent_output)\n",
    "        (kl).backward(retain_graph=True)\n",
    "\n",
    "        # Reconstruct real images from the latent space\n",
    "        reconstructed_image = generator(latent_output_noise, reverse=False)\n",
    "\n",
    "        # Compute the reconstruction loss for real images\n",
    "        rec_real = reconstruction_criterion (reconstructed_image, real_image)\n",
    "\n",
    "        if opt.W > 0.0:\n",
    "            (opt.W * rec_real).backward()\n",
    "\n",
    "        discriminator_optimizer .step()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Compute the mean of the discriminator output (between 0 and 1)\n",
    "        D_x = dis_output.cpu().mean()\n",
    "\n",
    "        # Compute the norm of the latent space representation\n",
    "        latent_norm = torch.mean(torch.norm(latent_output.squeeze(), dim=1)).item()\n",
    "\n",
    "\n",
    "        ###########################\n",
    "        # NREM perturbed dreaming (N)\n",
    "        ##########################\n",
    "        discriminator_optimizer .zero_grad()\n",
    "\n",
    "        # Detach the latent space representation\n",
    "        latent_z = latent_output.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate images from the detached latent space\n",
    "            nrem_image = generator(latent_z)\n",
    "\n",
    "            # Apply occlusion to the generated images\n",
    "            occlusion = Occlude(drop_rate=random.random(), tile_size=random.randint(1, 8))\n",
    "            occluded_nrem_image = occlusion(nrem_image, dim=1)\n",
    "\n",
    "        # Pass occluded NREM images through the discriminator\n",
    "        latent_recons_dream, _ = discriminator(occluded_nrem_image)\n",
    "\n",
    "        # Compute the reconstruction loss for fake images\n",
    "        rec_fake = reconstruction_criterion (latent_recons_dream, latent_output.detach())\n",
    "\n",
    "        if opt.N > 0.0:\n",
    "            (opt.N * rec_fake).backward()\n",
    "\n",
    "        discriminator_optimizer .step()\n",
    "\n",
    "\n",
    "       ###########################\n",
    "        # REM adversarial dreaming (R)\n",
    "        ##########################\n",
    "\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        generator_optimizer.zero_grad()\n",
    "        lmbd = opt.lmbd\n",
    "        noise = torch.randn(batch_size, latent_size, device=device)\n",
    "        if i==0:\n",
    "            latent_z = 0.5*latent_output.detach() + 0.5*noise\n",
    "        else:\n",
    "            latent_z = 0.25*latent_output.detach() + 0.25*old_latent_output + 0.5*noise\n",
    "        \n",
    "        dreamed_image_adv = generator(latent_z, reverse=True) # activate plasticity switch\n",
    "        latent_recons_dream, dis_output = discriminator(dreamed_image_adv)\n",
    "        discriminator_label[:] = fake_label_value # should be classified as fake\n",
    "        dis_errD_fake = discriminator_criterion(dis_output, discriminator_label)\n",
    "        if opt.R > 0.0: # if GAN learning occurs\n",
    "            dis_errD_fake.backward(retain_graph=True)\n",
    "            discriminator_optimizer.step()\n",
    "            generator_optimizer.step()\n",
    "        dis_errG = - dis_errD_fake\n",
    "\n",
    "        D_G_z1 = dis_output.cpu().mean()\n",
    "\n",
    "        old_latent_output = latent_output.detach()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        # Compute average losses\n",
    "        ###########################\n",
    "        store_loss_G.append(dis_errG.item())\n",
    "        store_loss_D.append((dis_errD_fake + dis_errD_real).item())\n",
    "        store_loss_R_real.append(rec_real.item())\n",
    "        store_loss_R_fake.append(rec_fake.item())\n",
    "        store_norm.append(latent_norm)\n",
    "        store_kl.append(kl.item())\n",
    "        \n",
    "\n",
    "\n",
    "        if i % 200 == 0 and i>1:\n",
    "            print('[%d/%d][%d/%d]  Loss_D: %.4f  Loss_G: %.4f  Loss_R_real: %.4f  Loss_R_fake: %.4f  D(x): %.4f  D(G(z)): %.4f  latent_norm : %.4f  '\n",
    "                % (epoch, opt.niter, i, len(dataloader),\n",
    "                    np.mean(store_loss_D), np.mean(store_loss_G), np.mean(store_loss_R_real), np.mean(store_loss_R_fake), D_x, D_G_z1, np.mean(latent_norm) ))\n",
    "            compare_img_rec = torch.zeros(batch_size * 2, real_image.size(1), real_image.size(2), real_image.size(3))\n",
    "            with torch.no_grad():\n",
    "                reconstructed_image = generator(latent_output)\n",
    "            compare_img_rec[::2] = real_image\n",
    "            compare_img_rec[1::2] = reconstructed_image\n",
    "            vutils.save_image(unorm(compare_img_rec[:128]), '%s/recon_%03d.png' % (dir_files, epoch), nrow=8)\n",
    "            fake = unorm(dreamed_image_adv)\n",
    "            vutils.save_image(fake[:64].data, '%s/fake_%03d.png' % (dir_files, epoch), nrow=8)\n",
    "            \n",
    "\n",
    "    d_losses.append(np.mean(store_loss_D))\n",
    "    g_losses.append(np.mean(store_loss_G))\n",
    "    r_losses_real.append(np.mean(store_loss_R_real))\n",
    "    r_losses_fake.append(np.mean(store_loss_R_fake))\n",
    "    kl_losses.append(np.mean(store_kl))\n",
    "    save_fig_losses(epoch, d_losses, g_losses, r_losses_real, r_losses_fake, kl_losses, None, None,  dir_files)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save({\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        'g_optim': generator_optimizer.state_dict(),\n",
    "        'd_optim': discriminator_optimizer.state_dict(),\n",
    "        'd_losses': d_losses,\n",
    "        'g_losses': g_losses,\n",
    "        'r_losses_real': r_losses_real,\n",
    "        'r_losses_fake': r_losses_fake,\n",
    "        'kl_losses': kl_losses,\n",
    "    }, dir_checkpoint+'/trained.pth')\n",
    "    \n",
    "    # save network after 1 learning epoch\n",
    "    if epoch ==1:\n",
    "            torch.save({\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        }, dir_checkpoint+'/trained2.pth')\n",
    "\n",
    "    print(f'Model successfully saved.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
