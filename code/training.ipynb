{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # Ensures print function compatibility with Python 2.x\n",
    "import argparse  # Library for parsing command-line arguments\n",
    "import os  # Library for interacting with the operating system\n",
    "import copy  # Library for creating object copies\n",
    "import numpy as np  # Numerical computing library\n",
    "import random  # Library for generating random numbers\n",
    "\n",
    "import torch  # Core library for PyTorch\n",
    "import torch.nn as nn  # Library for defining neural network components\n",
    "import torch.nn.parallel  # Library for parallelizing operations on multiple GPUs\n",
    "import torch.backends.cudnn as cudnn  # Interface to the cuDNN library for GPU optimizations\n",
    "import torch.optim as optim  # Library for optimization algorithms\n",
    "import torch.utils.data  # Tools for working with datasets in PyTorch\n",
    "import torchvision.datasets as dset  # Datasets provided by torchvision\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing\n",
    "import torchvision.utils as vutils  # Utility functions for visualizing images\n",
    "from torch.autograd import Variable  # Provides automatic differentiation for tensors\n",
    "import torch.nn.functional as F  # Library for various activation functions and loss functions\n",
    "\n",
    "from functions import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(imageSize=32, dataset='fashion', dataroot='./datasets/', num_workers=2, is_continue=1, batch_size=64, image_size=32, latent_size=256, num_epochs=55, weight_cycle_consistency=1.0, W=1.0, N=1.0, R=1.0, epsilon=0.0, num_filters=64, dropout_prob=0.0, learning_rate_generator=0.0002, learning_rate_discriminator=0.0002, beta1=0.5, lmbd=0.5, num_gpus=1, output_folder='output', gpu_id='0', outf='output', niter=55)\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import argparse\n",
    "\n",
    "# Creating an argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Adding arguments with their default values and descriptions\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\n",
    "parser.add_argument('--dataset', default='fashion', help='Dataset to use: cifar10 | imagenet | mnist')\n",
    "parser.add_argument('--dataroot', default='./datasets/', help='Path to the dataset')\n",
    "parser.add_argument('--num_workers', type=int, help='Number of data loading workers', default=2)\n",
    "parser.add_argument('--is_continue', type=int, default=1, help='Use pre-trained model')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Input batch size')\n",
    "parser.add_argument('--image_size', type=int, default=32, help='Height/width of the input image to the network')\n",
    "parser.add_argument('--latent_size', type=int, default=256, help='Size of the latent vector')\n",
    "parser.add_argument('--num_epochs', type=int, default=55, help='Number of epochs to train for')\n",
    "parser.add_argument('--weight_cycle_consistency', type=float, default=1.0, help='Weight of Cycle Consistency')\n",
    "parser.add_argument('--W', type=float, default=1.0, help='Wake')\n",
    "parser.add_argument('--N', type=float, default=1.0, help='NREM')\n",
    "parser.add_argument('--R', type=float, default=1.0, help='REM')\n",
    "parser.add_argument('--epsilon', type=float, default=0.0, help='Amount of noise in the wake latent space')\n",
    "parser.add_argument('--num_filters', type=int, default=64, help='Filters factor')\n",
    "parser.add_argument('--dropout_prob', type=float, default=0.0, help='Probability of dropout')\n",
    "parser.add_argument('--learning_rate_generator', type=float, default=0.0002, help='Learning rate for the generator')\n",
    "parser.add_argument('--learning_rate_discriminator', type=float, default=0.0002, help='Learning rate for the discriminator')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='Beta1 for Adam optimizer')\n",
    "parser.add_argument('--lmbd', type=float, default=0.5, help='convex combination factor for REM')\n",
    "parser.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use')\n",
    "parser.add_argument('--output_folder', default='output', help='Folder to output images and model checkpoints')\n",
    "parser.add_argument('--gpu_id', type=str, default='0', help='The ID of the specified GPU')\n",
    "parser.add_argument('--outf', default='output', help='folder to output images and model checkpoints')\n",
    "\n",
    "\n",
    "\n",
    "# Parsing the command-line arguments\n",
    "opt, unknown = parser.parse_known_args()\n",
    "\n",
    "# Set the number of iterations to the number of epochs\n",
    "opt.niter = opt.num_epochs\n",
    "\n",
    "# Assign the value of latent_size based on opt.latent_size\n",
    "latent_size = opt.latent_size\n",
    "\n",
    "# Printing the parsed arguments\n",
    "print(opt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. The code begins by importing the necessary library, **`argparse`**, which provides a way to parse command-line arguments.\n",
    "\n",
    "2. An argument parser object is created using **`argparse.ArgumentParser()`**. This object will handle the parsing of command-line arguments.\n",
    "\n",
    "3. Various arguments are added to the parser using the **`add_argument()`** method. Each argument has a unique name, a default value, and a description.\n",
    "\n",
    "4. The command-line arguments are parsed using **`parser.parse_known_args()`**, which returns two values: **`opt`** (containing the parsed argument values) and **`unknown`** (containing any unrecognized arguments).\n",
    "\n",
    "5. The parsed argument values are stored in the **`opt`** object.\n",
    "\n",
    "6. Finally, the values of the parsed arguments are printed using **`print(opt)`**.\n",
    "\n",
    "This code allows you to run the program with different options and values from the command line. Each option represents a specific setting or parameter that can be customized. The **`argparse`** library handles the parsing of these options, and the **`opt`** object stores the values for further use within the program. Printing the **`opt`** object provides a summary of the parsed argument values, allowing you to verify the settings before running the main logic of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the GPU ID if using only 1 GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_id\n",
    "\n",
    "# where to save samples and training curves\n",
    "dir_files = './results/'+opt.dataset+'/'+opt.outf\n",
    "# where to save model\n",
    "dir_checkpoint = './checkpoints/'+opt.dataset+'/'+opt.outf\n",
    "\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "try:\n",
    "    os.makedirs(dir_files)\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs(dir_checkpoint)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset and get relevant information\n",
    "dataset, unorm, img_channels = get_dataset(opt.dataset, opt.dataroot, opt.imageSize)\n",
    "\n",
    "# Create a data loader for loading the dataset in batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True,\n",
    "                                         num_workers=int(opt.num_workers), drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code sets the environment variable **`CUDA_VISIBLE_DEVICES`** to the GPU ID specified in **`opt.gpu_id`**. This ensures that only the specified GPU is used when running the program.\n",
    "\n",
    "2. The code defines two directory paths: **`dir_files`** for saving samples and training curves, and **`dir_checkpoint`** for saving models.\n",
    "\n",
    "3. The code attempts to create the directories specified by **`dir_files`** and **`dir_checkpoint`**. If the directories already exist, the **`OSError`** exception is caught and passed.\n",
    "\n",
    "4. The code checks if CUDA is available and assigns the device accordingly. If CUDA is available, the device is set to the GPU with ID 0; otherwise, it is set to CPU.\n",
    "\n",
    "5. The code calls the **`get_dataset()`** function to load the dataset specified in **`opt.dataset`** from the directory **`opt.dataroot`**, and also obtains the **`unorm`** (normalization) and **`img_channels`** (number of image channels) values.\n",
    "\n",
    "6. Finally, the code creates a **`DataLoader`** object called **`dataloader`**, which allows iterating over the dataset in batches. The batch size is specified by **`opt.batchSize`**, and the data is shuffled and loaded in parallel using **`opt.num_workers`** worker processes. The **`drop_last`** option ensures that the last incomplete batch is dropped if its size is less than **`opt.batchSize`**.\n",
    "\n",
    "This code prepares the necessary setup before training the neural network, such as configuring the device, setting up directories for saving results, and loading the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (dis): Sequential(\n",
       "    (0): Conv2d(256, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and assign values to hyperparameters\n",
    "num_gpus = int(opt.num_gpus)\n",
    "latent_dim = int(opt.latent_size)\n",
    "batch_size = opt.batch_size\n",
    "\n",
    "# Instantiate generator and discriminator networks\n",
    "generator = Generator(num_gpus, latent_dim=latent_dim, ngf=opt.num_filters, img_channels=img_channels)\n",
    "generator.apply(initialize_weights)\n",
    "discriminator = Discriminator(num_gpus, latent_dim=latent_dim, ndf=opt.num_filters, img_channels=img_channels, dropout_prob=opt.dropout_prob)\n",
    "discriminator.apply(initialize_weights)\n",
    "\n",
    "\n",
    "# Move networks to the GPU\n",
    "generator.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code sets up some hyperparameters, such as the number of GPUs available (num_gpus), the size of the latent space (latent_dim), and the batch size (batch_size).\n",
    "\n",
    "2. Two neural network models are instantiated: the generator (named \"generator\") and the discriminator (named \"discriminator\").\n",
    "\n",
    "3. The generator is an instance of the Generator class, which takes the number of GPUs, latent dimension, number of filters, and number of channels as arguments.\n",
    "\n",
    "4. The discriminator is an instance of the Discriminator class, which takes similar arguments as the generator along with a dropout probability.\n",
    "\n",
    "5. Weight initialization is applied to both the generator and discriminator using the weights_init function.\n",
    "\n",
    "6. The generator and discriminator models are moved to the specified device (e.g., GPU) using the to() method.\n",
    "\n",
    "7. This ensures that the computations for these models will be performed on the GPU if available, which can significantly speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers for discriminator and generator\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=opt.learning_rate_discriminator, betas=(opt.beta1, 0.999))\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=opt.learning_rate_generator, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# Initialize lists to store losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "r_losses_real = []\n",
    "r_losses_fake = []\n",
    "kl_losses = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up optimizers for the discriminator and generator models.\n",
    "\n",
    "1. The discriminator_optimizer uses the Adam optimizer and takes the discriminator parameters, learning rate (lrD), and beta values as arguments.\n",
    "\n",
    "2. The generator_optimizer uses the Adam optimizer and takes the generator parameters, learning rate (lrG), and beta values as arguments.\n",
    "\n",
    "3. Lists are initialized to store various types of losses during training.\n",
    "\n",
    "4. **discriminator_losses:** Stores the losses of the discriminator model.\n",
    "\n",
    "5. **generator_losses:** Stores the losses of the generator model.\n",
    "\n",
    "6. **real_losses:** Stores losses related to real images.\n",
    "\n",
    "7. **fake_losses:** Stores losses related to fake/generated images.\n",
    "\n",
    "8. **kl_losses:** Stores Kullback-Leibler divergence losses, which are often used in variational autoencoders (VAEs) or other generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model detected, restart training...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(dir_checkpoint+'/trained.pth') and opt.is_continue:\n",
    "    # Load data from last checkpoint\n",
    "    print('Loading pre-trained model...')\n",
    "    checkpoint = torch.load(dir_checkpoint+'/trained.pth', map_location=torch.device('cpu'))\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "    generator_optimizer.load_state_dict(checkpoint['g_optim'])\n",
    "    discriminator_optimizer.load_state_dict(checkpoint['d_optim'])\n",
    "    d_losses = checkpoint.get('d_losses', [float('inf')])\n",
    "    g_losses = checkpoint.get('g_losses', [float('inf')])\n",
    "    r_losses_real = checkpoint.get('r_losses_real', [float('inf')])\n",
    "    r_losses_fake = checkpoint.get('r_losses_fake', [float('inf')])\n",
    "    kl_losses = checkpoint.get('kl_losses', [float('inf')])\n",
    "    print('Start training from loaded model...')\n",
    "else:\n",
    "    print('No pre-trained model detected, restart training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions\n",
    "discriminator_criterion = nn.BCELoss()  # Binary Cross Entropy Loss for the discriminator\n",
    "reconstruction_criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "\n",
    "# Create tensor placeholders\n",
    "discriminator_label = torch.zeros(opt.batch_size, dtype=torch.float32, device=device)\n",
    "real_label_value = 1.0\n",
    "fake_label_value = 0\n",
    "\n",
    "evaluation_noise = torch.randn(batch_size, latent_dim, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "1. The code defines two loss functions: **`discriminator_criterion`** and **`reconstruction_criterion`**. The **`BCELoss`** (Binary Cross Entropy Loss) is used for the discriminator, and the **`MSELoss`** (Mean Squared Error Loss) is used for reconstruction.\n",
    "\n",
    "2. The variables **`dis_criterion`** and **`rec_criterion`** are changed to **`discriminator_criterion`** and **`reconstruction_criterion`**, respectively. \n",
    "\n",
    "3. The variable names **`dis_label`**, **`real_label_value`**, **`fake_label_value`**, and **`eval_noise`** are changed to **`discriminator_label`**, **`real_label_value`**, **`fake_label_value`**, and **`evaluation_noise`**, respectively.\n",
    "\n",
    "4. The order and structure of the code remain unchanged as they are necessary for defining the loss functions and creating tensor placeholders.\n",
    "\n",
    "Overall, this code snippet defines the loss functions for the discriminator and reconstruction tasks. The **`BCELoss`** is commonly used for binary classification tasks, such as determining whether an image is real or fake. The **`MSELoss`** is used for measuring the pixel-wise difference between the input and reconstructed images. The tensor placeholders are created to hold the discriminator labels, real and fake label values, and noise for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/55][200/937]  Loss_D: 1.3677  Loss_G: -0.7012  Loss_R_real: 0.2619  Loss_R_fake: 0.2448  D(x): 0.5282  D(G(z)): 0.5028  latent_norm : 11.0960  \n",
      "[0/55][400/937]  Loss_D: 1.3666  Loss_G: -0.6969  Loss_R_real: 0.2171  Loss_R_fake: 0.2201  D(x): 0.5346  D(G(z)): 0.4972  latent_norm : 15.8026  \n",
      "[0/55][600/937]  Loss_D: 1.3734  Loss_G: -0.7000  Loss_R_real: 0.1830  Loss_R_fake: 0.2113  D(x): 0.5119  D(G(z)): 0.4994  latent_norm : 15.2089  \n",
      "[0/55][800/937]  Loss_D: 1.3754  Loss_G: -0.6993  Loss_R_real: 0.1583  Loss_R_fake: 0.2071  D(x): 0.5118  D(G(z)): 0.4983  latent_norm : 14.5553  \n",
      "Model successfully saved.\n",
      "[1/55][200/937]  Loss_D: 1.3792  Loss_G: -0.6951  Loss_R_real: 0.0748  Loss_R_fake: 0.2156  D(x): 0.5148  D(G(z)): 0.5047  latent_norm : 15.8186  \n",
      "[1/55][400/937]  Loss_D: 1.3780  Loss_G: -0.6943  Loss_R_real: 0.0731  Loss_R_fake: 0.2151  D(x): 0.5087  D(G(z)): 0.4955  latent_norm : 14.3196  \n",
      "[1/55][600/937]  Loss_D: 1.3774  Loss_G: -0.6947  Loss_R_real: 0.0714  Loss_R_fake: 0.2057  D(x): 0.5032  D(G(z)): 0.4969  latent_norm : 13.9143  \n",
      "[1/55][800/937]  Loss_D: 1.3779  Loss_G: -0.6954  Loss_R_real: 0.0703  Loss_R_fake: 0.2126  D(x): 0.5077  D(G(z)): 0.4986  latent_norm : 12.5313  \n",
      "Model successfully saved.\n",
      "[2/55][200/937]  Loss_D: 1.3741  Loss_G: -0.6940  Loss_R_real: 0.0618  Loss_R_fake: 0.2514  D(x): 0.5235  D(G(z)): 0.4985  latent_norm : 16.0323  \n",
      "[2/55][400/937]  Loss_D: 1.3710  Loss_G: -0.6934  Loss_R_real: 0.0615  Loss_R_fake: 0.2219  D(x): 0.5175  D(G(z)): 0.4956  latent_norm : 13.8297  \n",
      "[2/55][600/937]  Loss_D: 1.3698  Loss_G: -0.6928  Loss_R_real: 0.0612  Loss_R_fake: 0.2276  D(x): 0.5031  D(G(z)): 0.4854  latent_norm : 15.9540  \n",
      "[2/55][800/937]  Loss_D: 1.3678  Loss_G: -0.6917  Loss_R_real: 0.0610  Loss_R_fake: 0.2270  D(x): 0.5077  D(G(z)): 0.4924  latent_norm : 14.6410  \n",
      "Model successfully saved.\n",
      "[3/55][200/937]  Loss_D: 1.3533  Loss_G: -0.6830  Loss_R_real: 0.0605  Loss_R_fake: 0.2475  D(x): 0.5071  D(G(z)): 0.4926  latent_norm : 16.2104  \n",
      "[3/55][400/937]  Loss_D: 1.3475  Loss_G: -0.6799  Loss_R_real: 0.0599  Loss_R_fake: 0.2287  D(x): 0.5212  D(G(z)): 0.4813  latent_norm : 15.1596  \n",
      "[3/55][600/937]  Loss_D: 1.3439  Loss_G: -0.6787  Loss_R_real: 0.0596  Loss_R_fake: 0.2349  D(x): 0.5209  D(G(z)): 0.4935  latent_norm : 14.3800  \n",
      "[3/55][800/937]  Loss_D: 1.3423  Loss_G: -0.6776  Loss_R_real: 0.0597  Loss_R_fake: 0.2347  D(x): 0.5416  D(G(z)): 0.4818  latent_norm : 16.5541  \n",
      "Model successfully saved.\n",
      "[4/55][200/937]  Loss_D: 1.3254  Loss_G: -0.6680  Loss_R_real: 0.0591  Loss_R_fake: 0.2387  D(x): 0.5267  D(G(z)): 0.5152  latent_norm : 14.9299  \n",
      "[4/55][400/937]  Loss_D: 1.3252  Loss_G: -0.6671  Loss_R_real: 0.0578  Loss_R_fake: 0.2379  D(x): 0.5171  D(G(z)): 0.4777  latent_norm : 14.7054  \n",
      "[4/55][600/937]  Loss_D: 1.3231  Loss_G: -0.6658  Loss_R_real: 0.0576  Loss_R_fake: 0.2345  D(x): 0.5083  D(G(z)): 0.4667  latent_norm : 15.0717  \n",
      "[4/55][800/937]  Loss_D: 1.3209  Loss_G: -0.6645  Loss_R_real: 0.0574  Loss_R_fake: 0.2352  D(x): 0.5145  D(G(z)): 0.4862  latent_norm : 14.7274  \n",
      "Model successfully saved.\n",
      "[5/55][200/937]  Loss_D: 1.3050  Loss_G: -0.6555  Loss_R_real: 0.0559  Loss_R_fake: 0.2047  D(x): 0.5502  D(G(z)): 0.4748  latent_norm : 18.0082  \n",
      "[5/55][400/937]  Loss_D: 1.3006  Loss_G: -0.6530  Loss_R_real: 0.0555  Loss_R_fake: 0.2193  D(x): 0.5290  D(G(z)): 0.4473  latent_norm : 15.4964  \n",
      "[5/55][600/937]  Loss_D: 1.2975  Loss_G: -0.6512  Loss_R_real: 0.0554  Loss_R_fake: 0.2220  D(x): 0.5156  D(G(z)): 0.4849  latent_norm : 15.1534  \n",
      "[5/55][800/937]  Loss_D: 1.2947  Loss_G: -0.6498  Loss_R_real: 0.0555  Loss_R_fake: 0.2290  D(x): 0.5330  D(G(z)): 0.4671  latent_norm : 16.5984  \n",
      "Model successfully saved.\n",
      "[6/55][200/937]  Loss_D: 1.2648  Loss_G: -0.6339  Loss_R_real: 0.0543  Loss_R_fake: 0.2580  D(x): 0.5524  D(G(z)): 0.4550  latent_norm : 14.3586  \n",
      "[6/55][400/937]  Loss_D: 1.2630  Loss_G: -0.6329  Loss_R_real: 0.0546  Loss_R_fake: 0.2746  D(x): 0.5452  D(G(z)): 0.4701  latent_norm : 14.8266  \n",
      "[6/55][600/937]  Loss_D: 1.2601  Loss_G: -0.6314  Loss_R_real: 0.0545  Loss_R_fake: 0.2664  D(x): 0.5530  D(G(z)): 0.4509  latent_norm : 16.3392  \n",
      "[6/55][800/937]  Loss_D: 1.2555  Loss_G: -0.6288  Loss_R_real: 0.0546  Loss_R_fake: 0.2666  D(x): 0.5427  D(G(z)): 0.4572  latent_norm : 15.2837  \n",
      "Model successfully saved.\n",
      "[7/55][200/937]  Loss_D: 1.2055  Loss_G: -0.6023  Loss_R_real: 0.0536  Loss_R_fake: 0.2774  D(x): 0.5931  D(G(z)): 0.4329  latent_norm : 15.2336  \n",
      "[7/55][400/937]  Loss_D: 1.1828  Loss_G: -0.5897  Loss_R_real: 0.0541  Loss_R_fake: 0.2742  D(x): 0.5980  D(G(z)): 0.3917  latent_norm : 16.3811  \n",
      "[7/55][600/937]  Loss_D: 1.1579  Loss_G: -0.5768  Loss_R_real: 0.0544  Loss_R_fake: 0.2786  D(x): 0.5902  D(G(z)): 0.3967  latent_norm : 15.4493  \n",
      "[7/55][800/937]  Loss_D: 1.1259  Loss_G: -0.5605  Loss_R_real: 0.0549  Loss_R_fake: 0.2843  D(x): 0.6291  D(G(z)): 0.3455  latent_norm : 15.3685  \n",
      "Model successfully saved.\n",
      "[8/55][200/937]  Loss_D: 0.9009  Loss_G: -0.4482  Loss_R_real: 0.0546  Loss_R_fake: 0.3043  D(x): 0.7357  D(G(z)): 0.3521  latent_norm : 16.8780  \n",
      "[8/55][400/937]  Loss_D: 0.8666  Loss_G: -0.4309  Loss_R_real: 0.0546  Loss_R_fake: 0.3156  D(x): 0.6687  D(G(z)): 0.2921  latent_norm : 15.4629  \n",
      "[8/55][600/937]  Loss_D: 0.8327  Loss_G: -0.4140  Loss_R_real: 0.0544  Loss_R_fake: 0.3201  D(x): 0.7246  D(G(z)): 0.3015  latent_norm : 15.5876  \n",
      "[8/55][800/937]  Loss_D: 0.7965  Loss_G: -0.3956  Loss_R_real: 0.0550  Loss_R_fake: 0.3222  D(x): 0.7437  D(G(z)): 0.2684  latent_norm : 15.8068  \n",
      "Model successfully saved.\n",
      "[9/55][200/937]  Loss_D: 0.5973  Loss_G: -0.2983  Loss_R_real: 0.0569  Loss_R_fake: 0.3462  D(x): 0.7812  D(G(z)): 0.2133  latent_norm : 15.7131  \n",
      "[9/55][400/937]  Loss_D: 0.5796  Loss_G: -0.2885  Loss_R_real: 0.0542  Loss_R_fake: 0.3362  D(x): 0.7757  D(G(z)): 0.3079  latent_norm : 15.3830  \n",
      "[9/55][600/937]  Loss_D: 0.5691  Loss_G: -0.2839  Loss_R_real: 0.0551  Loss_R_fake: 0.3450  D(x): 0.8007  D(G(z)): 0.3446  latent_norm : 15.0209  \n",
      "[9/55][800/937]  Loss_D: 0.5527  Loss_G: -0.2751  Loss_R_real: 0.0539  Loss_R_fake: 0.3478  D(x): 0.8201  D(G(z)): 0.2084  latent_norm : 16.1126  \n",
      "Model successfully saved.\n",
      "[10/55][200/937]  Loss_D: 0.4965  Loss_G: -0.2419  Loss_R_real: 0.0579  Loss_R_fake: 0.3394  D(x): 0.8484  D(G(z)): 0.1129  latent_norm : 15.9724  \n",
      "[10/55][400/937]  Loss_D: 0.4708  Loss_G: -0.2303  Loss_R_real: 0.0581  Loss_R_fake: 0.3425  D(x): 0.9397  D(G(z)): 0.1984  latent_norm : 16.7474  \n",
      "[10/55][600/937]  Loss_D: 0.4493  Loss_G: -0.2206  Loss_R_real: 0.0570  Loss_R_fake: 0.3395  D(x): 0.8119  D(G(z)): 0.1178  latent_norm : 16.2915  \n",
      "[10/55][800/937]  Loss_D: 0.4545  Loss_G: -0.2233  Loss_R_real: 0.0581  Loss_R_fake: 0.3453  D(x): 0.8660  D(G(z)): 0.1734  latent_norm : 15.6751  \n",
      "Model successfully saved.\n",
      "[11/55][200/937]  Loss_D: 0.3730  Loss_G: -0.1861  Loss_R_real: 0.0653  Loss_R_fake: 0.4562  D(x): 0.8061  D(G(z)): 0.2031  latent_norm : 15.9389  \n",
      "[11/55][400/937]  Loss_D: 0.4102  Loss_G: -0.1981  Loss_R_real: 0.0587  Loss_R_fake: 0.3866  D(x): 0.8188  D(G(z)): 0.1347  latent_norm : 15.3142  \n",
      "[11/55][600/937]  Loss_D: 0.3853  Loss_G: -0.1873  Loss_R_real: 0.0547  Loss_R_fake: 0.3645  D(x): 0.8804  D(G(z)): 0.1061  latent_norm : 17.1176  \n",
      "[11/55][800/937]  Loss_D: 0.3983  Loss_G: -0.1939  Loss_R_real: 0.0567  Loss_R_fake: 0.3577  D(x): 0.8689  D(G(z)): 0.1056  latent_norm : 15.4788  \n",
      "Model successfully saved.\n",
      "[12/55][200/937]  Loss_D: 0.2784  Loss_G: -0.1387  Loss_R_real: 0.0480  Loss_R_fake: 0.3454  D(x): 0.9381  D(G(z)): 0.1150  latent_norm : 15.0658  \n",
      "[12/55][400/937]  Loss_D: 0.3431  Loss_G: -0.1629  Loss_R_real: 0.0529  Loss_R_fake: 0.3386  D(x): 0.8109  D(G(z)): 0.1512  latent_norm : 16.1946  \n",
      "[12/55][600/937]  Loss_D: 0.3286  Loss_G: -0.1575  Loss_R_real: 0.0510  Loss_R_fake: 0.3419  D(x): 0.8839  D(G(z)): 0.1411  latent_norm : 15.7257  \n",
      "[12/55][800/937]  Loss_D: 0.3220  Loss_G: -0.1559  Loss_R_real: 0.0537  Loss_R_fake: 0.3437  D(x): 0.8529  D(G(z)): 0.1515  latent_norm : 16.4650  \n",
      "Model successfully saved.\n",
      "[13/55][200/937]  Loss_D: 0.3100  Loss_G: -0.1438  Loss_R_real: 0.0607  Loss_R_fake: 0.3639  D(x): 0.9195  D(G(z)): 0.1223  latent_norm : 14.6102  \n",
      "[13/55][400/937]  Loss_D: 0.2751  Loss_G: -0.1315  Loss_R_real: 0.0531  Loss_R_fake: 0.3634  D(x): 0.9527  D(G(z)): 0.1085  latent_norm : 15.9198  \n",
      "[13/55][600/937]  Loss_D: 0.2353  Loss_G: -0.1145  Loss_R_real: 0.0591  Loss_R_fake: 0.3756  D(x): 0.9427  D(G(z)): 0.0982  latent_norm : 15.2517  \n",
      "[13/55][800/937]  Loss_D: 0.2604  Loss_G: -0.1245  Loss_R_real: 0.0614  Loss_R_fake: 0.3660  D(x): 0.9369  D(G(z)): 0.0833  latent_norm : 16.3541  \n",
      "Model successfully saved.\n",
      "[14/55][200/937]  Loss_D: 0.3162  Loss_G: -0.1406  Loss_R_real: 0.0612  Loss_R_fake: 0.3113  D(x): 0.9232  D(G(z)): 0.0819  latent_norm : 16.7781  \n",
      "[14/55][400/937]  Loss_D: 0.2632  Loss_G: -0.1227  Loss_R_real: 0.0529  Loss_R_fake: 0.3088  D(x): 0.8738  D(G(z)): 0.0939  latent_norm : 16.4048  \n",
      "[14/55][600/937]  Loss_D: 0.2552  Loss_G: -0.1128  Loss_R_real: 0.0550  Loss_R_fake: 0.3321  D(x): 0.9734  D(G(z)): 0.0274  latent_norm : 16.8220  \n",
      "[14/55][800/937]  Loss_D: 0.2410  Loss_G: -0.1095  Loss_R_real: 0.0520  Loss_R_fake: 0.3245  D(x): 0.8835  D(G(z)): 0.0767  latent_norm : 14.8217  \n",
      "Model successfully saved.\n",
      "[15/55][200/937]  Loss_D: 0.2197  Loss_G: -0.1056  Loss_R_real: 0.0467  Loss_R_fake: 0.3788  D(x): 0.9605  D(G(z)): 0.0490  latent_norm : 15.1346  \n",
      "[15/55][400/937]  Loss_D: 0.1969  Loss_G: -0.0959  Loss_R_real: 0.0446  Loss_R_fake: 0.3420  D(x): 0.9857  D(G(z)): 0.0455  latent_norm : 15.1611  \n",
      "[15/55][600/937]  Loss_D: 0.2197  Loss_G: -0.1044  Loss_R_real: 0.0493  Loss_R_fake: 0.3438  D(x): 0.9296  D(G(z)): 0.0765  latent_norm : 15.3313  \n",
      "[15/55][800/937]  Loss_D: 0.2279  Loss_G: -0.1068  Loss_R_real: 0.0512  Loss_R_fake: 0.3454  D(x): 0.9011  D(G(z)): 0.1239  latent_norm : 14.8934  \n",
      "Model successfully saved.\n",
      "[16/55][200/937]  Loss_D: 0.3804  Loss_G: -0.2635  Loss_R_real: 0.0606  Loss_R_fake: 0.3769  D(x): 0.9830  D(G(z)): 0.0363  latent_norm : 16.1324  \n",
      "[16/55][400/937]  Loss_D: 0.2676  Loss_G: -0.1704  Loss_R_real: 0.0513  Loss_R_fake: 0.3314  D(x): 0.9647  D(G(z)): 0.0609  latent_norm : 15.9182  \n",
      "[16/55][600/937]  Loss_D: 0.2379  Loss_G: -0.1420  Loss_R_real: 0.0512  Loss_R_fake: 0.3299  D(x): 0.9366  D(G(z)): 0.0555  latent_norm : 15.6462  \n",
      "[16/55][800/937]  Loss_D: 0.2505  Loss_G: -0.1392  Loss_R_real: 0.0519  Loss_R_fake: 0.3350  D(x): 0.9424  D(G(z)): 0.0171  latent_norm : 15.6409  \n",
      "Model successfully saved.\n",
      "[17/55][200/937]  Loss_D: 0.3264  Loss_G: -0.1829  Loss_R_real: 0.0611  Loss_R_fake: 0.3538  D(x): 0.9755  D(G(z)): 0.0215  latent_norm : 15.4846  \n",
      "[17/55][400/937]  Loss_D: 0.2171  Loss_G: -0.1194  Loss_R_real: 0.0528  Loss_R_fake: 0.3361  D(x): 0.9579  D(G(z)): 0.0514  latent_norm : 14.6751  \n",
      "[17/55][600/937]  Loss_D: 0.1868  Loss_G: -0.1008  Loss_R_real: 0.0490  Loss_R_fake: 0.3308  D(x): 0.9553  D(G(z)): 0.0460  latent_norm : 15.0171  \n",
      "[17/55][800/937]  Loss_D: 0.2056  Loss_G: -0.1091  Loss_R_real: 0.0543  Loss_R_fake: 0.3356  D(x): 0.9729  D(G(z)): 0.0788  latent_norm : 16.8790  \n",
      "Model successfully saved.\n",
      "[18/55][200/937]  Loss_D: 0.4148  Loss_G: -0.3158  Loss_R_real: 0.0652  Loss_R_fake: 0.3592  D(x): 0.9835  D(G(z)): 0.0190  latent_norm : 16.4263  \n",
      "[18/55][400/937]  Loss_D: 0.2643  Loss_G: -0.1865  Loss_R_real: 0.0540  Loss_R_fake: 0.3516  D(x): 0.9629  D(G(z)): 0.0405  latent_norm : 16.2333  \n",
      "[18/55][600/937]  Loss_D: 0.2590  Loss_G: -0.1578  Loss_R_real: 0.0585  Loss_R_fake: 0.3602  D(x): 0.9576  D(G(z)): 0.0040  latent_norm : 15.1755  \n",
      "[18/55][800/937]  Loss_D: 0.2249  Loss_G: -0.1337  Loss_R_real: 0.0545  Loss_R_fake: 0.3392  D(x): 0.9896  D(G(z)): 0.0555  latent_norm : 16.3137  \n",
      "Model successfully saved.\n",
      "[19/55][200/937]  Loss_D: 0.1915  Loss_G: -0.1064  Loss_R_real: 0.0517  Loss_R_fake: 0.3022  D(x): 0.9783  D(G(z)): 0.0084  latent_norm : 15.8426  \n",
      "[19/55][400/937]  Loss_D: 0.1435  Loss_G: -0.0776  Loss_R_real: 0.0481  Loss_R_fake: 0.3044  D(x): 0.9238  D(G(z)): 0.0115  latent_norm : 15.2643  \n",
      "[19/55][600/937]  Loss_D: 0.1782  Loss_G: -0.0852  Loss_R_real: 0.0500  Loss_R_fake: 0.3134  D(x): 0.9378  D(G(z)): 0.0724  latent_norm : 15.6776  \n",
      "[19/55][800/937]  Loss_D: 0.1586  Loss_G: -0.0762  Loss_R_real: 0.0471  Loss_R_fake: 0.3253  D(x): 0.9659  D(G(z)): 0.0216  latent_norm : 15.5364  \n",
      "Model successfully saved.\n",
      "[20/55][200/937]  Loss_D: 0.1030  Loss_G: -0.0517  Loss_R_real: 0.0383  Loss_R_fake: 0.3059  D(x): 0.9538  D(G(z)): 0.0224  latent_norm : 14.9801  \n",
      "[20/55][400/937]  Loss_D: 0.1525  Loss_G: -0.0629  Loss_R_real: 0.0496  Loss_R_fake: 0.3207  D(x): 0.9528  D(G(z)): 0.0743  latent_norm : 15.4961  \n",
      "[20/55][600/937]  Loss_D: 0.1312  Loss_G: -0.0567  Loss_R_real: 0.0462  Loss_R_fake: 0.3220  D(x): 0.9621  D(G(z)): 0.0315  latent_norm : 15.3212  \n",
      "[20/55][800/937]  Loss_D: 0.1526  Loss_G: -0.0668  Loss_R_real: 0.0478  Loss_R_fake: 0.3232  D(x): 0.9184  D(G(z)): 0.0665  latent_norm : 15.8285  \n",
      "Model successfully saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thamires/Documents/Knowledge_Discovery/Results-Learning-Cortical-Representations-Trough-Perturbed-and-Adversarial-Dreaming/code/functions.py:244: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/55][200/937]  Loss_D: 0.5386  Loss_G: -0.4127  Loss_R_real: 0.0596  Loss_R_fake: 0.3436  D(x): 0.9858  D(G(z)): 0.0187  latent_norm : 15.3571  \n",
      "[21/55][400/937]  Loss_D: 0.3150  Loss_G: -0.2296  Loss_R_real: 0.0506  Loss_R_fake: 0.3357  D(x): 0.9579  D(G(z)): 0.0142  latent_norm : 16.2841  \n",
      "[21/55][600/937]  Loss_D: 0.2384  Loss_G: -0.1673  Loss_R_real: 0.0467  Loss_R_fake: 0.3474  D(x): 0.9779  D(G(z)): 0.0493  latent_norm : 14.8626  \n",
      "[21/55][800/937]  Loss_D: 0.2480  Loss_G: -0.1453  Loss_R_real: 0.0490  Loss_R_fake: 0.3507  D(x): 0.9553  D(G(z)): 0.0474  latent_norm : 15.6733  \n",
      "Model successfully saved.\n",
      "[22/55][200/937]  Loss_D: 0.0921  Loss_G: -0.0461  Loss_R_real: 0.0379  Loss_R_fake: 0.3320  D(x): 0.9906  D(G(z)): 0.0435  latent_norm : 15.5846  \n",
      "[22/55][400/937]  Loss_D: 0.1794  Loss_G: -0.1093  Loss_R_real: 0.0488  Loss_R_fake: 0.3492  D(x): 0.9567  D(G(z)): 0.0279  latent_norm : 16.0981  \n",
      "[22/55][600/937]  Loss_D: 0.1441  Loss_G: -0.0849  Loss_R_real: 0.0453  Loss_R_fake: 0.3371  D(x): 0.9938  D(G(z)): 0.0033  latent_norm : 15.5346  \n",
      "[22/55][800/937]  Loss_D: 0.1476  Loss_G: -0.0810  Loss_R_real: 0.0483  Loss_R_fake: 0.3352  D(x): 0.9827  D(G(z)): 0.0311  latent_norm : 16.0018  \n",
      "Model successfully saved.\n",
      "[23/55][200/937]  Loss_D: 0.1688  Loss_G: -0.0805  Loss_R_real: 0.0525  Loss_R_fake: 0.3244  D(x): 0.9675  D(G(z)): 0.0441  latent_norm : 15.2368  \n",
      "[23/55][400/937]  Loss_D: 0.1155  Loss_G: -0.0558  Loss_R_real: 0.0456  Loss_R_fake: 0.3172  D(x): 0.9790  D(G(z)): 0.0209  latent_norm : 15.3815  \n",
      "[23/55][600/937]  Loss_D: 0.1012  Loss_G: -0.0495  Loss_R_real: 0.0428  Loss_R_fake: 0.3230  D(x): 0.9661  D(G(z)): 0.0402  latent_norm : 16.0758  \n",
      "[23/55][800/937]  Loss_D: 0.1429  Loss_G: -0.0736  Loss_R_real: 0.0468  Loss_R_fake: 0.3226  D(x): 0.9653  D(G(z)): 0.0576  latent_norm : 15.6801  \n",
      "Model successfully saved.\n",
      "[24/55][200/937]  Loss_D: 0.0842  Loss_G: -0.0416  Loss_R_real: 0.0388  Loss_R_fake: 0.3129  D(x): 0.9901  D(G(z)): 0.0278  latent_norm : 16.4809  \n",
      "[24/55][400/937]  Loss_D: 0.2180  Loss_G: -0.1022  Loss_R_real: 0.0510  Loss_R_fake: 0.3326  D(x): 0.9741  D(G(z)): 0.0139  latent_norm : 16.6644  \n",
      "[24/55][600/937]  Loss_D: 0.1658  Loss_G: -0.0784  Loss_R_real: 0.0466  Loss_R_fake: 0.3236  D(x): 0.9878  D(G(z)): 0.0059  latent_norm : 15.8252  \n",
      "[24/55][800/937]  Loss_D: 0.1424  Loss_G: -0.0680  Loss_R_real: 0.0442  Loss_R_fake: 0.3245  D(x): 0.9845  D(G(z)): 0.0222  latent_norm : 16.7195  \n",
      "Model successfully saved.\n",
      "[25/55][200/937]  Loss_D: 0.1780  Loss_G: -0.1075  Loss_R_real: 0.0499  Loss_R_fake: 0.3485  D(x): 0.9969  D(G(z)): 0.0272  latent_norm : 15.3554  \n",
      "[25/55][400/937]  Loss_D: 0.0999  Loss_G: -0.0598  Loss_R_real: 0.0456  Loss_R_fake: 0.3282  D(x): 0.9671  D(G(z)): 0.0725  latent_norm : 15.9941  \n",
      "[25/55][600/937]  Loss_D: 0.0892  Loss_G: -0.0511  Loss_R_real: 0.0422  Loss_R_fake: 0.3118  D(x): 0.9809  D(G(z)): 0.0348  latent_norm : 15.2346  \n",
      "[25/55][800/937]  Loss_D: 0.0865  Loss_G: -0.0478  Loss_R_real: 0.0420  Loss_R_fake: 0.3178  D(x): 0.9596  D(G(z)): 0.0188  latent_norm : 16.3838  \n",
      "Model successfully saved.\n",
      "[26/55][200/937]  Loss_D: 0.0443  Loss_G: -0.0227  Loss_R_real: 0.0440  Loss_R_fake: 0.3073  D(x): 0.9263  D(G(z)): 0.1038  latent_norm : 15.3276  \n",
      "[26/55][400/937]  Loss_D: 0.1362  Loss_G: -0.0741  Loss_R_real: 0.0494  Loss_R_fake: 0.3168  D(x): 0.9505  D(G(z)): 0.0459  latent_norm : 15.2833  \n",
      "[26/55][600/937]  Loss_D: 0.1109  Loss_G: -0.0595  Loss_R_real: 0.0461  Loss_R_fake: 0.3235  D(x): 0.9764  D(G(z)): 0.0299  latent_norm : 15.8322  \n",
      "[26/55][800/937]  Loss_D: 0.1164  Loss_G: -0.0623  Loss_R_real: 0.0460  Loss_R_fake: 0.3253  D(x): 0.9518  D(G(z)): 0.0394  latent_norm : 15.3895  \n",
      "Model successfully saved.\n",
      "[27/55][200/937]  Loss_D: 0.3763  Loss_G: -0.2415  Loss_R_real: 0.0593  Loss_R_fake: 0.3394  D(x): 0.9828  D(G(z)): 0.0237  latent_norm : 14.3266  \n",
      "[27/55][400/937]  Loss_D: 0.2054  Loss_G: -0.1294  Loss_R_real: 0.0616  Loss_R_fake: 0.3180  D(x): 0.9593  D(G(z)): 0.0066  latent_norm : 16.2578  \n",
      "[27/55][600/937]  Loss_D: 0.1488  Loss_G: -0.0924  Loss_R_real: 0.0542  Loss_R_fake: 0.3123  D(x): 0.9929  D(G(z)): 0.0126  latent_norm : 16.0486  \n",
      "[27/55][800/937]  Loss_D: 0.1799  Loss_G: -0.1194  Loss_R_real: 0.0533  Loss_R_fake: 0.3216  D(x): 0.9730  D(G(z)): 0.0173  latent_norm : 16.4337  \n",
      "Model successfully saved.\n",
      "[28/55][200/937]  Loss_D: 0.1633  Loss_G: -0.0488  Loss_R_real: 0.0463  Loss_R_fake: 0.2970  D(x): 0.9881  D(G(z)): 0.0274  latent_norm : 15.7499  \n",
      "[28/55][400/937]  Loss_D: 0.1068  Loss_G: -0.0378  Loss_R_real: 0.0412  Loss_R_fake: 0.3003  D(x): 0.9949  D(G(z)): 0.0058  latent_norm : 16.1265  \n",
      "[28/55][600/937]  Loss_D: 0.1200  Loss_G: -0.0550  Loss_R_real: 0.0432  Loss_R_fake: 0.3067  D(x): 0.9879  D(G(z)): 0.0122  latent_norm : 16.0989  \n",
      "[28/55][800/937]  Loss_D: 0.0931  Loss_G: -0.0430  Loss_R_real: 0.0424  Loss_R_fake: 0.3041  D(x): 0.9990  D(G(z)): 0.0021  latent_norm : 15.9202  \n",
      "Model successfully saved.\n",
      "[29/55][200/937]  Loss_D: 0.0355  Loss_G: -0.0180  Loss_R_real: 0.0340  Loss_R_fake: 0.2768  D(x): 0.9836  D(G(z)): 0.0233  latent_norm : 15.3857  \n",
      "[29/55][400/937]  Loss_D: 0.1428  Loss_G: -0.0600  Loss_R_real: 0.0414  Loss_R_fake: 0.2744  D(x): 0.9911  D(G(z)): 0.0077  latent_norm : 15.1581  \n",
      "[29/55][600/937]  Loss_D: 0.1101  Loss_G: -0.0477  Loss_R_real: 0.0401  Loss_R_fake: 0.2871  D(x): 0.9898  D(G(z)): 0.0537  latent_norm : 14.0024  \n",
      "[29/55][800/937]  Loss_D: 0.1128  Loss_G: -0.0487  Loss_R_real: 0.0413  Loss_R_fake: 0.2942  D(x): 0.9753  D(G(z)): 0.0373  latent_norm : 16.4633  \n",
      "Model successfully saved.\n",
      "[30/55][200/937]  Loss_D: 0.0822  Loss_G: -0.0386  Loss_R_real: 0.0523  Loss_R_fake: 0.3799  D(x): 0.9997  D(G(z)): 0.0014  latent_norm : 16.3971  \n",
      "[30/55][400/937]  Loss_D: 0.0439  Loss_G: -0.0210  Loss_R_real: 0.0465  Loss_R_fake: 0.3333  D(x): 0.9991  D(G(z)): 0.0002  latent_norm : 15.9598  \n",
      "[30/55][600/937]  Loss_D: 0.0303  Loss_G: -0.0145  Loss_R_real: 0.0424  Loss_R_fake: 0.3211  D(x): 0.9985  D(G(z)): 0.0013  latent_norm : 15.3247  \n",
      "[30/55][800/937]  Loss_D: 0.0247  Loss_G: -0.0119  Loss_R_real: 0.0402  Loss_R_fake: 0.3138  D(x): 0.9986  D(G(z)): 0.0025  latent_norm : 15.7896  \n",
      "Model successfully saved.\n",
      "[31/55][200/937]  Loss_D: 0.0476  Loss_G: -0.0248  Loss_R_real: 0.0398  Loss_R_fake: 0.2922  D(x): 0.9805  D(G(z)): 0.0026  latent_norm : 16.4444  \n",
      "[31/55][400/937]  Loss_D: 0.0581  Loss_G: -0.0289  Loss_R_real: 0.0384  Loss_R_fake: 0.2972  D(x): 0.9801  D(G(z)): 0.0086  latent_norm : 15.3006  \n",
      "[31/55][600/937]  Loss_D: 0.0555  Loss_G: -0.0277  Loss_R_real: 0.0373  Loss_R_fake: 0.2868  D(x): 0.9980  D(G(z)): 0.0045  latent_norm : 15.0192  \n",
      "[31/55][800/937]  Loss_D: 0.0739  Loss_G: -0.0401  Loss_R_real: 0.0412  Loss_R_fake: 0.2937  D(x): 0.9958  D(G(z)): 0.0025  latent_norm : 15.2043  \n",
      "Model successfully saved.\n",
      "[32/55][200/937]  Loss_D: 0.1927  Loss_G: -0.0989  Loss_R_real: 0.0448  Loss_R_fake: 0.3053  D(x): 0.9539  D(G(z)): 0.0387  latent_norm : 15.8536  \n",
      "[32/55][400/937]  Loss_D: 0.1252  Loss_G: -0.0646  Loss_R_real: 0.0408  Loss_R_fake: 0.3023  D(x): 0.9401  D(G(z)): 0.0511  latent_norm : 15.2665  \n",
      "[32/55][600/937]  Loss_D: 0.1023  Loss_G: -0.0525  Loss_R_real: 0.0383  Loss_R_fake: 0.2976  D(x): 0.9865  D(G(z)): 0.0393  latent_norm : 15.4448  \n",
      "[32/55][800/937]  Loss_D: 0.1163  Loss_G: -0.0632  Loss_R_real: 0.0424  Loss_R_fake: 0.2998  D(x): 0.9908  D(G(z)): 0.0003  latent_norm : 15.6352  \n",
      "Model successfully saved.\n",
      "[33/55][200/937]  Loss_D: 0.0094  Loss_G: -0.0051  Loss_R_real: 0.0347  Loss_R_fake: 0.2769  D(x): 0.9941  D(G(z)): 0.0026  latent_norm : 15.3193  \n",
      "[33/55][400/937]  Loss_D: 0.1449  Loss_G: -0.1062  Loss_R_real: 0.0357  Loss_R_fake: 0.2665  D(x): 0.6268  D(G(z)): 0.8656  latent_norm : 15.2357  \n",
      "[33/55][600/937]  Loss_D: 0.1126  Loss_G: -0.0782  Loss_R_real: 0.0438  Loss_R_fake: 0.2779  D(x): 0.9857  D(G(z)): 0.0083  latent_norm : 15.3300  \n",
      "[33/55][800/937]  Loss_D: 0.0950  Loss_G: -0.0641  Loss_R_real: 0.0414  Loss_R_fake: 0.2793  D(x): 0.9569  D(G(z)): 0.0291  latent_norm : 13.8410  \n",
      "Model successfully saved.\n",
      "[34/55][200/937]  Loss_D: 0.0455  Loss_G: -0.0230  Loss_R_real: 0.0333  Loss_R_fake: 0.2655  D(x): 0.9863  D(G(z)): 0.0116  latent_norm : 15.6086  \n",
      "[34/55][400/937]  Loss_D: 0.1309  Loss_G: -0.0718  Loss_R_real: 0.0420  Loss_R_fake: 0.2924  D(x): 0.9798  D(G(z)): 0.0049  latent_norm : 16.2282  \n",
      "[34/55][600/937]  Loss_D: 0.1039  Loss_G: -0.0563  Loss_R_real: 0.0392  Loss_R_fake: 0.2941  D(x): 0.9872  D(G(z)): 0.0217  latent_norm : 15.9400  \n",
      "[34/55][800/937]  Loss_D: 0.0908  Loss_G: -0.0486  Loss_R_real: 0.0378  Loss_R_fake: 0.2982  D(x): 0.9992  D(G(z)): 0.0031  latent_norm : 15.3652  \n",
      "Model successfully saved.\n",
      "[35/55][200/937]  Loss_D: 0.0498  Loss_G: -0.0255  Loss_R_real: 0.0351  Loss_R_fake: 0.3031  D(x): 0.9870  D(G(z)): 0.0136  latent_norm : 15.5750  \n",
      "[35/55][400/937]  Loss_D: 0.0458  Loss_G: -0.0234  Loss_R_real: 0.0340  Loss_R_fake: 0.2871  D(x): 0.9833  D(G(z)): 0.0174  latent_norm : 15.0458  \n",
      "[35/55][600/937]  Loss_D: 0.0799  Loss_G: -0.0497  Loss_R_real: 0.0369  Loss_R_fake: 0.2956  D(x): 0.9971  D(G(z)): 0.0057  latent_norm : 16.2094  \n",
      "[35/55][800/937]  Loss_D: 0.0670  Loss_G: -0.0409  Loss_R_real: 0.0369  Loss_R_fake: 0.2947  D(x): 0.9818  D(G(z)): 0.0136  latent_norm : 16.0104  \n",
      "Model successfully saved.\n",
      "[36/55][200/937]  Loss_D: 0.1743  Loss_G: -0.0870  Loss_R_real: 0.0464  Loss_R_fake: 0.3176  D(x): 0.9683  D(G(z)): 0.0254  latent_norm : 15.1727  \n",
      "[36/55][400/937]  Loss_D: 0.1123  Loss_G: -0.0562  Loss_R_real: 0.0413  Loss_R_fake: 0.3236  D(x): 0.9784  D(G(z)): 0.0230  latent_norm : 15.5066  \n",
      "[36/55][600/937]  Loss_D: 0.0916  Loss_G: -0.0458  Loss_R_real: 0.0389  Loss_R_fake: 0.3230  D(x): 0.9834  D(G(z)): 0.0162  latent_norm : 15.8113  \n",
      "[36/55][800/937]  Loss_D: 0.1040  Loss_G: -0.0548  Loss_R_real: 0.0410  Loss_R_fake: 0.3299  D(x): 0.9913  D(G(z)): 0.0315  latent_norm : 14.1802  \n",
      "Model successfully saved.\n",
      "[37/55][200/937]  Loss_D: 0.0284  Loss_G: -0.0148  Loss_R_real: 0.0348  Loss_R_fake: 0.3174  D(x): 0.9809  D(G(z)): 0.0023  latent_norm : 14.9044  \n",
      "[37/55][400/937]  Loss_D: 0.1012  Loss_G: -0.0366  Loss_R_real: 0.0415  Loss_R_fake: 0.3165  D(x): 0.9925  D(G(z)): 0.0132  latent_norm : 15.6771  \n",
      "[37/55][600/937]  Loss_D: 0.0841  Loss_G: -0.0327  Loss_R_real: 0.0390  Loss_R_fake: 0.3009  D(x): 0.9971  D(G(z)): 0.0017  latent_norm : 16.0970  \n",
      "[37/55][800/937]  Loss_D: 0.0762  Loss_G: -0.0310  Loss_R_real: 0.0377  Loss_R_fake: 0.3033  D(x): 0.9925  D(G(z)): 0.0219  latent_norm : 16.0632  \n",
      "Model successfully saved.\n",
      "[38/55][200/937]  Loss_D: 0.1566  Loss_G: -0.0680  Loss_R_real: 0.0479  Loss_R_fake: 0.3124  D(x): 0.9744  D(G(z)): 0.0167  latent_norm : 15.8985  \n",
      "[38/55][400/937]  Loss_D: 0.0953  Loss_G: -0.0425  Loss_R_real: 0.0402  Loss_R_fake: 0.2918  D(x): 0.9767  D(G(z)): 0.0067  latent_norm : 15.6747  \n",
      "[38/55][600/937]  Loss_D: 0.1235  Loss_G: -0.0594  Loss_R_real: 0.0405  Loss_R_fake: 0.2968  D(x): 0.9271  D(G(z)): 0.1103  latent_norm : 14.6867  \n",
      "[38/55][800/937]  Loss_D: 0.1073  Loss_G: -0.0520  Loss_R_real: 0.0407  Loss_R_fake: 0.3024  D(x): 0.9910  D(G(z)): 0.0171  latent_norm : 15.7566  \n",
      "Model successfully saved.\n",
      "[39/55][200/937]  Loss_D: 0.0375  Loss_G: -0.0194  Loss_R_real: 0.0314  Loss_R_fake: 0.2665  D(x): 0.9729  D(G(z)): 0.0095  latent_norm : 15.7777  \n",
      "[39/55][400/937]  Loss_D: 0.0299  Loss_G: -0.0156  Loss_R_real: 0.0317  Loss_R_fake: 0.2771  D(x): 0.9880  D(G(z)): 0.0014  latent_norm : 16.0389  \n",
      "[39/55][600/937]  Loss_D: 0.1367  Loss_G: -0.1039  Loss_R_real: 0.0368  Loss_R_fake: 0.2971  D(x): 0.9826  D(G(z)): 0.0631  latent_norm : 15.2964  \n",
      "[39/55][800/937]  Loss_D: 0.1116  Loss_G: -0.0825  Loss_R_real: 0.0360  Loss_R_fake: 0.2987  D(x): 0.9854  D(G(z)): 0.0188  latent_norm : 16.1598  \n",
      "Model successfully saved.\n",
      "[40/55][200/937]  Loss_D: 0.0507  Loss_G: -0.0273  Loss_R_real: 0.0362  Loss_R_fake: 0.2881  D(x): 0.9950  D(G(z)): 0.0081  latent_norm : 15.6190  \n",
      "[40/55][400/937]  Loss_D: 0.0425  Loss_G: -0.0225  Loss_R_real: 0.0342  Loss_R_fake: 0.3088  D(x): 0.9876  D(G(z)): 0.0021  latent_norm : 16.3597  \n",
      "[40/55][600/937]  Loss_D: 0.0401  Loss_G: -0.0210  Loss_R_real: 0.0336  Loss_R_fake: 0.2989  D(x): 0.9762  D(G(z)): 0.0109  latent_norm : 16.2951  \n",
      "[40/55][800/937]  Loss_D: 0.0699  Loss_G: -0.0319  Loss_R_real: 0.0389  Loss_R_fake: 0.3187  D(x): 0.9792  D(G(z)): 0.0256  latent_norm : 15.2643  \n",
      "Model successfully saved.\n",
      "[41/55][200/937]  Loss_D: 0.0314  Loss_G: -0.0159  Loss_R_real: 0.0326  Loss_R_fake: 0.3145  D(x): 0.9980  D(G(z)): 0.0080  latent_norm : 15.7025  \n",
      "[41/55][400/937]  Loss_D: 0.0848  Loss_G: -0.0494  Loss_R_real: 0.0383  Loss_R_fake: 0.3084  D(x): 0.9949  D(G(z)): 0.0075  latent_norm : 15.5192  \n",
      "[41/55][600/937]  Loss_D: 0.0675  Loss_G: -0.0386  Loss_R_real: 0.0365  Loss_R_fake: 0.3077  D(x): 0.9952  D(G(z)): 0.0077  latent_norm : 15.3672  \n",
      "[41/55][800/937]  Loss_D: 0.0586  Loss_G: -0.0331  Loss_R_real: 0.0355  Loss_R_fake: 0.3078  D(x): 0.9977  D(G(z)): 0.0029  latent_norm : 15.8185  \n",
      "Model successfully saved.\n",
      "[42/55][200/937]  Loss_D: 0.0365  Loss_G: -0.0188  Loss_R_real: 0.0339  Loss_R_fake: 0.3095  D(x): 0.9907  D(G(z)): 0.0155  latent_norm : 14.6776  \n",
      "[42/55][400/937]  Loss_D: 0.0371  Loss_G: -0.0189  Loss_R_real: 0.0327  Loss_R_fake: 0.3110  D(x): 0.9976  D(G(z)): 0.0068  latent_norm : 15.6113  \n",
      "[42/55][600/937]  Loss_D: 0.0823  Loss_G: -0.0344  Loss_R_real: 0.0367  Loss_R_fake: 0.3174  D(x): 0.9263  D(G(z)): 0.0905  latent_norm : 14.3085  \n",
      "[42/55][800/937]  Loss_D: 0.0737  Loss_G: -0.0319  Loss_R_real: 0.0369  Loss_R_fake: 0.3090  D(x): 0.9912  D(G(z)): 0.0062  latent_norm : 16.1168  \n",
      "Model successfully saved.\n",
      "[43/55][200/937]  Loss_D: 0.0286  Loss_G: -0.0147  Loss_R_real: 0.0311  Loss_R_fake: 0.2849  D(x): 0.9808  D(G(z)): 0.0046  latent_norm : 15.7017  \n",
      "[43/55][400/937]  Loss_D: 0.0344  Loss_G: -0.0176  Loss_R_real: 0.0310  Loss_R_fake: 0.2956  D(x): 0.9996  D(G(z)): 0.0078  latent_norm : 16.6823  \n",
      "[43/55][600/937]  Loss_D: 0.0901  Loss_G: -0.0383  Loss_R_real: 0.0343  Loss_R_fake: 0.2997  D(x): 0.9975  D(G(z)): 0.0407  latent_norm : 14.9347  \n",
      "[43/55][800/937]  Loss_D: 0.0753  Loss_G: -0.0328  Loss_R_real: 0.0350  Loss_R_fake: 0.2999  D(x): 0.9908  D(G(z)): 0.0047  latent_norm : 14.9420  \n",
      "Model successfully saved.\n",
      "[44/55][200/937]  Loss_D: 0.0322  Loss_G: -0.0165  Loss_R_real: 0.0305  Loss_R_fake: 0.2912  D(x): 0.9862  D(G(z)): 0.0043  latent_norm : 14.9276  \n",
      "[44/55][400/937]  Loss_D: 0.1261  Loss_G: -0.0573  Loss_R_real: 0.0406  Loss_R_fake: 0.3109  D(x): 0.9804  D(G(z)): 0.0172  latent_norm : 15.5517  \n",
      "[44/55][600/937]  Loss_D: 0.0923  Loss_G: -0.0424  Loss_R_real: 0.0379  Loss_R_fake: 0.3117  D(x): 0.9784  D(G(z)): 0.0158  latent_norm : 15.9854  \n",
      "[44/55][800/937]  Loss_D: 0.0765  Loss_G: -0.0354  Loss_R_real: 0.0361  Loss_R_fake: 0.3072  D(x): 0.9977  D(G(z)): 0.0046  latent_norm : 16.0043  \n",
      "Model successfully saved.\n",
      "[45/55][200/937]  Loss_D: 0.0245  Loss_G: -0.0127  Loss_R_real: 0.0337  Loss_R_fake: 0.3023  D(x): 0.9940  D(G(z)): 0.0075  latent_norm : 14.7545  \n",
      "[45/55][400/937]  Loss_D: 0.0254  Loss_G: -0.0131  Loss_R_real: 0.0325  Loss_R_fake: 0.3077  D(x): 0.9934  D(G(z)): 0.0107  latent_norm : 15.2810  \n",
      "[45/55][600/937]  Loss_D: 0.0271  Loss_G: -0.0138  Loss_R_real: 0.0320  Loss_R_fake: 0.3072  D(x): 0.9902  D(G(z)): 0.0034  latent_norm : 14.2446  \n",
      "[45/55][800/937]  Loss_D: 0.0638  Loss_G: -0.0298  Loss_R_real: 0.0365  Loss_R_fake: 0.3151  D(x): 0.9977  D(G(z)): 0.0050  latent_norm : 16.1406  \n",
      "Model successfully saved.\n",
      "[46/55][200/937]  Loss_D: 0.0440  Loss_G: -0.0218  Loss_R_real: 0.0357  Loss_R_fake: 0.3051  D(x): 0.9989  D(G(z)): 0.0000  latent_norm : 15.2607  \n",
      "[46/55][400/937]  Loss_D: 0.0229  Loss_G: -0.0114  Loss_R_real: 0.0339  Loss_R_fake: 0.2824  D(x): 0.9999  D(G(z)): 0.0019  latent_norm : 15.5678  \n",
      "[46/55][600/937]  Loss_D: 0.0160  Loss_G: -0.0080  Loss_R_real: 0.0324  Loss_R_fake: 0.2825  D(x): 0.9995  D(G(z)): 0.0010  latent_norm : 13.8829  \n",
      "[46/55][800/937]  Loss_D: 0.0135  Loss_G: -0.0068  Loss_R_real: 0.0315  Loss_R_fake: 0.2852  D(x): 0.9786  D(G(z)): 0.0050  latent_norm : 14.8642  \n",
      "Model successfully saved.\n",
      "[47/55][200/937]  Loss_D: 0.0028  Loss_G: -0.0016  Loss_R_real: 0.0295  Loss_R_fake: 0.2728  D(x): 0.9955  D(G(z)): 0.0001  latent_norm : 15.1063  \n",
      "[47/55][400/937]  Loss_D: 0.0042  Loss_G: -0.0024  Loss_R_real: 0.0292  Loss_R_fake: 0.2708  D(x): 0.9984  D(G(z)): 0.0082  latent_norm : 15.4512  \n",
      "[47/55][600/937]  Loss_D: 0.0670  Loss_G: -0.0404  Loss_R_real: 0.0405  Loss_R_fake: 0.2811  D(x): 0.9742  D(G(z)): 0.0133  latent_norm : 15.6775  \n",
      "[47/55][800/937]  Loss_D: 0.0563  Loss_G: -0.0335  Loss_R_real: 0.0387  Loss_R_fake: 0.2857  D(x): 0.9940  D(G(z)): 0.0182  latent_norm : 16.4878  \n",
      "Model successfully saved.\n",
      "[48/55][200/937]  Loss_D: 0.0669  Loss_G: -0.0312  Loss_R_real: 0.0338  Loss_R_fake: 0.2791  D(x): 0.9888  D(G(z)): 0.0201  latent_norm : 16.7676  \n",
      "[48/55][400/937]  Loss_D: 0.0467  Loss_G: -0.0223  Loss_R_real: 0.0327  Loss_R_fake: 0.2636  D(x): 0.9852  D(G(z)): 0.0111  latent_norm : 15.6130  \n",
      "[48/55][600/937]  Loss_D: 0.0410  Loss_G: -0.0198  Loss_R_real: 0.0319  Loss_R_fake: 0.2597  D(x): 0.9987  D(G(z)): 0.0032  latent_norm : 14.9962  \n",
      "[48/55][800/937]  Loss_D: 0.0700  Loss_G: -0.0283  Loss_R_real: 0.0345  Loss_R_fake: 0.2677  D(x): 0.9920  D(G(z)): 0.0075  latent_norm : 17.2989  \n",
      "Model successfully saved.\n",
      "[49/55][200/937]  Loss_D: 0.0353  Loss_G: -0.0182  Loss_R_real: 0.0314  Loss_R_fake: 0.3133  D(x): 0.9972  D(G(z)): 0.0146  latent_norm : 15.4140  \n",
      "[49/55][400/937]  Loss_D: 0.0301  Loss_G: -0.0155  Loss_R_real: 0.0315  Loss_R_fake: 0.2968  D(x): 0.9979  D(G(z)): 0.0012  latent_norm : 16.6955  \n",
      "[49/55][600/937]  Loss_D: 0.0285  Loss_G: -0.0146  Loss_R_real: 0.0308  Loss_R_fake: 0.3103  D(x): 0.9817  D(G(z)): 0.0233  latent_norm : 14.9411  \n",
      "[49/55][800/937]  Loss_D: 0.0573  Loss_G: -0.0264  Loss_R_real: 0.0355  Loss_R_fake: 0.3150  D(x): 0.9967  D(G(z)): 0.0029  latent_norm : 16.2221  \n",
      "Model successfully saved.\n",
      "[50/55][200/937]  Loss_D: 0.0168  Loss_G: -0.0084  Loss_R_real: 0.0307  Loss_R_fake: 0.2484  D(x): 0.9955  D(G(z)): 0.0013  latent_norm : 15.4634  \n",
      "[50/55][400/937]  Loss_D: 0.0665  Loss_G: -0.0451  Loss_R_real: 0.0355  Loss_R_fake: 0.2653  D(x): 0.9960  D(G(z)): 0.0051  latent_norm : 15.4621  \n",
      "[50/55][600/937]  Loss_D: 0.0467  Loss_G: -0.0313  Loss_R_real: 0.0344  Loss_R_fake: 0.2708  D(x): 0.9892  D(G(z)): 0.0111  latent_norm : 14.0810  \n",
      "[50/55][800/937]  Loss_D: 0.0744  Loss_G: -0.0535  Loss_R_real: 0.0358  Loss_R_fake: 0.2736  D(x): 0.9954  D(G(z)): 0.0144  latent_norm : 16.3400  \n",
      "Model successfully saved.\n",
      "[51/55][200/937]  Loss_D: 0.0062  Loss_G: -0.0033  Loss_R_real: 0.0315  Loss_R_fake: 0.2895  D(x): 0.9970  D(G(z)): 0.0018  latent_norm : 16.0635  \n",
      "[51/55][400/937]  Loss_D: 0.0082  Loss_G: -0.0043  Loss_R_real: 0.0303  Loss_R_fake: 0.2914  D(x): 0.9970  D(G(z)): 0.0147  latent_norm : 15.2737  \n",
      "[51/55][600/937]  Loss_D: 0.1039  Loss_G: -0.0818  Loss_R_real: 0.0325  Loss_R_fake: 0.2805  D(x): 0.9953  D(G(z)): 0.0095  latent_norm : 13.8879  \n",
      "[51/55][800/937]  Loss_D: 0.0817  Loss_G: -0.0634  Loss_R_real: 0.0333  Loss_R_fake: 0.2791  D(x): 0.9984  D(G(z)): 0.0020  latent_norm : 15.4298  \n",
      "Model successfully saved.\n",
      "[52/55][200/937]  Loss_D: 0.0210  Loss_G: -0.0109  Loss_R_real: 0.0291  Loss_R_fake: 0.2724  D(x): 0.9959  D(G(z)): 0.0025  latent_norm : 16.5103  \n",
      "[52/55][400/937]  Loss_D: 0.1001  Loss_G: -0.0680  Loss_R_real: 0.0383  Loss_R_fake: 0.2914  D(x): 0.9921  D(G(z)): 0.0079  latent_norm : 15.3255  \n",
      "[52/55][600/937]  Loss_D: 0.0723  Loss_G: -0.0482  Loss_R_real: 0.0355  Loss_R_fake: 0.2776  D(x): 0.9943  D(G(z)): 0.0035  latent_norm : 15.5910  \n",
      "[52/55][800/937]  Loss_D: 0.0585  Loss_G: -0.0384  Loss_R_real: 0.0339  Loss_R_fake: 0.2751  D(x): 0.9922  D(G(z)): 0.0051  latent_norm : 14.6981  \n",
      "Model successfully saved.\n",
      "[53/55][200/937]  Loss_D: 0.1414  Loss_G: -0.0557  Loss_R_real: 0.0445  Loss_R_fake: 0.3101  D(x): 0.9820  D(G(z)): 0.0188  latent_norm : 15.7507  \n",
      "[53/55][400/937]  Loss_D: 0.0839  Loss_G: -0.0346  Loss_R_real: 0.0367  Loss_R_fake: 0.2934  D(x): 0.9953  D(G(z)): 0.0105  latent_norm : 15.5173  \n",
      "[53/55][600/937]  Loss_D: 0.0761  Loss_G: -0.0333  Loss_R_real: 0.0369  Loss_R_fake: 0.2912  D(x): 0.9881  D(G(z)): 0.0122  latent_norm : 15.7526  \n",
      "[53/55][800/937]  Loss_D: 0.0633  Loss_G: -0.0281  Loss_R_real: 0.0350  Loss_R_fake: 0.2904  D(x): 0.9999  D(G(z)): 0.0005  latent_norm : 16.2865  \n",
      "Model successfully saved.\n",
      "[54/55][200/937]  Loss_D: 0.0220  Loss_G: -0.0112  Loss_R_real: 0.0394  Loss_R_fake: 0.2948  D(x): 0.9973  D(G(z)): 0.0248  latent_norm : 15.5151  \n",
      "[54/55][400/937]  Loss_D: 0.0212  Loss_G: -0.0108  Loss_R_real: 0.0348  Loss_R_fake: 0.2896  D(x): 0.9819  D(G(z)): 0.0176  latent_norm : 14.5882  \n",
      "[54/55][600/937]  Loss_D: 0.0221  Loss_G: -0.0112  Loss_R_real: 0.0328  Loss_R_fake: 0.2805  D(x): 0.9902  D(G(z)): 0.0048  latent_norm : 15.7485  \n",
      "[54/55][800/937]  Loss_D: 0.0540  Loss_G: -0.0225  Loss_R_real: 0.0372  Loss_R_fake: 0.2891  D(x): 0.9993  D(G(z)): 0.0008  latent_norm : 15.2661  \n",
      "Model successfully saved.\n"
     ]
    }
   ],
   "source": [
    "# Enable anomaly detection during training (optional)\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(len(d_losses), opt.niter):\n",
    "    \n",
    "    # Initialize lists to store losses and other metrics\n",
    "    store_loss_D = []\n",
    "    store_loss_G = []\n",
    "    store_loss_R_real = []\n",
    "    store_loss_R_fake = []\n",
    "    store_norm = []\n",
    "    store_kl = []\n",
    "\n",
    "    # Iterate over the data batches\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # Wake (W)\n",
    "        ###########################\n",
    "\n",
    "        # Discrimination wake\n",
    "        discriminator_optimizer .zero_grad()\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Fetch real images and labels\n",
    "        real_image, label = data\n",
    "        real_image, label = real_image.to(device), label.to(device)\n",
    "\n",
    "        # Pass real images through the discriminator\n",
    "        latent_output, dis_output = discriminator(real_image)\n",
    "\n",
    "        # Add noise to the latent space\n",
    "        latent_output_noise = latent_output + opt.epsilon * torch.randn(batch_size, latent_size, device=device)\n",
    "\n",
    "        # Set the discriminator label for real images\n",
    "        discriminator_label[:] = real_label_value\n",
    "\n",
    "        # Compute the discriminator loss for real images\n",
    "        dis_errD_real = discriminator_criterion(dis_output, discriminator_label)\n",
    "\n",
    "        if opt.R > 0.0:  # if GAN learning occurs\n",
    "            (dis_errD_real).backward(retain_graph=True)\n",
    "\n",
    "        # Compute the KL divergence regularization loss\n",
    "        kl = kl_loss(latent_output)\n",
    "        (kl).backward(retain_graph=True)\n",
    "\n",
    "        # Reconstruct real images from the latent space\n",
    "        reconstructed_image = generator(latent_output_noise, reverse=False)\n",
    "\n",
    "        # Compute the reconstruction loss for real images\n",
    "        rec_real = reconstruction_criterion (reconstructed_image, real_image)\n",
    "\n",
    "        if opt.W > 0.0:\n",
    "            (opt.W * rec_real).backward()\n",
    "\n",
    "        discriminator_optimizer .step()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Compute the mean of the discriminator output (between 0 and 1)\n",
    "        D_x = dis_output.cpu().mean()\n",
    "\n",
    "        # Compute the norm of the latent space representation\n",
    "        latent_norm = torch.mean(torch.norm(latent_output.squeeze(), dim=1)).item()\n",
    "\n",
    "\n",
    "        ###########################\n",
    "        # NREM perturbed dreaming (N)\n",
    "        ##########################\n",
    "        discriminator_optimizer .zero_grad()\n",
    "\n",
    "        # Detach the latent space representation\n",
    "        latent_z = latent_output.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate images from the detached latent space\n",
    "            nrem_image = generator(latent_z)\n",
    "\n",
    "            # Apply occlusion to the generated images\n",
    "            occlusion = Occlude(drop_rate=random.random(), tile_size=random.randint(1, 8))\n",
    "            occluded_nrem_image = occlusion(nrem_image, dim=1)\n",
    "\n",
    "        # Pass occluded NREM images through the discriminator\n",
    "        latent_recons_dream, _ = discriminator(occluded_nrem_image)\n",
    "\n",
    "        # Compute the reconstruction loss for fake images\n",
    "        rec_fake = reconstruction_criterion (latent_recons_dream, latent_output.detach())\n",
    "\n",
    "        if opt.N > 0.0:\n",
    "            (opt.N * rec_fake).backward()\n",
    "\n",
    "        discriminator_optimizer .step()\n",
    "\n",
    "\n",
    "       ###########################\n",
    "        # REM adversarial dreaming (R)\n",
    "        ##########################\n",
    "\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        generator_optimizer.zero_grad()\n",
    "        lmbd = opt.lmbd\n",
    "        noise = torch.randn(batch_size, latent_size, device=device)\n",
    "        if i==0:\n",
    "            latent_z = 0.5*latent_output.detach() + 0.5*noise\n",
    "        else:\n",
    "            latent_z = 0.25*latent_output.detach() + 0.25*old_latent_output + 0.5*noise\n",
    "        \n",
    "        dreamed_image_adv = generator(latent_z, reverse=True) # activate plasticity switch\n",
    "        latent_recons_dream, dis_output = discriminator(dreamed_image_adv)\n",
    "        discriminator_label[:] = fake_label_value # should be classified as fake\n",
    "        dis_errD_fake = discriminator_criterion(dis_output, discriminator_label)\n",
    "        if opt.R > 0.0: # if GAN learning occurs\n",
    "            dis_errD_fake.backward(retain_graph=True)\n",
    "            discriminator_optimizer.step()\n",
    "            generator_optimizer.step()\n",
    "        dis_errG = - dis_errD_fake\n",
    "\n",
    "        D_G_z1 = dis_output.cpu().mean()\n",
    "\n",
    "        old_latent_output = latent_output.detach()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        # Compute average losses\n",
    "        ###########################\n",
    "        store_loss_G.append(dis_errG.item())\n",
    "        store_loss_D.append((dis_errD_fake + dis_errD_real).item())\n",
    "        store_loss_R_real.append(rec_real.item())\n",
    "        store_loss_R_fake.append(rec_fake.item())\n",
    "        store_norm.append(latent_norm)\n",
    "        store_kl.append(kl.item())\n",
    "        \n",
    "\n",
    "\n",
    "        if i % 200 == 0 and i>1:\n",
    "            print('[%d/%d][%d/%d]  Loss_D: %.4f  Loss_G: %.4f  Loss_R_real: %.4f  Loss_R_fake: %.4f  D(x): %.4f  D(G(z)): %.4f  latent_norm : %.4f  '\n",
    "                % (epoch, opt.niter, i, len(dataloader),\n",
    "                    np.mean(store_loss_D), np.mean(store_loss_G), np.mean(store_loss_R_real), np.mean(store_loss_R_fake), D_x, D_G_z1, np.mean(latent_norm) ))\n",
    "            compare_img_rec = torch.zeros(batch_size * 2, real_image.size(1), real_image.size(2), real_image.size(3))\n",
    "            with torch.no_grad():\n",
    "                reconstructed_image = generator(latent_output)\n",
    "            compare_img_rec[::2] = real_image\n",
    "            compare_img_rec[1::2] = reconstructed_image\n",
    "            vutils.save_image(unorm(compare_img_rec[:128]), '%s/recon_%03d.png' % (dir_files, epoch), nrow=8)\n",
    "            fake = unorm(dreamed_image_adv)\n",
    "            vutils.save_image(fake[:64].data, '%s/fake_%03d.png' % (dir_files, epoch), nrow=8)\n",
    "            \n",
    "\n",
    "    d_losses.append(np.mean(store_loss_D))\n",
    "    g_losses.append(np.mean(store_loss_G))\n",
    "    r_losses_real.append(np.mean(store_loss_R_real))\n",
    "    r_losses_fake.append(np.mean(store_loss_R_fake))\n",
    "    kl_losses.append(np.mean(store_kl))\n",
    "    save_fig_losses(epoch, d_losses, g_losses, r_losses_real, r_losses_fake, kl_losses, None, None,  dir_files)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save({\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        'g_optim': generator_optimizer.state_dict(),\n",
    "        'd_optim': discriminator_optimizer.state_dict(),\n",
    "        'd_losses': d_losses,\n",
    "        'g_losses': g_losses,\n",
    "        'r_losses_real': r_losses_real,\n",
    "        'r_losses_fake': r_losses_fake,\n",
    "        'kl_losses': kl_losses,\n",
    "    }, dir_checkpoint+'/trained.pth')\n",
    "    \n",
    "    # save network after 1 learning epoch\n",
    "    if epoch ==1:\n",
    "            torch.save({\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        }, dir_checkpoint+'/trained2.pth')\n",
    "\n",
    "    print(f'Model successfully saved.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
